number,created_at,merged_at,closed_at,files,additions,deletions,body_text,participants,comments
23,2022-10-26T03:23:09Z,2022-10-26T05:00:34Z,2022-10-26T05:00:34Z,11,228,28,"Add support for huggingface hub
I could not find a good way to enforce stop tokens over the huggingface hub api - that needs to hopefully be cleaned up in the future",2,0
31,2022-10-27T13:33:00Z,,2023-04-12T00:36:50Z,19,602,36,"Major changes:

Expose a ""generate"" method to permit sampling / n-best list generation
Expose ability to return logprobs of the generated tokens. Add to cohere and OpenAI llms
Add example prompts from paper https://arxiv.org/pdf/2203.11171.pdf
Add self-consistent chain-of-thought prompt logic to a chain

I don't like a lot of the overloading int he chain I made so will clean things up. Pubbing for initial feedbak",2,1
33,2022-10-27T19:12:22Z,2022-10-28T01:17:04Z,2022-10-28T01:17:04Z,1,1,1,Currently the cohere module uses a non-supported model. Updating this to use the default model if one is not specified.,3,1
44,2022-10-31T01:21:57Z,2022-10-31T05:48:52Z,2022-10-31T05:48:52Z,1,25,0,@sjwhitmore anything you think would have been helpful to know?,2,0
48,2022-11-01T04:33:38Z,2022-11-02T04:29:40Z,2022-11-02T04:29:40Z,19,427,5,also adds embeddings and an in memory docstore,2,5
49,2022-11-01T04:58:32Z,2022-11-05T19:43:22Z,2022-11-05T19:43:22Z,3,219,2,Checking that this structure looks generally ok -- going to sub in logic where the TODO comment is then add a test.,2,2
60,2022-11-03T21:47:38Z,2022-11-04T15:42:45Z,2022-11-04T15:42:45Z,4,32,6,,2,0
62,2022-11-05T01:37:14Z,2022-11-10T19:24:12Z,2022-11-10T19:24:12Z,5,191,0,,2,0
64,2022-11-05T21:22:45Z,2022-11-06T23:40:33Z,2022-11-06T23:40:33Z,20,414,254,,2,0
67,2022-11-05T23:51:39Z,2022-11-08T15:01:42Z,2022-11-08T15:01:42Z,5,221,4,woo!,2,2
68,2022-11-06T16:18:27Z,2022-11-07T13:46:44Z,2022-11-07T13:46:44Z,3,86,0,Add support of HuggingFace embedding models,2,3
79,2022-11-07T18:11:09Z,2022-11-07T21:34:45Z,2022-11-07T21:34:46Z,7,67,36,"Addresses the issue in #76 by either using the relevant environment variable if set or using a string passed in the constructor.
Prefers the constructor string over the environment variable, which seemed like the natural choice to me.",2,1
81,2022-11-08T03:15:24Z,2022-11-08T14:24:23Z,2022-11-08T14:24:23Z,4,149,1,"lots of kwargs! generation docs here: https://docs.nlpcloud.com/#generation
This somewhat breaks the paradigm introduced in LLM base class as the stop sequence isn't a list, and should rightfully be introduced at the time of initialization of the class, along with the other kwargs that depend on its presence (e.g. remove_end_sequence, etc.) curious if you'd want to refactor LLM base class to take out stop as a specific named kwarg?",2,1
91,2022-11-08T23:08:31Z,2022-11-10T05:15:42Z,2022-11-10T05:15:42Z,3,371,3,certainly broken atm but pushing for visibility while afk,2,1
92,2022-11-09T01:15:14Z,2022-11-09T02:19:39Z,2022-11-09T02:19:39Z,6,29,9,,2,0
99,2022-11-09T13:54:03Z,2022-11-10T16:12:29Z,2022-11-10T16:12:29Z,2,142,0,"Integrate AI21 /complete API into langchain, to allow access to Jurassic models.",2,2
102,2022-11-09T18:25:13Z,2022-11-09T21:23:17Z,2022-11-09T21:23:17Z,2,2,1,"change requirements.txt to fix Issue #101
update .gitignore to support VSCode dev environment",2,0
103,2022-11-09T19:37:58Z,2022-11-10T04:45:30Z,2022-11-10T04:45:30Z,4,118,16,"This PR is for Issue #88

 make format
 make lint
 make tests",2,1
105,2022-11-09T20:07:48Z,2022-11-09T21:44:27Z,2022-11-09T21:44:27Z,3,101,1,Add support for cohere embeddings,2,0
106,2022-11-09T20:24:17Z,2022-11-09T21:26:58Z,2022-11-09T21:26:58Z,2,5,1,"This fixes Issue #104
The tests for HF Embeddings is skipped because of the segfault issue mentioned there. Perhaps, a new issue should be created for that?",2,0
113,2022-11-10T07:11:04Z,2022-11-10T15:53:45Z,2022-11-10T15:53:46Z,2,1,9,i dont think either of these variables are used?,2,0
119,2022-11-11T23:03:28Z,2022-11-21T00:23:58Z,2022-11-21T00:23:58Z,7,146,15,"this will break atm but wanted to get thoughts on implementation.

should add() be on docstore interface?
should InMemoryDocstore change to take a list of documents as init? (makes this slightly easier to implement in FAISS -- if we think it is less clean then could expose a method to get the number of documents currently in the dict, and perform the logic of creating the necessary dictionary in the FAISS.add_texts method.",3,0
125,2022-11-12T22:30:26Z,2022-11-27T08:25:00Z,2022-11-27T08:25:00Z,7,148,4,"Main design q is whether this should be consolidated with HuggingFaceEmbeddings, could be confusing to have 2 separate huggingface embedings classes. Will add unit tests and docs once that's decided.
Closes #86",2,0
127,2022-11-13T18:25:41Z,2022-11-13T21:15:30Z,2022-11-13T21:15:30Z,3,25,0,"This is a simple proof of concept of using external files as templates.
I'm still feeling my way around the codebase.
As a user, I want to use files as prompts, so it will be easier to manage and test prompts.
The future direction is to use a template engine, most likely Mako.",2,0
128,2022-11-13T20:05:35Z,2022-11-13T21:16:19Z,2022-11-13T21:16:19Z,2,6,10,Fix a few typos and wrapped f-strings,2,0
135,2022-11-14T07:24:47Z,2022-11-14T16:34:02Z,2022-11-14T16:34:02Z,15,51,59,"fixes the Jupyter environment variable issues mentioned in issue #134
fixes format/lint issues in some unrelated files (from make format/lint)",2,1
139,2022-11-14T18:09:55Z,2022-11-14T19:34:08Z,2022-11-14T19:34:08Z,2,2,2,"pull_request runs on the merge commit between the opened PR and the target branch where the PR is to be merged — master in this case. This is desirable because that way the new changes get linted and tested.
The existing pull_request_target specifier causes lint and test to run on the target branch itself (i.e. master in this case). That way the new code in the PR doesn't get linted and tested at all. This can also lead to security vulnerabilities, as described in the GitHub docs:

Screenshot from here: https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request_target
Link from the screenshot: https://securitylab.github.com/research/github-actions-preventing-pwn-requests/",2,0
163,2022-11-20T15:23:09Z,,2023-08-08T14:36:14Z,5,39,0,"load a template using Mako template engine.
https://www.makotemplates.org/
The goal is to have a more robust templating format.",5,2
166,2022-11-21T01:41:03Z,2022-11-24T00:35:38Z,2022-11-24T00:35:38Z,12,578,6,"Add MemoryChain and ConversationChain as chains that take a docstore in addition to the prompt, and use the docstore to stuff context into the prompt. This can be used to have an ongoing conversation with a chatbot.
Probably needs a bit of refactoring for code quality",2,2
178,2022-11-23T19:13:26Z,2022-11-23T21:12:47Z,2022-11-23T21:12:47Z,8,119,14,"Also updated docs, and noticed an issue with the add_texts method on VectorStores that I had missed before -- the metadatas arg should be required to match the classmethod which initializes the VectorStores (the add_example methods break otherwise in the ExampleSelectors)",2,0
180,2022-11-24T01:06:49Z,,2022-12-14T19:32:00Z,4,446,14,,2,0
191,2022-11-25T15:43:28Z,2022-11-25T17:41:27Z,2022-11-25T17:41:27Z,4,53,18,,2,1
194,2022-11-26T04:03:05Z,2022-11-27T17:10:35Z,2022-11-27T17:10:35Z,3,77,1,,2,0
203,2022-11-26T14:51:54Z,2022-11-26T16:34:17Z,2022-11-26T16:34:17Z,1,1,1,"Fix Unicode error on Windows during setup, while trying to read contents of README.md.
(Issue #200)",2,0
205,2022-11-26T20:50:12Z,,2022-11-26T23:15:32Z,1,3,3,"Currently fails for any questions that don't need a follow up.
As it's currently written if the question asked doesn't require a follow up then we check on line 35 to see if have an answer and throw an error if not. That means that if no follow up is required it just throws an error.
The fix here is a bit hacky but works.",2,2
214,2022-11-28T16:59:11Z,2022-11-28T19:11:31Z,2022-11-28T19:11:31Z,1,4,2,"Adds some context over what chain is running, thereby making it more obvious how different chains are entered and existed

(note that the ... is because the output is too long and VSCode truncated it)",2,1
239,2022-12-02T00:28:58Z,,2023-05-15T21:56:34Z,4,388,3,introduce output parser and use it in a for loop,3,1
241,2022-12-02T01:58:00Z,2022-12-02T20:28:22Z,2022-12-02T20:28:22Z,4,131,0,"Adding an API chain w/ a unit test, would love some feedback/review!
Credits to @hwchase17 for early design suggestions/feedback/etc!",2,0
242,2022-12-02T08:28:44Z,2022-12-04T00:42:59Z,2022-12-04T00:42:59Z,11,3655,89,"Adopts Poetry as a dependency manager
Introduces dependency version requirements
Deprecates Python 3.7 support

TODO

 Update developer guide
 Add back playwright, manifest-ml, and jupyter to dependency group

Not Doing => Fast Follow

Investigate single source for version, perhaps relying on GitHub tags and tackling this issue",2,2
248,2022-12-03T03:33:08Z,2022-12-03T16:11:38Z,2022-12-03T16:11:38Z,1,1,1,"With the original prompt, the chain keeps trying to jump straight to doing math directly, without first looking up ages. With this two-part question, it behaves more as intended:

Entering new ZeroShotAgent chain...
How old is Olivia Wilde's boyfriend? What is that number raised to the 0.23 power?
Thought: I need to find out how old Olivia Wilde's boyfriend is, and then use a calculator to calculate the power.
Action: Search
Action Input: Olivia Wilde's boyfriend age
Observation: While Wilde, 37, and Styles, 27, have both kept a low profile when it comes to talking about their relationship, Wilde did address their ...
Thought: Olivia Wilde's boyfriend is 27 years old.
Action: Calculator
Action Input: 27^0.23


Entering new LLMMathChain chain...
27^0.23

import math
print(math.pow(27, 0.23))
Answer: 2.1340945944237553

Finished LLMMathChain chain.

Observation: Answer: 2.1340945944237553
Thought: I now know the final answer.
Final Answer: 2.1340945944237553

Finished ZeroShotAgent chain.",2,0
254,2022-12-03T22:13:13Z,2022-12-05T20:50:48Z,2022-12-05T20:50:48Z,5,43,12,not actually sure the desired return in add_example to example selector is actually general/good - whats the use case?,2,1
257,2022-12-04T00:04:12Z,2022-12-07T05:58:16Z,2022-12-07T05:58:16Z,2,81,0,Arbitrary transformation chains that can be used to add dictionary extractions from llms/other chains,2,0
260,2022-12-05T02:19:52Z,2022-12-07T05:57:50Z,2022-12-07T05:57:50Z,9,291,4,"Love the project, a ton of fun!
I think the PR is pretty self-explanatory, happy to make any changes! I am working on using it in an LLMBashChain and may update as that progresses.",2,6
261,2022-12-05T02:38:12Z,2022-12-20T14:41:33Z,2022-12-20T14:41:33Z,2,183,24,,2,0
281,2022-12-07T21:24:12Z,2022-12-09T20:49:05Z,2022-12-09T20:49:06Z,7,238,0,"Implementation of https://github.com/jagilley/fact-checker. Works pretty well.

Verifying this manually:

""Only two kinds of egg-laying mammals are left on the planet today—the duck-billed platypus and the echidna, or spiny anteater."" https://www.scientificamerican.com/article/extreme-monotremes/
""An [Echidna] egg weighs 1.5 to 2 grams (0.05 to 0.07 oz)[19] and is about 1.4 centimetres (0.55 in) long."" https://en.wikipedia.org/wiki/Echidna#:~:text=sleep%20is%20suppressed.-,Reproduction,a%20reptile%2Dlike%20egg%20tooth.
""A [platypus] lays one to three (usually two) small, leathery eggs (similar to those of reptiles), about 11 mm (7⁄16 in) in diameter and slightly rounder than bird eggs."" https://en.wikipedia.org/wiki/Platypus#:~:text=It%20lays%20one%20to%20three,slightly%20rounder%20than%20bird%20eggs.
Therefore, an Echidna is the mammal that lays the biggest eggs.

cc @hwchase17",2,5
284,2022-12-08T08:54:40Z,2022-12-20T14:34:39Z,2022-12-20T14:34:39Z,2,213,4,closes #87,3,0
286,2022-12-08T17:41:30Z,,2022-12-19T01:36:28Z,11,89,83,"basic idea: all agent prompts need to end with thoughts input variable (where the inner monologue of the agent occurs)
but other than that, can take arbitary keys",2,4
293,2022-12-09T20:21:42Z,,2022-12-11T23:56:57Z,11,426,17,TODO,2,1
297,2022-12-10T09:24:19Z,2022-12-11T15:22:59Z,2022-12-11T15:22:59Z,1,71,0,,4,2
303,2022-12-10T21:11:53Z,2022-12-11T00:18:01Z,2022-12-11T00:18:01Z,1,306,306,Nothing of substance was changed. I simply corrected a few minor errors that could slow down the reader.,2,0
304,2022-12-10T21:49:03Z,,2022-12-12T19:11:27Z,3,49,4,Allows for batching via the apply method of a LLM subclass. Note that only OpenAI's client is optimized in this PR.,2,1
305,2022-12-10T23:14:31Z,2022-12-11T15:09:07Z,2022-12-11T15:09:07Z,3,37,0,a simple helper to clear the buffer in Conversation*Memory classes,2,0
306,2022-12-11T01:21:23Z,2022-12-11T07:16:32Z,2022-12-11T07:16:32Z,9,171,68,,2,0
313,2022-12-11T16:50:07Z,2022-12-13T13:50:03Z,2022-12-13T13:50:03Z,2,73,13,,2,0
316,2022-12-12T00:14:20Z,,2022-12-12T03:36:33Z,13,281,5,Save and Load LLM Chain. Will add tests before merging.,2,0
317,2022-12-12T00:50:36Z,,2023-05-15T21:56:50Z,4,216,0,"this agent allow switching between the normal llm and PAL chain automatical. PAL chain will be used whenever the question has to do with mathematical calculations.
also improve the base PAL chain.",3,1
318,2022-12-12T03:36:24Z,2022-12-13T14:38:49Z,2022-12-13T14:38:49Z,20,277,28,Will add tests. Have not supported saving Manifest yet - use of dictionary a bit weird,2,2
321,2022-12-12T06:43:17Z,2022-12-13T02:11:35Z,2022-12-13T02:11:35Z,2,50,15,,3,0
322,2022-12-12T08:52:08Z,2022-12-13T13:48:53Z,2022-12-13T13:48:53Z,9,1921,1644,"This PR has two contributions:


Add test for when stop token is found in middle of text


Add code coverage tooling and instructions



Add pytest-cov via poetry
Add necessary config files
Add new make instruction for coverage
Update README with coverage guidance
Update minor README formatting/spelling",2,4
329,2022-12-13T06:36:46Z,2022-12-13T13:20:22Z,2022-12-13T13:20:23Z,2,7,0,,2,0
330,2022-12-13T08:51:40Z,2022-12-13T13:15:51Z,2022-12-13T13:15:51Z,1,3,1,,2,0
338,2022-12-14T15:28:53Z,2022-12-15T05:07:30Z,2022-12-15T05:07:30Z,3,71,3,#301,2,2
344,2022-12-15T05:59:37Z,2022-12-16T01:49:14Z,2022-12-16T01:49:14Z,5,217,9,,4,8
351,2022-12-15T21:26:29Z,2022-12-16T01:01:39Z,2022-12-16T01:01:40Z,1,24,7,"add more formal support for explicitly specifying each model, but in a backwards compatible way",3,2
352,2022-12-15T23:46:55Z,,2023-05-09T22:33:09Z,21,124,116,"Add two new interfaces: SingleVariableChain and MultiVariableChain
This fixes a couple of problems:

It was inconsistent to call single input chains with run and multiple input chains with __call__
The apply function for single input chains expected a List[Dict[str, Any]], which broke the convention of keeping input and output keys abstracted away from the user.
Allows run to be called with **kwargs instead using __call__ directly with a Dict, which is a bit cleaner.
No more need for validation logic for certain chains (i.e. no need for SimpleSequentialChain to validate that all chains are single in/out, just make all the chains SingleVariableChains)",3,2
353,2022-12-16T00:11:46Z,2022-12-17T15:00:04Z,2022-12-17T15:00:04Z,7,332,39,"#354
Add support for running your own HF pipeline locally. This would allow you to get a lot more dynamic with what HF features and models you support since you wouldn't be beholden to what is hosted in HF hub. You could also do stuff with HF Optimum to quantize your models and stuff to get pretty fast inference even running on a laptop.",2,7
355,2022-12-16T01:46:25Z,2022-12-16T06:35:42Z,2022-12-16T06:35:42Z,6,1767,1559,,2,0
358,2022-12-16T06:48:52Z,2022-12-16T14:25:30Z,2022-12-16T14:25:30Z,4,44,6,,2,0
361,2022-12-16T08:00:09Z,2022-12-17T00:47:24Z,2022-12-17T00:47:24Z,2,30,8,"Created a generic SQLAlchemyCache class to plug any database supported by SQAlchemy. (I am using Postgres).
I also based the class SQLiteCache class on this class SQLAlchemyCache.
As a side note, I'm questioning the need for two distinct class LLMCache, FullLLMCache. Shouldn't we merge both ?",2,3
364,2022-12-16T23:22:15Z,2022-12-17T15:02:59Z,2022-12-17T15:02:59Z,2,44,1,"#363
@hwchase17 how much does this make you want to cry?",2,3
367,2022-12-18T04:30:10Z,2022-12-18T20:54:57Z,2022-12-18T20:54:57Z,3,61,10,"Before, run was not able to be called with multiple arguments. This expands the functionality.",2,0
376,2022-12-18T21:31:33Z,2022-12-19T20:59:48Z,2022-12-19T20:59:48Z,3,13,4,"We may want to include by default, a flexible templating engine (that support list loop !).
Since Jinja2 is the most famous one, I guess it's the good one.
Open for debate.
If it's not approved, I propose to add an example with it in the documentation.",2,0
377,2022-12-19T01:15:35Z,2022-12-19T22:09:27Z,2022-12-19T22:09:27Z,6,259,6,implement max marginal relevance example selector,2,0
395,2022-12-21T21:09:35Z,2022-12-22T01:45:38Z,2022-12-22T01:45:38Z,3,198,8,"Hi!  This PR adds support for the Azure OpenAI service to LangChain.
I've tried to follow the contributing guidelines.",2,10
397,2022-12-22T01:03:55Z,2022-12-22T17:31:27Z,2022-12-22T17:31:27Z,4,317,190,"I'm using a hash function for the key just to make sure its length doesn't get out of hand, otherwise the implementation is quite similar.",2,3
406,2022-12-23T15:35:18Z,,2023-05-09T22:35:32Z,2,60,5,,5,3
409,2022-12-23T17:00:14Z,2022-12-23T18:13:07Z,2022-12-23T18:13:07Z,7,19,19,"I was honored by the twitter mention, so used PyCharm to try and... help docs even a little bit.
Mostly typo-s and correct spellings.
PyCharm really complains about ""really good"" being used all the time and recommended alternative wordings haha",2,0
410,2022-12-23T21:33:07Z,,2023-05-16T10:22:56Z,192,11027,1573,"Many TODOs but already workable.
Now its easy to load big open source models.
I was able to load neox 20b and facebook 30b on my laptop with 64gb ram and 16gb vram.",40,5
422,2022-12-25T08:23:55Z,2022-12-26T18:08:21Z,2022-12-26T18:08:21Z,1,82,0,Open to feedbacks. Will add in support for summarizing once reaching token limit,2,1
424,2022-12-25T16:37:08Z,2022-12-26T04:03:06Z,2022-12-26T04:03:06Z,1,2,2,HuggingFace -> Hugging Face,2,0
428,2022-12-26T14:15:31Z,2022-12-27T13:22:48Z,2022-12-27T13:22:48Z,10,325,20,,2,0
441,2022-12-27T20:57:56Z,2022-12-28T00:54:59Z,2022-12-28T00:54:59Z,1,4,0,This updates the serpapi wrapper to look for sports results and knowledge graph results.,2,0
442,2022-12-27T21:43:39Z,2022-12-28T00:53:45Z,2022-12-28T00:53:45Z,1,2,1,,2,0
448,2022-12-28T10:35:55Z,2022-12-28T22:16:20Z,2022-12-28T22:16:20Z,4,151,3,,2,2
453,2022-12-28T14:53:23Z,2022-12-28T22:13:09Z,2022-12-28T22:13:09Z,3,92,18,,2,0
456,2022-12-28T15:30:42Z,,2023-07-26T07:36:42Z,1,12,0,"Confirm this API is better than a zipped version of this, ie. list of tuples [(inputs, outputs)]
 Add tests
 Add example notebook",4,1
458,2022-12-28T17:33:55Z,2022-12-29T13:22:32Z,2022-12-29T13:22:32Z,3,46,24,"When using chains such as Summarization chain (load_summarize_chain), the verbose flag wasn't propagated to the LLMChain.",2,0
460,2022-12-28T18:42:38Z,,2023-05-09T22:40:42Z,2,114,5,Signed-off-by: Diwank Singh Tomer diwank.singh@gmail.com,3,3
463,2022-12-28T23:29:40Z,2022-12-29T19:29:37Z,2022-12-29T19:29:37Z,7,356,46,"Related to #199
Motivation: SerpAPI is very expensive and scrapping can cause problems.
Solution: Implemented the new Google search API done through the Programmable search engine.
It is a bit longer to set up but worth it as it gives 10,000 search queries per day for free.
Instructions:
1. Install google-api-python-client
- If you don't already have a Google account, sign up.
- If you have never created a Google APIs Console project,
read the Managing Projects page and create a project in the Google API Console.
- Install the library using pip install google-api-python-client
The current version of the library is 2.70.0 at this time
2. To create an API key:
- Navigate to the APIs & Services→Credentials panel in Cloud Console.
- Select Create credentials, then select API key from the drop-down menu.
- The API key created dialog box displays your newly created key.
- You now have an `API_KEY`

3. Setup Custom Search Engine so you can search the entire web
- Create a custom search engine in this link.
- In Sites to search, add any valid URL (i.e. www.stackoverflow.com).
- That’s all you have to fill up, the rest doesn’t matter.
In the left-side menu, click Edit search engine → {your search engine name}
→ Setup Set Search the entire web to ON. Remove the URL you added from
 the list of Sites to search.
- Under Search engine ID you’ll find the `search-engine-ID`.

4. Enable the Custom Search API
- Navigate to the APIs & Services→Dashboard panel in Cloud Console.
- Click Enable APIs and Services.
- Search for Custom Search API and click on it.
- Click Enable.
URL for it: https://console.cloud.google.com/apis/library/customsearch.googleapis
.com
Adapted from: Instructions adapated from https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search


 Implementation
 Tests, Inline Docs, Formatting
 Add it to load_tools.
 Improve external documentation

I think it still needs some general work here. Has been a while since I've coded in Python. I tried my best to follow the steps provided in the resources.",4,1
464,2022-12-29T04:30:17Z,2023-01-06T15:25:55Z,2023-01-06T15:25:55Z,8,432,1,,3,0
468,2022-12-29T13:12:59Z,2022-12-29T15:19:52Z,2022-12-29T15:19:52Z,2,2,2,,2,0
471,2022-12-29T14:35:30Z,2023-01-04T18:48:43Z,2023-01-04T18:48:43Z,2,53,6,"This patch adds an optional parameter to SQLAlchemyCache which allows you to pass your own cache schema class, so that you can customize the table or field types used by the cache.
Additionally, this includes documentation on how to use this feature to add full-text Postgres search support using the sqlalchemy-utils package.
(See discussion on previous attempt in #469 as well)",2,3
477,2022-12-29T19:31:29Z,2022-12-30T13:06:58Z,2022-12-30T13:06:58Z,10,651,3,,2,0
478,2022-12-29T20:12:09Z,2023-01-04T15:54:25Z,2023-01-04T15:54:25Z,33,1014,177,,3,0
479,2022-12-29T21:15:38Z,2022-12-30T02:34:47Z,2022-12-30T02:34:47Z,16,219,73,"first pass at stdout callback
for the most part, went pretty smoothly. aside from the code here, here are some comments/observations.



should somehow default to stdouthandler so i dont have to do
from langchain.callbacks import get_callback_manager
from langchain.callbacks.stdout import StdOutCallbackHandler

get_callback_manager().add_handler(StdOutCallbackHandler())



I kept around the verbosity flag. 1) this is pretty important for getting the stdout to look good for agents (and other things). 2) I actually added this for LLM class since it didn't have it.


The only part that isn't basically perfectly moved over is the end of the agent run. Here's a screenshot of the new stdout tracing



Noticing it is missing logging of the final thought, eg before this is what it looked like

The reason its missing is that this was previously logged as part of agent end (lines 205 and 206)
this is probably only relevant for the std out logger? any thoughts for how to get it back in?",2,0
480,2022-12-29T21:57:11Z,2023-01-02T16:24:09Z,2023-01-02T16:24:09Z,164,4302,2562,"Big docs refactor! Motivation is to make it easier for people to find resources they are looking for. To accomplish this, there are now three main sections:

Getting Started: steps for getting started, walking through most core functionality
Modules: these are different modules of functionality that langchain provides. Each part here has a ""getting started"", ""how to"", ""key concepts"" and ""reference"" section (except in a few select cases where it didnt easily fit).
Use Cases: this is to separate use cases (like summarization, question answering, evaluation, etc) from the modules, and provide a different entry point to the code base.

There is also a full reference section, as well as extra resources (glossary, gallery, etc)",3,0
481,2022-12-29T21:57:57Z,2022-12-30T13:21:14Z,2022-12-30T13:21:14Z,1,2,2,this enables more things from myst like colon fences in notebooks,2,0
482,2022-12-29T23:24:07Z,2022-12-30T01:30:32Z,2022-12-30T01:30:32Z,9,185,90,,2,0
485,2022-12-30T02:53:11Z,2022-12-30T04:07:56Z,2022-12-30T04:07:56Z,3,33,25,"i kinda like this just because we call self.callback_manager so many times, and thats nicer than self._get_callback_manager()?",2,0
488,2022-12-30T03:53:15Z,2022-12-30T19:43:28Z,2022-12-30T19:43:28Z,5,94,2,"also add a set handler method
usage is:
from langchain.callbacks.streamlit import StreamlitCallbackHandler
import langchain
langchain.set_handler(StreamlitCallbackHandler())

produces the following output

only works for agent stuff currently",2,0
490,2022-12-30T04:14:06Z,2022-12-30T18:55:30Z,2022-12-30T18:55:30Z,11,35,34,deprecate all prints in favor of callback_manager.on_text (open to better naming),2,0
492,2022-12-30T05:15:24Z,2022-12-30T20:35:22Z,2022-12-30T20:35:22Z,14,893,367,,2,0
493,2022-12-30T05:16:00Z,2022-12-30T20:34:36Z,2022-12-30T20:34:36Z,12,102,102,,2,0
494,2022-12-30T05:42:15Z,2022-12-30T12:20:13Z,2022-12-30T12:20:13Z,2,2,2,,2,0
496,2022-12-30T12:57:12Z,2022-12-30T18:26:41Z,2022-12-30T18:26:41Z,7,43,15,"remove verbose from someplace it didnt relaly belong
everywhere else, make verbose Optional[bool] with default to None
make base classes accept None, and then look up globabl verbosity if thats the case",2,1
500,2022-12-31T06:37:42Z,2022-12-31T11:06:16Z,2022-12-31T11:06:16Z,1,1,1,,2,0
505,2023-01-01T16:34:51Z,2023-01-01T18:25:05Z,2023-01-01T18:25:05Z,3,201,70,,2,0
506,2023-01-01T18:26:27Z,2023-01-01T21:50:24Z,2023-01-01T21:50:24Z,54,976,1147,,2,0
507,2023-01-01T22:47:12Z,2023-01-02T03:17:24Z,2023-01-02T03:17:24Z,1,1,1,"Update model= to model_name=.
No need to credit me for this 😄",2,0
513,2023-01-02T19:02:14Z,2023-01-03T15:45:09Z,2023-01-03T15:45:09Z,4,50,26,"unify names in map reduce and refine chains to just be return_intermediate_steps
also unify the return key",2,0
514,2023-01-02T19:22:45Z,2023-01-03T15:46:08Z,2023-01-03T15:46:08Z,2,122,11,add a generate method which makes one final forward pass through the llm,2,1
516,2023-01-03T00:55:25Z,2023-01-08T14:49:22Z,2023-01-08T14:49:22Z,7,408,10,"add a chain that applies a prompt to all inputs and then returns not only an answer but scores it
add examples for question answering and question answering with sources",2,0
517,2023-01-03T01:27:54Z,2023-01-03T04:25:50Z,2023-01-03T04:25:50Z,1,1,1,,2,0
524,2023-01-03T16:49:12Z,2023-01-03T18:17:00Z,2023-01-03T18:17:00Z,1,6,0,,2,0
526,2023-01-04T02:35:56Z,2023-01-06T15:15:25Z,2023-01-06T15:15:25Z,2,15,2,"Add finish_reason to Generation as well as extend BaseOpenAI._generate to include it in the output. This can be useful for usage in downstream tasks when we need to filter for only generations that finished because of ""stop"" for example. Maybe we should add this to LLMChain as well?
For more details, see https://beta.openai.com/docs/guides/completion/best-practices",2,3
531,2023-01-04T11:00:23Z,2023-01-04T18:43:53Z,2023-01-04T18:43:53Z,1,105,6,updated embeddings.ipynb,2,0
534,2023-01-04T16:46:50Z,2023-01-05T05:35:41Z,2023-01-05T05:35:41Z,32,865,220,"Add support for local build and linkchecking of docs
Add GitHub Action to automatically check links before prior to publication
Minor reformat of Contributing readme
Fix existing broken links",2,7
535,2023-01-04T17:36:03Z,2023-01-04T18:43:02Z,2023-01-04T18:43:02Z,1,1,1,therefor -> therefore,2,0
541,2023-01-05T02:50:10Z,2023-01-05T04:23:55Z,2023-01-05T04:23:55Z,3,32,10,,2,0
552,2023-01-06T09:27:25Z,2023-01-06T14:45:01Z,2023-01-06T14:45:01Z,1,42,36,"I was very confused about where to put the tool priority statement.
I thought it would be better to put it in a place other than the place you suggested, so I added it to the NOTEBOOK. What do you think?",2,0
557,2023-01-06T16:30:00Z,2023-01-06T17:34:09Z,2023-01-06T17:34:09Z,1,3,3,"Small quick fix:
Suggest making the order of the menu the same as it is written on the page (Getting Started -> Key Concepts). Before the menu order was not the same as it was on the page. Not sure if this is the only place the menu is affected.
Mismatch is found here: https://langchain.readthedocs.io/en/latest/modules/llms.html",2,0
562,2023-01-08T19:20:12Z,2023-01-12T04:16:42Z,2023-01-12T04:16:42Z,1,7,0,"This is a sample PR to construct VectorStore from the existing index for Pinecone; I am happy to implement similar methods for the rest of the implementations.
While I used index_name  here, a consistent implementation would be to pass the index instead, especially for stores like FAISS.
Let me know your thoughts.
Usecase:

For caching/long-term purposes
Using in a serverless environment, etc",3,2
569,2023-01-10T01:04:09Z,2023-01-10T03:13:00Z,2023-01-10T03:13:00Z,1,4,1,Sorry for the detail. this is a correction to the docstring.,2,0
570,2023-01-10T01:46:27Z,2023-01-10T03:12:35Z,2023-01-10T03:12:35Z,1,1,1,"I found a typo, which might be important for a conversational Agent.
if My PR is wrong, I am so sorry",2,0
575,2023-01-10T12:51:25Z,2023-01-11T13:54:20Z,2023-01-11T13:54:20Z,1,44,0,"feature: Helper class for combining multiple memories
Usage:
conv_memory = ConversationBufferWindowMemory(
    memory_key=""chat_history"",
    input_key=""input"",
    output_key=""bot_output"",
    k=5,
    ai_prefix=ai_prefix,
    human_prefix=human_prefix,
)

additional_info_memory = AdditionalInfoMemory(
    memory_key=""additional_info"",
    read_key=""output"",
    include_intermediate_steps=True,
)

# Combined
memory = CombinedMemory(memories=[conv_memory, additional_info_memory])",2,0
578,2023-01-10T16:59:53Z,2023-01-11T03:07:00Z,2023-01-11T03:07:00Z,11,737,413,"Issue linked with: #455
Added Wolfram Alpha as a tool.",3,0
588,2023-01-12T00:10:22Z,2023-01-12T04:18:29Z,2023-01-12T04:18:29Z,1,55,7,"Add namespace param to similarity_search and add_texts methods to they can be called on specific pinecone namespaces, add similarity_search_with_score method to have pinecone return scores, make from_text use uuids for pinecone ids instead of list indices.",3,3
599,2023-01-12T20:22:40Z,2023-01-13T02:16:55Z,2023-01-13T02:16:55Z,1,38,0,Motivation is that these don't get lost in the Twitterverse!,2,0
607,2023-01-13T14:42:28Z,2023-01-13T15:48:00Z,2023-01-13T15:48:00Z,1,1,1,,2,0
609,2023-01-13T15:47:44Z,2023-01-14T01:36:10Z,2023-01-14T01:36:10Z,1,13,7,,2,1
610,2023-01-13T19:13:09Z,2023-01-14T15:23:48Z,2023-01-14T15:23:48Z,2,73,5,"I originally had only modified the from_llm to include the prompt but I realized that if the prompt keys used on the custom prompt didn't match the default prompt, it wouldn't work because of how apply works.
So I made some changes to the evaluate method to check if the prompt is the default and if not, it will check if the input keys are the same as the prompt key and update the inputs appropriately.
Let me know if there is a better way to do this.
Also added the custom prompt to the QA eval notebook.",3,1
611,2023-01-13T19:56:23Z,2023-01-14T01:31:33Z,2023-01-14T01:31:33Z,1,1,1,,2,0
612,2023-01-13T20:06:01Z,2023-01-14T15:22:52Z,2023-01-14T15:22:52Z,3,88,8,,2,0
620,2023-01-15T12:26:52Z,2023-01-16T00:40:12Z,2023-01-16T00:40:12Z,1,2,1,,2,0
623,2023-01-16T01:30:12Z,2023-01-16T02:34:44Z,2023-01-16T02:34:44Z,6,263,6,,2,0
627,2023-01-16T10:14:05Z,2023-01-18T05:50:50Z,2023-01-18T05:50:50Z,1,12,5,,2,2
629,2023-01-16T16:11:50Z,2023-01-17T06:45:15Z,2023-01-17T06:45:15Z,1,1,1,,2,0
631,2023-01-16T16:33:08Z,2023-01-18T06:12:51Z,2023-01-18T06:12:51Z,2,20,4,"Added a comment interpreting regex for ZeroShotAgent
Added a note to the Custom Agent notebook",2,0
636,2023-01-17T19:15:39Z,2023-01-18T06:17:50Z,2023-01-18T06:17:50Z,1,1,1,there is a small typo in one of the docs.,2,0
637,2023-01-18T03:45:52Z,2023-01-18T06:26:07Z,2023-01-18T06:26:07Z,1,1,1,"Running the Cohere embeddings example from the docs:
from langchain.embeddings import CohereEmbeddings
embeddings = CohereEmbeddings(cohere_api_key= cohere_api_key)

text = ""This is a test document.""
query_result = embeddings.embed_query(text)
doc_result = embeddings.embed_documents([text])
I get the error:
CohereError(message=res['message'], http_status=response.status_code, headers=response.headers)      
cohere.error.CohereError: embed is not an available endpoint on this model
This is because the model string is set to medium which is not currently available.
From the Cohere docs:

Currently available models are small and large (default)",2,0
651,2023-01-18T23:11:11Z,2023-01-19T15:05:20Z,2023-01-19T15:05:20Z,1,8,8,"tl;dr: input -> word, output -> antonym, rename to dynamic_prompt consistently
The provided code in this example doesn't run, because the keys are word and antonym, rather than input and output.
Also, the ExampleSelector-based prompt is named few_shot_prompt when defined and dynamic_prompt in the follow-up example. The former name is less descriptive and collides with an earlier example, so I opted for the latter.
Thanks for making a really cool library!",2,0
652,2023-01-18T23:23:27Z,,2023-08-08T14:37:46Z,9,498,0,"Functionality:

 Create event
 View event
 View events
 Delete event
 Reschedule event

Docs:

 Google calendar, create event example
 Google calendar ecosystem (not using load_tools yet)

Tests:

 Simple test for creating event

I removed from load_tools for now, cause not sure if belongs there yet.
Instructions:

Follow instructions here: https://developers.google.com/calendar/api/quickstart/python
When you run for the first time, you will need to authenticate OAuth w/ Google",9,9
659,2023-01-19T23:54:23Z,2023-01-20T14:50:04Z,2023-01-20T14:50:04Z,1,10,5,"Allow optionally specifying a list of ids for pinecone rather than having them randomly generated.
This also permits editing the embedding/metadata of existing pinecone entries, by id.",3,2
660,2023-01-20T04:56:33Z,2023-01-20T15:37:01Z,2023-01-20T15:37:01Z,3,3,3,"Adding quotation marks around {text} avoids generating empty or completely random responses from OpenAI davinci-003. Empty or completely unrelated intermediate responses in summarization messes up the final result or makes it very inaccurate.
The error from OpenAI would be: ""The model predicted a completion that begins with a stop sequence, resulting in no output. Consider adjusting your prompt or stop sequences.""
This fix corrects the prompting for summarization chain. This works on API too, the images are for demonstrative purposes.
This approach can be applied to other similar prompts too.
Examples:


Without quotation marks



With quotation marks",2,0
662,2023-01-20T12:48:40Z,2023-01-20T15:44:03Z,2023-01-20T15:44:03Z,5,996,340,"Full support of Qdrant as a vector store, including MRR. Integration tests are included.",2,2
668,2023-01-20T22:27:50Z,2023-01-21T23:51:50Z,2023-01-21T23:51:50Z,2,40,18,Wolfram and google-search tools should allow passing in kwargs via load_tools,2,0
671,2023-01-21T02:44:18Z,2023-01-25T05:36:20Z,2023-01-25T05:36:20Z,14,578,14,,3,0
672,2023-01-21T03:01:38Z,2023-01-21T23:55:59Z,2023-01-21T23:55:59Z,1,9,0,"You may want to add doi/orcid
Followed this: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files",2,0
675,2023-01-21T16:25:50Z,2023-01-21T23:56:56Z,2023-01-21T23:56:56Z,4,136,7,"The examples I want to load are long-ish passages with plenty of characters that weren't working with JSON formatting, so I added support for loading examples in a few-shot prompt template in yaml format.
I also added examples to the prompt serialization section of the docs to show people how to do this. The YAML formatting is important so that it gets loaded correctly as a list for the FewShotPromptTemplate to be happy.
I was having env issues so I didn't run all of the contributing tests, but I don't think this is a huge update. It just made my QOL better -- I want my non-technical colleagues to be able to provide examples that I can select dynamically and YAML seemed like a better copy+paste option to teach them.",2,0
676,2023-01-21T16:51:48Z,2023-01-22T00:08:15Z,2023-01-22T00:08:15Z,2,54,11,"This uses the faiss built-in write_index and load_index to save and load faiss indexes locally
Also fixes #674
The save/load functions also use the faiss library, so I refactored the dependency into a function",2,0
677,2023-01-21T17:34:02Z,2023-01-22T00:03:21Z,2023-01-22T00:03:21Z,1,1,1,"The current link points to a non-existent page. I've updated the link to match what is on the ""Create a custom example selector"" page.",2,1
678,2023-01-21T23:10:11Z,2023-01-22T18:10:02Z,2023-01-22T18:10:03Z,5,638,1,"…marization prompt to maintain a key-value store of memory information
cc @devennavani",2,0
683,2023-01-22T01:12:09Z,2023-01-22T20:44:14Z,2023-01-22T20:44:14Z,17,184,169,,2,0
684,2023-01-22T01:39:45Z,2023-01-23T07:37:01Z,2023-01-23T07:37:01Z,5,285,1,,2,0
685,2023-01-22T03:48:00Z,2023-01-22T22:49:25Z,2023-01-22T22:49:26Z,3,7,2,"On the Getting Started page for prompt templates, I believe the very last example
print(dynamic_prompt.format(adjective=long_string))
should actually be
print(dynamic_prompt.format(input=long_string))
The existing example produces KeyError: 'input' as expected

On the Create a custom prompt template page, I believe the line
Function Name: {kwargs[""function_name""]}
should actually be
Function Name: {kwargs[""function_name""].__name__}
The existing example produces the prompt:
        Given the function name and source code, generate an English language explanation of the function.
        Function Name: <function get_source_code at 0x7f907bc0e0e0>
        Source Code:
        def get_source_code(function_name):
    # Get the source code of the function
    return inspect.getsource(function_name)

        Explanation:


On the Example Selectors page, the first example does not define example_prompt, which is also subtly different from previous example prompts used. For user convenience, I suggest including
example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}"",
)
in the code to be copy-pasted",2,1
688,2023-01-22T15:19:38Z,2023-01-22T16:21:32Z,2023-01-22T16:21:32Z,1,1,1,therefor -> therefore,2,0
694,2023-01-22T20:52:15Z,2023-01-22T22:48:21Z,2023-01-22T22:48:21Z,2,5,2,This PR aims to move the contents of .coveragerc to pyproject.toml to make the overall file structure more minimal.,2,1
696,2023-01-22T22:54:40Z,2023-01-23T07:38:47Z,2023-01-23T07:38:47Z,1,5,5,"Since the tokenizer and model are constructed manually, model_kwargs needs to
be passed to their constructors.  Additionally, the pipeline has a specific
named parameter to pass these with, which can provide forward compatibility if
they are used for something other than tokenizer or model construction.",2,0
704,2023-01-23T22:44:21Z,2023-01-24T07:06:23Z,2023-01-24T07:06:23Z,3,19,1,,2,0
705,2023-01-23T22:56:23Z,2023-01-24T07:08:38Z,2023-01-24T07:08:38Z,1,1,1,,2,1
706,2023-01-23T23:57:54Z,2023-01-24T07:06:54Z,2023-01-24T07:06:54Z,1,1,1,,2,0
707,2023-01-24T02:56:06Z,2023-01-24T06:49:00Z,2023-01-24T06:49:00Z,2,16,1,,2,0
708,2023-01-24T04:12:11Z,,2023-06-23T13:09:04Z,5,207,4,"Adding retry functionality for SQLDatabaseChain when compile/runtime errors are encountered. For now, this only includes InvalidRequestErrors.
@andersenchen @Pablongo24",4,5
713,2023-01-24T10:33:23Z,2023-01-24T15:01:07Z,2023-01-24T15:01:07Z,1,5,2,"I'm providing a hotfix for Qdrant integration. Calculating a single embedding to obtain the vector size was great idea. However, that change introduced a bug trying to put only that single embedding into the database. It's fixed. Right now all the embeddings will be pushed to Qdrant.",2,1
714,2023-01-24T10:39:22Z,2023-01-24T15:01:41Z,2023-01-24T15:01:41Z,1,2,2,"SentenceTransformer returns a NumPy array, not a List[List[float]] or List[float] as specified in the interface of Embeddings. That PR makes it consistent with the interface.",2,0
715,2023-01-24T13:12:51Z,2023-01-24T15:06:50Z,2023-01-24T15:06:50Z,1,1,1,"otherwise @validator(""input_variables"") do not work",2,0
719,2023-01-24T14:35:51Z,2023-01-24T18:56:15Z,2023-01-24T18:56:16Z,1,1,1,,2,0
720,2023-01-24T17:03:33Z,2023-01-24T18:59:23Z,2023-01-24T18:59:23Z,2,10,4,Fix typos in readme and TextSplitter documentation.,2,0
728,2023-01-25T04:27:49Z,2023-01-25T06:23:32Z,2023-01-25T06:23:32Z,7,34,14,,2,0
731,2023-01-25T05:58:51Z,2023-02-07T04:02:19Z,2023-02-07T04:02:19Z,3,215,1,"add analyze document chain, which does text splitting and then analysis",2,0
733,2023-01-25T07:33:44Z,2023-01-25T15:14:08Z,2023-01-25T15:14:08Z,5,6,7,This has been bugging me when running my own tests that call langchain methods :P,2,0
735,2023-01-25T15:38:47Z,2023-01-25T17:39:51Z,2023-01-25T17:39:51Z,1,3,2,If distance_func and collection_name are in kwargs they are sent to the QdrantClient which results in an error being raised.,2,0
736,2023-01-25T17:07:02Z,,2023-01-27T16:36:53Z,5,1437,10,The agents usually benefit from understanding what the data looks like to be able to filter effectively. Sending just one row in the table info allows the agent to understand the data before querying and get better results.,2,1
738,2023-01-25T18:54:18Z,2023-01-27T03:37:31Z,2023-01-27T03:37:31Z,2,108,82,Added type information to crawler.py to make it safer to use and understand.,3,1
739,2023-01-25T20:30:18Z,2023-01-27T03:43:01Z,2023-01-27T03:43:01Z,1,28,1,"Referring to #687, I implemented the functionality to reduce K if it exceeds the token limit.
Edit: I should have ran make lint locally. Also, this only applies to StuffDocumentChain",5,3
740,2023-01-25T23:27:00Z,2023-01-26T01:47:29Z,2023-01-26T01:47:29Z,1,27,0,,3,0
741,2023-01-26T00:21:35Z,2023-01-27T01:38:13Z,2023-01-27T01:38:13Z,23,1422,71,"add implementations of BaseCallbackHandler to support tracing: SharedTracer which is thread-safe and Tracer which is not and is meant to be used locally.
Tracers persist runs to locally running langchain-server",2,0
748,2023-01-26T16:27:14Z,2023-01-28T16:05:35Z,2023-01-28T16:05:35Z,1,3,3,text-davinci-003 supports a context size of 4097 tokens so return 4097 instead of 4000 in modelname_to_contextsize() for text-davinci-003,3,0
749,2023-01-26T17:16:48Z,,2023-02-03T03:57:12Z,2,61,3,"The OpenAI completion API can fail due to various reasons including network errors or more commonly their RateLimitError, or ServiceUnavailableError.
This adds a retry() decorator that implements an exponential backoff retry strategy and uses that decorator to wrap the openai create() call in the normal non-streaming use case.
The default behavior of the retry decorator is to retry 5 times, with an initial delay of 0.5 seconds with subsequent tries doubling the delay.",5,2
750,2023-01-26T17:26:14Z,2023-01-28T16:10:52Z,2023-01-28T16:10:52Z,1,51,16,"Allow HuggingFace pipeline to use local GPUs if available.
If transformers is installed but torch is not, the existing ImportError still insisted that transformers is not installed.",3,0
754,2023-01-26T23:42:03Z,2023-01-27T03:47:01Z,2023-01-27T03:47:01Z,1,1,1,"Some custom agents might continue to iterate until they find the correct answer, getting stuck on loops that generate request after request and are really expensive for the end user. Putting an upper bound for the number of iterations
by default controls this and can be explicitly tweaked by the user if necessary.",2,0
761,2023-01-27T07:30:43Z,2023-01-27T08:45:18Z,2023-01-27T08:45:18Z,17,478,21,,2,0
765,2023-01-27T13:57:21Z,2023-01-28T16:05:20Z,2023-01-28T16:05:20Z,2,26,4,Please feel free to disregard any changes you disagree with,2,0
766,2023-01-27T14:23:43Z,2023-01-28T16:20:26Z,2023-01-28T16:20:26Z,1,27,16,The base agent class seems a bit opinionated right now. It took a bit of effort to experiment with the effects of swapping out default ZeroShotAgent logic with #764. These are some suggestions to make agent experimentation easier.,2,0
768,2023-01-27T15:29:12Z,2023-01-28T15:26:27Z,2023-01-28T15:26:28Z,1,1,1,"Mini drive-by PR:
I came across this sentence in a stack trace for an error I had, and it confused me because the verb I missing. So I added the verb.",2,0
771,2023-01-27T18:33:01Z,,2023-02-02T16:49:38Z,2,64,12,Add new Instructor model to embeddings,3,6
774,2023-01-28T04:19:19Z,2023-02-02T16:34:24Z,2023-02-02T16:34:24Z,1,4,2,"Passing additional variables to the python environment can be useful for example if you want to generate code to analyze a dataset.
I also added a tracker for the executed code - code_history.",2,5
787,2023-01-28T19:44:30Z,2023-01-28T21:34:16Z,2023-01-28T21:34:16Z,1,5,0,,2,0
795,2023-01-29T13:42:47Z,2023-01-30T22:55:45Z,2023-01-30T22:55:45Z,1,2,2,"When stop tokens are set in Cohere LLM constructor, they are currently not stripped from the response, and they should be stripped",2,1
797,2023-01-29T17:27:05Z,2023-01-30T22:55:08Z,2023-01-30T22:55:08Z,1,1,1,"Currently, the class parameter 'model_name' of the CohereEmbeddings class is not supported, but 'model' is. The class documentation is inconsistent with this, though, so I propose to either fix the documentation (this PR right now) or fix the parameter.
It will create the following error:
ValidationError: 1 validation error for CohereEmbeddings
model_name
  extra fields not permitted (type=value_error.extra)",2,0
798,2023-01-29T18:26:03Z,2023-02-01T15:09:04Z,2023-02-01T15:09:04Z,1,9,2,"Currently, the 'truncate' parameter of the cohere API is not supported.
This means that by default, if trying to generate and embedding that is too big, the call will just fail with an error (which is frustrating if using this embedding source e.g. with GPT-Index, because it's hard to handle it properly when generating a lot of embeddings).
With the parameter, one can decide to either truncate the START or END of the text to fit the max token length and still generate an embedding without throwing the error.
In this PR, I added this parameter to the class.
Arguably, there should be a better way to handle this error, e.g. by optionally calling a function or so that gets triggered when the token limit is reached and can split the document or some such. Especially in the use case with GPT-Index, its often hard to estimate the token counts for each document and I'd rather sort out the troublemakers or simply split them than interrupting the whole execution.
Thoughts?",2,0
799,2023-01-29T20:56:02Z,2023-01-30T22:54:09Z,2023-01-30T22:54:09Z,2,29,1,"Problem
I noticed that in order to change the prefix of the prompt in the zero-shot-react-description agent
we had to dig around to subset strings deep into the agent's attributes. It requires the user to inspect a long chain of attributes and classes.
initialize_agent ->  AgentExecutor -> Agent  -> LLMChain -> Prompt  from  Agent.create_prompt
agent = initialize_agent(
    tools=tools,
    llm=fake_llm,
    agent=""zero-shot-react-description""
)
prompt_str = agent.agent.llm_chain.prompt.template
new_prompt_str = change_prefix(prompt_str)
agent.agent.llm_chain.prompt.template = new_prompt_str
Implemented Solution
initialize_agent accepts **kwargs but passes it to AgentExecutor but not ZeroShotAgent, by simply giving the kwargs to the agent class methods we can support changing the prefix and suffix for one agent while allowing future agents to take advantage of initialize_agent.
agent = initialize_agent(
    tools=tools,
    llm=fake_llm,
    agent=""zero-shot-react-description"",
    agent_kwargs={""prefix"": prefix, ""suffix"": suffix}
)

To be fair, this was before finding docs around custom agents here: https://langchain.readthedocs.io/en/latest/modules/agents/examples/custom_agent.html?highlight=custom%20#custom-llmchain but i find that my use case just needed to change the prefix a little.
Changes

Pass kwargs to Agent class method
Added a test to check suffix and prefix",2,0
805,2023-01-30T03:18:34Z,2023-01-30T22:52:17Z,2023-01-30T22:52:17Z,9,201,68,"It's generally considered to be a good practice to pin dependencies to prevent surprise breakages when a new version of a dependency is released. This commit adds the ability to pin dependencies when loading from LangChainHub.
Centralizing this logic and using urllib fixes an issue identified by some windows users highlighted in this video - https://youtu.be/aJ6IQUh8MLQ?t=537",2,1
808,2023-01-30T06:37:48Z,2023-01-30T22:48:13Z,2023-01-30T22:48:13Z,1,3,3,,4,4
812,2023-01-31T00:37:19Z,2023-02-02T16:52:14Z,2023-02-02T16:52:14Z,3,401,3,"Add ngram overlap example selector #628
Add corresponding unit tests
Add jupyter notebook doc
First time contributing to an open source project -- appreciate any feedback.
Thanks!",2,1
816,2023-01-31T06:48:46Z,2023-02-03T03:55:13Z,2023-02-03T03:55:13Z,2,51,1,"This does not involve a separator, and will naively chunk input text at the appropriate boundaries in token space.
This is helpful if we have strict token length limits that we need to strictly follow the specified chunk size, and we can't use aggressive separators like spaces to guarantee the absence of long strings.
CharacterTextSplitter will let these strings through without splitting them, which could cause overflow errors downstream.
Splitting at arbitrary token boundaries is not ideal but is hopefully mitigated by having a decent overlap quantity. Also this results in chunks which has exact number of tokens desired, instead of sometimes overcounting if we concatenate shorter strings.
Potentially also helps with #528.",2,0
820,2023-01-31T08:25:06Z,2023-02-02T16:23:54Z,2023-02-02T16:23:54Z,1,13,0,,2,0
824,2023-02-01T08:36:47Z,2023-02-01T15:10:15Z,2023-02-01T15:10:15Z,1,11,0,,2,0
825,2023-02-01T09:27:40Z,2023-02-02T07:32:36Z,2023-02-02T07:32:36Z,1,1,1,,2,0
827,2023-02-01T15:01:20Z,2023-02-02T07:31:38Z,2023-02-02T07:31:38Z,1,0,1,Remove duplicate APIChain,2,0
835,2023-02-02T00:12:25Z,2023-02-02T07:38:36Z,2023-02-02T07:38:36Z,1,6,0,"This PR introduces a new template for deploying LangChain apps as web endpoints. It includes template code, and links to a detailed code-walkthrough.",2,0
837,2023-02-02T04:30:29Z,2023-02-07T05:52:38Z,2023-02-07T05:52:38Z,6,212,6,,4,0
841,2023-02-02T07:10:08Z,2023-02-08T05:21:57Z,2023-02-08T05:21:57Z,21,1695,205,"Supporting asyncio in langchain primitives allows for users to run them concurrently and creates more seamless integration with asyncio-supported frameworks (FastAPI, etc.)
Summary of changes:
LLM

Add agenerate and _agenerate
Implement in OpenAI by leveraging client.Completions.acreate

Chain

Add arun, acall, _acall
Implement them in LLMChain and LLMMathChain for now

Agent

Refactor and leverage async chain and llm methods
Add ability for Tools to contain async coroutine
Implement async SerpaPI arun

Create demo notebook.
Open questions:

Should all the async stuff go in separate classes? I've seen both patterns (keeping the same class and having async and sync methods vs. having class separation)",4,2
842,2023-02-02T09:54:55Z,2023-02-02T16:23:39Z,2023-02-02T16:23:39Z,1,1,1,seperator -> separator,2,0
849,2023-02-02T19:39:51Z,2023-02-03T03:56:26Z,2023-02-03T03:56:26Z,3,59,3,"add ability to retry when certain exceptions are raised by openai.Completions.create
Test plan: ran all OpenAI integration tests.",2,0
871,2023-02-03T10:51:25Z,,2023-05-09T23:01:03Z,40,1309,88,"The guard directive makes it easy to add a protective step on top of a llm chain.
For example:
@Guard(restrictions=['must not talk about politics or political figures'], llm=llm, retries=1)
def call_chain():
    return chain.run(adjective=""political"")

This use of @guard will ask the provided llm to check if the output of chain.run violates the restriction provided. If it does, it will retry once before throwing an error.
While writing this directive I also had to write a boolean normalization function so included that as well though normalization functions to translate llm responses into json, lists, etc would be a great feature to add as well in the future.
Still need to make some additions to the docs and test on agents not just chains. Boolean normalization is fully tested.",5,14
877,2023-02-03T22:02:30Z,2023-02-07T02:18:52Z,2023-02-07T02:18:52Z,3,94,45,"#782 added the ability to include a sample row of a table to the prompt to provide more context for the model. As demonstrated in this paper, performance increases with the number of rows provided, up to 3 in their tests. This PR builds on #782 to allow for selecting the number of rows to include.
Also, a few of the outputs in the sqlite example notebook were incorrect for the chinook db - there are 8 entries in the employee table, not 9.",2,2
880,2023-02-04T06:50:31Z,2023-02-07T02:36:18Z,2023-02-07T02:36:18Z,1,27,8,Fixes issues #789 and #853 by saving/loading docstore and index_to_docstore_id alongside index,3,0
881,2023-02-04T09:36:01Z,,2023-08-08T14:40:08Z,4,152,14,"As you all know, one of the big issue with LLM is the output form (type / format / syntax...).
I propose to add a new chain, LLMChainWithValidator, that accept a validator, a function that try parse the output / validate it's syntax, and return the appropriate type.
Here are example of possible validator:

boolean: ""true"" -> True
age: ""34"" -> 34  (parse int + check 0 < 130)
data structure
json
SQL
...

The chain also accept two others parameters: correct_on_error and retry.

correct_on_error try to correct the output with the LLM
retry is the number of possible retry of the LLM

The chain return None if it didn't succeed to return a validated output.
Some notes:

I initially wanted to add validator directly in LLMChain, but I figured that it was probably best to start with a simple custom chain.
So far, this implementation doesn't handle multiple call / responses. It added too much complexities due to the retry / correct logic.


Examples
import os

from langchain.chains.llm_validator import VALIDATORS, LLMChainWithValidator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

llm = OpenAI()

template = """"""What is the price of {product}?""""""
prompt = PromptTemplate(
    template=template,
    input_variables=[""product""],
)
chain = LLMChainWithValidator(
    llm=llm,
    prompt=prompt,
    output_key=""response"",
    validator=int,
    retry=2,
    correct_on_error=True,
)
chain(
    {
        ""product"": ""an iphone"",
    }
)
# > {'product': 'an iphone', 'response': 699}
prompt = PromptTemplate(
    template=""generate a json of top 3 french cities"",
    input_variables=[]
)    
chain = LLMChainWithValidator(
    llm=llm,
    prompt=prompt,
    output_key=""response"",
    validator=VALIDATORS[""json""],
    retry=2,
    correct_on_error=True
)
chain({})
# > {'response': {'Top 5 French Cities': [{'City': 'Paris', 'Population': 2244000},{'City': 'Marseille', 'Population': 861635},{'City': 'Lyon', 'Population': 495268}]}}",3,3
882,2023-02-04T14:38:29Z,2023-02-04T17:45:20Z,2023-02-04T17:45:20Z,1,11,0,,2,0
886,2023-02-05T00:36:22Z,2023-02-05T06:49:17Z,2023-02-05T06:49:17Z,1,7,0,"Was passing prompt in directly as string and getting nonsense outputs. Had to inspect source code to realize that first arg should be a list. Could be nice if there was an explicit error or warning, seems like this could be a common mistake.",2,0
890,2023-02-05T01:09:34Z,2023-02-11T07:26:55Z,2023-02-11T07:26:55Z,14,44,52,"some other versions from the class. I.e. delete the FakeLLM classes in

tests/unit_tests/chains/test_hyde.py
tests/unit_tests/chains/test_natbot.py

and make them use the central one.
Risks
I changed the definition of FakeLLM to have a kind of union between the functionality in:

the original
the one in tests/unit_tests/chains/test_hyde.py
the one in tests/unit_tests/chains/test_natbot.py

But is this ok? Will all consumers of this class be ok with these changes?
Testing and validation

The unit tests pass: python3 -m pytest tests/unit_tests for running unit tests in langchain`.
But integration tests: I can't run them. Current blocker there is a Google API key.

cc: @hwchase17, @hinthornw",2,1
892,2023-02-05T02:41:43Z,2023-02-05T04:41:34Z,2023-02-05T04:41:34Z,1,1,1,,2,0
894,2023-02-05T12:50:41Z,2023-02-05T23:42:54Z,2023-02-05T23:42:54Z,2,18,2,"This allows the LLM to correct its previous command by looking at the error message output to the shell.
Additionally, this uses subprocess.run because that is now recommended over subprocess.check_output: https://docs.python.org/3/library/subprocess.html#using-the-subprocess-module",2,1
898,2023-02-05T18:35:39Z,2023-02-05T23:21:56Z,2023-02-05T23:21:56Z,1,4,1,"PR to fix outdated environment details in the docs, see issue #897
I added code comments as pointers to where users go to get API keys, and where they can find the relevant environment variable.",2,0
901,2023-02-06T04:51:47Z,,2023-02-09T20:19:13Z,9,362,6,"Add GooseAI integration, Documentation, and tests. Hopefully, I am not missing anything.
Usage:
import os
from langchain.llms import GooseAI
from langchain import PromptTemplate, LLMChain

os.environ[""GOOSEAI_API_KEY""] = """"

llm = GooseAI(max_tokens=10)

template = """"""Question: {question}

Answer: Let's think step by step.""""""

prompt = PromptTemplate(template=template, input_variables=[""question""])

llm_chain = LLMChain(prompt=prompt, llm=llm)

question = ""What NFL team won the Super Bowl in the year Justin Beiber was born?""

print(llm_chain.run(question))",11,4
904,2023-02-06T07:01:37Z,,2023-03-14T08:12:47Z,0,0,0,"Added setup_index to the interface VectorStore for the procedure of creating and setting up the data schema
Added DataSchemaBuilder as a base interface for all vectorStores
Added a special data_schema_builder for elasticsearch for creating index mapping in elasticsearch
Added ElasticConf. A context to be passed to ElasticVectorStore to pass the credential context to the object (Not tested properly yet)
Added similarity_search_by_id and similarity_search_by_vector, this basically seperates the logic in similar_search more accurately in methods
Added VectorStoreFilter, a functionality to filter the metadata before ANN indexing
incorporated query filter instead of default elasticsearch filter
implemented concrete class ElasticFilter

close #834",4,5
907,2023-02-06T07:54:29Z,2023-02-06T20:45:56Z,2023-02-06T20:45:56Z,1,3,3,"Fix for issue #906
Switches [i : i + batch_size] to [i : i_end] in Pinecone from_texts method",2,0
909,2023-02-06T15:30:29Z,2023-02-16T06:47:18Z,2023-02-16T06:47:18Z,11,489,47,"Adds Google Search integration with Serper a low-cost alternative to SerpAPI (10x cheaper + generous free tier). Includes documentation, tests and examples. Hopefully I am not missing anything.
Developers can sign up for a free account at serper.dev and obtain an api key.
Usage
from langchain.utilities import GoogleSerperAPIWrapper
from langchain.llms.openai import OpenAI
from langchain.agents import initialize_agent, Tool

import os
os.environ[""SERPER_API_KEY""] = """"
os.environ['OPENAI_API_KEY'] = """"

llm = OpenAI(temperature=0)
search = GoogleSerperAPIWrapper()
tools = [
    Tool(
        name=""Intermediate Answer"",
        func=search.run
    )
]

self_ask_with_search = initialize_agent(tools, llm, agent=""self-ask-with-search"", verbose=True)
self_ask_with_search.run(""What is the hometown of the reigning men's U.S. Open champion?"")
Output
Entering new AgentExecutor chain...
 Yes.
Follow up: Who is the reigning men's U.S. Open champion?
Intermediate answer: Current champions Carlos Alcaraz, 2022 men's singles champion.
Follow up: Where is Carlos Alcaraz from?
Intermediate answer: El Palmar, Spain
So the final answer is: El Palmar, Spain

> Finished chain.

'El Palmar, Spain'",2,2
924,2023-02-07T08:20:08Z,2023-02-10T07:16:42Z,2023-02-10T07:16:42Z,1,35,6,"The PR allows for allowed_special and disallowed_special parameters to be used (see issue #923 ). The default parameters for these are set() and ""all"" respectively as per the code.
The reason this is needed is because when a GPT special token appears in some text to be encoded, an error will be raised (see issue #923 ) - using these special token params is the only way to get around it.
Also added the same functionality for the TokenTextSplitter, so now this will work:
from langchain.text_splitter import TokenTextSplitter

text_splitter = TokenTextSplitter.from_tiktoken_encoder(
    encoding_name=encoder_name,
    chunk_size=300,
    chunk_overlap=50
)
text_splitter.split_text(
    some_text, 
    disallowed_special=()
)",2,1
932,2023-02-07T19:05:51Z,2023-02-10T07:15:41Z,2023-02-10T07:15:41Z,1,6,16,"-Address TODO: deprecate for sample_row_in_table_info
-Simplify set operations by casting to sets to not need multiple set casts + .difference() calls",2,0
934,2023-02-07T23:13:15Z,2023-02-08T19:13:36Z,2023-02-08T19:13:36Z,1,1,1,,2,0
937,2023-02-08T04:34:07Z,2023-02-10T07:10:58Z,2023-02-10T07:10:58Z,2,441,7,A few of the extra dependencies were not getting installed by poetry - they needed to have entries in [tool.poetry.dependencies] with optional = true.,2,0
938,2023-02-08T06:09:06Z,2023-02-08T19:58:07Z,2023-02-08T19:58:07Z,4,114,0,,2,0
942,2023-02-08T16:33:13Z,2023-02-09T15:52:51Z,2023-02-09T15:52:51Z,10,504,0,,2,0
943,2023-02-08T16:54:38Z,2023-02-09T00:01:08Z,2023-02-09T00:01:08Z,1,5,1,"Sometimes, the docs may be empty. For example for the  text = soup.find_all(""main"", {""id"": ""main-content""}) was an empty list. To cater to these edge cases, the clean function needs to be checked if it is empty or not.",2,0
944,2023-02-08T17:34:19Z,2023-02-08T19:05:29Z,2023-02-08T19:05:29Z,1,2,2,HuggingFace -> Hugging Face,2,0
952,2023-02-09T05:03:37Z,2023-02-09T15:53:53Z,2023-02-09T15:53:53Z,1,33,3,"Hi team,
Love the work you guys have done on LangChain, hope i can also contribute a tiny bit!
have done similar side project on Youtube summarization and question answering with LLM,
I find that Youtube video meta infor, especially video title and descriptions are quite helpful for model to better understand the context.
Additionally, the thumbnail image url can also be useful when build web app with this.
a sample video meta infor would be
{
'source': 'I845O57ZSy4',
 'title': 'John Carmack: Doom, Quake, VR, AGI, Programming, Video Games, and Rockets | Lex Fridman Podcast #309',
 'description': 'John Carmack is a legendary programmer, co-founder of id Software, and lead programmer of many revol...',
 'view_count': 1229347,
 'thumbnail_url': 'https://i.ytimg.com/vi/I845O57ZSy4/sddefault.jpg',
 'publish_date': datetime.datetime(2022, 8, 4, 0, 0),
 'length': 18890,
 'author': 'Lex Fridman',
}",2,0
959,2023-02-10T00:05:35Z,,2023-02-11T02:08:14Z,1,1,1,,2,0
961,2023-02-10T03:13:24Z,2023-02-16T06:50:00Z,2023-02-16T06:50:00Z,4,129,8,Alternate implementation to PR #960 Again - only FAISS is implemented. If accepted can add this to other vectorstores or leave as NotImplemented? Suggestions welcome...,2,1
965,2023-02-10T07:29:03Z,,2023-05-09T23:05:54Z,4,367,1,,7,17
968,2023-02-10T10:24:01Z,2023-02-10T14:57:50Z,2023-02-10T14:57:51Z,2,8,3,"In real-life scenarios, we do indeed need to make slight adjustments to the text of ""format_instructions"", making the user experience friendlier and more closely aligned with specific scenarios.",2,0
969,2023-02-10T11:02:11Z,2023-02-10T14:49:16Z,2023-02-10T14:49:16Z,1,8,6,"Previously the OpenAI embedding class was making one call to the OpenAI API for each document, but this is much slower than it needs to be and also prone to getting rate-limited and or getting a ""come back later"" response. OpenAI's API is not well documented, but I think this is the intended use. Their example of usage for a single doc was a bit confusing. This PR fixes that problem and it also works for large numbers of documents.",3,1
979,2023-02-10T17:05:14Z,2023-02-10T18:18:39Z,2023-02-10T18:18:39Z,3,112,0,"Summary
Adds a UnstructuredURLLoader that supports loading data from a list of URLs.
Testing
from langchain.document_loaders import UnstructuredURLLoader

urls = [
    ""https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023"",
    ""https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023""
]
loader = UnstructuredURLLoader(urls=urls)
raw_documents = loader.load()",4,4
981,2023-02-10T18:35:15Z,2023-02-14T05:20:19Z,2023-02-14T05:20:19Z,20,1283,1,"Add GooseAI, CerebriumAI, Petals, ForefrontAI",3,4
984,2023-02-10T20:00:50Z,2023-02-10T23:42:30Z,2023-02-10T23:42:30Z,4,163,0,,3,1
988,2023-02-11T00:37:33Z,2023-02-11T01:56:16Z,2023-02-11T01:56:16Z,1,2,2,"The provided example uses the default max_length of 20 tokens, which leads to the example generation getting cut off. 20 tokens is way too short to show CoT reasoning, so I boosted it to 64.
Without knowing HF's API well, it can be hard to figure out just where those model_kwargs come from, and max_length is a super critical one.",2,0
992,2023-02-11T08:57:15Z,2023-02-13T19:59:33Z,2023-02-13T19:59:33Z,15,771,86,Async support for all combine docs chains and chatvectordb chain,2,0
993,2023-02-11T09:01:48Z,2023-02-16T06:57:25Z,2023-02-16T06:57:25Z,1,25,5,"This PR adds an argument input_keys for semantic search. If provided, the search is based on the input variables instead of all variables.
To be consistent with the previous versions, input_keys is optional in this PR. However, explicitly specifying input_keys should be encouraged since the output variables are not expected to be used to create the search index.",3,1
995,2023-02-11T14:41:34Z,,2023-03-23T01:24:19Z,5,138,3,"I implemented saving and loading conversation chain with memory:
from langchain.llms import OpenAI
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferMemory

llm = OpenAI(temperature=0)
conversation = ConversationChain(
    llm=llm, 
    verbose=True, 
    memory=ConversationBufferMemory()
)

conversation.predict(input=""Hi there! My name is Ibis Prevedello"")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there! My name is Ibis Prevedello
AI:

> Finished chain.
"" Hi Ibis Prevedello! It's nice to meet you. My name is AI-1. What can I do for you today?""

Saving the chain!
conversation.save(""conversation_test.json"")
Loading the chain!
from langchain.chains.loading import load_chain_from_config
import json

# Open json file
with open(""conversation_test.json"", ""r"") as f:
    conversation_json = json.load(f)

loaded = load_chain_from_config(conversation_json)

loaded.predict(input=""What is my name?"")
> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there! My name is Ibis Prevedello
AI:  Hi Ibis Prevedello! It's nice to meet you. My name is AI-1. What can I do for you today?
Human: What is my name?
AI:

> Finished chain.
' Your name is Ibis Prevedello. Is there anything else I can help you with?'",3,4
997,2023-02-11T22:23:58Z,2023-02-13T07:02:14Z,2023-02-13T07:02:14Z,1,59,42,,2,0
999,2023-02-12T02:31:12Z,2023-02-14T04:40:16Z,2023-02-14T04:40:16Z,3,56,29,"Currently the chain is getting the column names and types on the one side and the example rows on the other. It is easier for the llm to read the table information if the column name and examples are shown together so that it can easily understand to which columns do the examples refer to. For an instantiation of this, please refer to the changes in the sqlite.ipynb notebook.
Also changed eval for ast.literal_eval when interpreting the results from the sample row query since it is a better practice.",2,0
1000,2023-02-12T02:43:11Z,2023-02-12T04:31:35Z,2023-02-12T04:31:35Z,1,5,5,,2,0
1008,2023-02-12T18:10:34Z,2023-02-14T05:21:38Z,2023-02-14T05:21:38Z,1,17,8,,2,0
1010,2023-02-12T21:46:01Z,2023-02-13T01:43:49Z,2023-02-13T01:43:49Z,3,206,0,"Chroma is a simple to use, open-source, zero-config, zero setup vectorstore.
Simply pip install chromadb, and you're good to go.
Out-of-the-box Chroma is suitable for most LangChain workloads, but is highly flexible. I tested to 1M embs on my M1 mac, with out issues and reasonably fast query times.
Look out for future releases as we integrate more Chroma features with LangChain!",3,1
1014,2023-02-13T02:44:18Z,2023-02-13T19:56:32Z,2023-02-13T19:56:32Z,9,632,66,"Create async callback manager to support async callbacks
Fallback to sync callback manager if manager is not async",2,1
1022,2023-02-13T14:29:20Z,2023-02-13T21:30:07Z,2023-02-13T21:30:07Z,1,1,0,Imho retries should be performed for ServiceUnavailableError (which tends to happen to me quite often).,2,0
1032,2023-02-14T03:39:55Z,2023-02-14T05:53:14Z,2023-02-14T05:53:15Z,3,21,0,,2,0
1040,2023-02-14T06:01:31Z,2023-02-16T19:53:59Z,2023-02-16T19:53:59Z,24,2402,110,"Migrate the Tool abstraction to a separate file
Add a Toolkit abstraction that can own the generation of tools around a shared concept or state

Example would be a JSON Toolkit where each tool may define different functions an agent can run against the shared data object. We don't want the Agent to own the data object, and it makes more sense for now to keep the data object as a value that's managed by the tool/skill itself.",7,2
1046,2023-02-14T10:51:22Z,2023-02-14T15:06:08Z,2023-02-14T15:06:08Z,1,1,1,,2,0
1053,2023-02-14T19:12:25Z,2023-02-16T06:37:48Z,2023-02-16T06:37:48Z,1,88,15,"This PR updates the usage instructions for PromptLayerOpenAI in Langchain's documentation. The updated instructions provide more detail and conform better to the style of other LLM integration documentation pages.
No code changes were made in this PR, only improvements to the documentation. This update will make it easier for users to understand how to use PromptLayerOpenAI",2,0
1058,2023-02-15T03:13:15Z,,2023-03-29T20:28:31Z,4,508,14,"Hello, I have created a serverless memory class that uses DynamoDB that worked successfully on my local machine. However when I was trying to add boto3 as a dependency using poetry I ran into some issues. If someone could help me with that it would be great! The rest of the class is solid and working as expected and can be tested as explained in the serverless memory documentation I added.",5,23
1066,2023-02-15T19:18:00Z,2023-02-16T06:48:09Z,2023-02-16T06:48:09Z,2,26,1,"This PR updates PromptLayerOpenAI to now support requests using the Async API
It also updates the documentation on Async API to let users know that PromptLayerOpenAI also supports this.
PromptLayerOpenAI now redefines _agenerate a similar was to how it redefines _generate",2,0
1068,2023-02-15T19:55:30Z,2023-02-16T06:36:19Z,2023-02-16T06:36:19Z,1,11,4,"Summary
Adds tracked metadata from unstructured elements to the document metadata when UnstructuredFileLoader is used in ""elements"" mode. Tracked metadata is available in unstructured>=0.4.9, but the code is written for backward compatibility with older unstructured versions.
Testing
Before running, make sure to upgrade to unstructured==0.4.9. In the code snippet below, you should see page_number, filename, and category in the metadata for each document. doc[0] should have page_number: 1 and doc[-1] should have page_number: 2. The example document is layout-parser-paper-fast.pdf from the unstructured sample docs.
from langchain.document_loaders import UnstructuredFileLoader
loader = UnstructuredFileLoader(file_path=f""layout-parser-paper-fast.pdf"", mode=""elements"")
docs = loader.load()",2,0
1070,2023-02-15T21:28:22Z,2023-02-16T06:37:58Z,2023-02-16T06:37:59Z,1,1,1,We introduced a breaking change but missed this call. This PR fixes langchain to work with upstream chroma.,2,0
1088,2023-02-16T13:26:19Z,2023-02-16T15:06:02Z,2023-02-16T15:06:02Z,1,1,1,Fixes #1087,3,1
1095,2023-02-16T19:55:34Z,2023-02-18T21:40:43Z,2023-02-18T21:40:43Z,37,1107,611,"Follow-up of @hinthornw's PR:

Migrate the Tool abstraction to a separate file (BaseTool).
Tool implementation of BaseTool takes in function and coroutine to more easily maintain backwards compatibility
Add a Toolkit abstraction that can own the generation of tools around a shared concept or state",3,0
1104,2023-02-17T04:20:53Z,2023-02-17T07:13:34Z,2023-02-17T07:13:34Z,1,1,1,"This import works fine:
from langchain import Anthropic
This import does not:
from langchain import AI21
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'AI21' from 'langchain' (/opt/anaconda3/envs/fed_nlp/lib/python3.9/site-packages/langchain/__init__.py)

I think there is a slight documentation inconsistency here: https://langchain.readthedocs.io/en/latest/reference/modules/llms.html
This PR starts to solve that. Should all the import examples be
from langchain.llms import X instead of from langchain import X?",2,1
1105,2023-02-17T05:03:26Z,,2023-02-18T00:22:03Z,0,0,0,This adds a docs change for instruct embeddings - the InstructorEmbbeding python package is also required.,2,5
1117,2023-02-17T14:09:57Z,2023-02-17T23:12:31Z,2023-02-17T23:12:31Z,1,3,0,"Implementation fails if there are not enough documents. Added the same check as used for similarity search.
Current implementation raises
File "".venv/lib/python3.9/site-packages/langchain/vectorstores/faiss.py"", line 160, in max_marginal_relevance_search
    _id = self.index_to_docstore_id[i]
KeyError: -1",2,0
1118,2023-02-17T14:11:37Z,2023-02-17T21:04:03Z,2023-02-17T21:04:03Z,2,11,1,"Fix KeyError 'items' when no result found.
Problem
When no result found for a query, google search crashed with KeyError 'items'.
Solution
I added a check for an empty response before accessing the 'items' key. It will handle the case correctly.
Other
my twitter: yakigac
(I don't mind even if you don't mention me for this PR. But just because last time my real name was shout out :) )",2,0
1119,2023-02-17T14:40:05Z,,2023-05-09T23:21:38Z,3,18,7,"As referenced in other issues (see #814, #1026), the current implementation of LLMMathChain allows for prompt injection attacks which can execute arbitrary code using Python's exec method. As a quick patch, I have modified the chain to use the slightly safer eval method and modified the prompt template accordingly. From my quick testing, this doesn't seem to hurt the chain's mathematical ability and prevents the exploit outlined in #814.
NB: This is intended as a quick patch to mitigate the current security risks. Future developments, as in #1055, should further help with solving this vulnerability and others.",5,3
1147,2023-02-19T02:01:54Z,2023-02-19T03:31:51Z,2023-02-19T03:31:51Z,7,480,0,Added self-critique constitutional chain based on this paper.,2,0
1152,2023-02-19T15:15:09Z,,2023-08-08T19:29:51Z,2,9,4,"Options are unstructured, paged_pdf, pdf, and text
fixes #1107",3,2
1153,2023-02-19T16:47:39Z,2023-02-21T05:00:32Z,2023-02-21T05:00:32Z,1,7,0,It is useful to be able to specify verbose or memory while still keeping the chain's overall structure.,2,1
1155,2023-02-19T18:44:29Z,2023-02-20T04:48:24Z,2023-02-20T04:48:24Z,1,7,2,"given that we allow user define chunk size, think it would be useful for user to define how many chunks of context will be retrieved.",2,0
1163,2023-02-20T03:03:26Z,2023-02-20T05:15:11Z,2023-02-20T05:15:11Z,2,3,0,add missing links to toc,2,0
1165,2023-02-20T03:28:33Z,2023-02-21T06:54:16Z,2023-02-21T06:54:16Z,14,131,66,"conceptually, no reason a tool should know what an ""agent action"" is
unless any objections, can change in all callback handlers",2,0
1170,2023-02-20T06:01:02Z,,2023-02-20T15:00:09Z,2,5,0,"there is a module for sphinx that makes nice little icon in right bottom for dark mode. Also light<->dark transition is smooth and distinctive, so definitely needed to add.",2,1
1172,2023-02-20T10:43:47Z,,2023-02-20T16:02:58Z,2,20,6,"This PR proposes updates to the serpapi Python library to add support for multiple search results and currency converter feature.
Problem:
The current implementation of the serpapi Python library fetches only the first search result for a query even if the num parameter is set. This limits the usability of the library to fetch a more comprehensive search result list. Additionally, the library does not support the currency converter feature available in some search engine results pages.
Example:
Before it would scrape:
Bitcoin USD price, real-time (live) charts, news and videos. Learn about BTC value, bitcoin cryptocurrency, crypto trading, and more.
The live price of Bitcoin is $ 24,475.15 per (BTC / USD) today with a current market cap of $ 472.29B USD. 24-hour trading volume is $ 28.68B USD.
February 20, 2023 - The current price of Bitcoin is $24605.78 per (BTC / USD). Bitcoin is 64.23% below the all time high of $68789.63.
The live Bitcoin price today is $24,501.18 USD with a 24-hour trading volume of $29,043,133,446 USD. We update our BTC to USD price in real-time.
Bitcoin's price today is US$24,874.07, with a 24-hour trading volume of $26.63 B. BTC is +1.36% in the last 24 hours. It is currently -1.29% from its 7-day ...
Get Bitcoin/USD Coin Metrics (BTC.CM=:Exchange) real-time stock quotes, news, price and financial information from CNBC.
Bitcoin Price is at a current level of 24642.79, up from 24628.82 yesterday and down from 40073.50 one year ago. This is a change of 0.06% from yesterday ...
Find the latest Bitcoin USD (BTC-USD) price quote, history, news and other vital information to help you with your cryptocurrency trading and investing.
The Bitcoin (BTC) live price today is $24517, changes over 24H (-0.23%). Current market cup is $473.10 B. All given information about Bitcoin (BTC) updated ...

But, it only returned:
Bitcoin USD price, real-time (live) charts, news and videos. Learn about BTC value, bitcoin cryptocurrency, crypto trading, and more.
Solution:
To address the above problem, this PR proposes the following updates to the serpapi library:
Add num parameter to _get_default_params() function: The num parameter is set to a default value of 3 to fetch the top three search results. The returned search results are then concatenated to form a single string.
Update snippet extraction code to handle multiple search results: With the updated num parameter, the library extracts all search results' snippets instead of just the first one. This is done by looping through all the search results and concatenating the snippets.
Add support for currency converter: The library is updated to support currency conversion results available in some search engines. The currency converter result is appended to the end of the snippet string, along with the currency symbol.",2,2
1175,2023-02-20T12:54:44Z,2023-02-20T14:46:45Z,2023-02-20T14:46:45Z,2,32,35,"fix notebook formatting, remove empty cells and add scrolling for long text",2,0
1176,2023-02-20T13:53:01Z,2023-02-21T05:09:40Z,2023-02-21T05:09:40Z,3,6,1,"When I try to import the Class HuggingFaceEndpoint I get an Import Error: cannot import name 'HuggingFaceEndpoint' from 'langchain'. (langchain version 0.0.88)
These two imports work fine: from langchain import HuggingFacePipeline and from langchain import HuggingFaceHub.
So I corrected the import statement in the example. There is probably a better solution to this, but this fixes the Error for me.",2,5
1185,2023-02-20T18:57:22Z,2023-02-21T00:38:43Z,2023-02-21T00:38:44Z,9,41,51,,2,0
1187,2023-02-20T19:20:59Z,2023-02-21T00:39:14Z,2023-02-21T00:39:14Z,2,6,2,"Fixes issue #1186. For some reason, #1117 didn't seem to fix it.",2,0
1191,2023-02-20T23:50:30Z,2023-02-21T02:39:34Z,2023-02-21T02:39:34Z,10,786,5,"Description
This PR adds a wrapper which adds support for the OpenSearch vector database. Using opensearch-py client we are ingesting the embeddings of given text into opensearch cluster using Bulk API. We can perform the similarity_search on the index using the 3 popular searching methods of OpenSearch k-NN plugin:

Approximate k-NN Search use approximate nearest neighbor (ANN) algorithms from the nmslib, faiss, and Lucene libraries to power k-NN search.
Script Scoring extends OpenSearch’s script scoring functionality to execute a brute force, exact k-NN search.
Painless Scripting adds the distance functions as painless extensions that can be used in more complex combinations. Also, supports brute force, exact k-NN search like Script Scoring.

Issues Resolved
#1054",2,1
1195,2023-02-21T03:49:14Z,2023-02-21T05:05:13Z,2023-02-21T05:05:13Z,5,51,50,"There's a lot of valuable data that can come from SerpAPI, more than just a single snippet or answer.
Add a results method which returns the raw api response from serpapi.  It's very useful grabbing snippets and the answer box for crafting context summaries in quality checking answers for recent or more obscure information.

I did make a proactive move to move SerpApiWrapper into langchain.utilities since that was where the other Serp API utilities all were, and it seemed like it might have been a legacy thing.  I tested that backwards compatibility was maintained with SerpApiChain.  Also did a small amount of refactoring to bring in the process_response method into the class so that it was more in line with what I saw with the other utilities.
If this is out of scope or not what you want, let me know, I'm fine pulling those changes out and just including the results method.",2,0
1203,2023-02-21T07:01:17Z,2023-02-25T16:59:53Z,2023-02-25T16:59:53Z,4,42,97,,3,2
1216,2023-02-21T18:08:06Z,2023-02-22T01:03:45Z,2023-02-22T01:03:46Z,1,1,1,Found a typo in the documentation code for the constitutional_ai module,2,0
1217,2023-02-21T18:47:32Z,2023-02-22T01:02:04Z,2023-02-22T01:02:04Z,2,283,53,,2,0
1218,2023-02-21T18:51:06Z,2023-02-27T01:55:28Z,2023-02-27T01:55:28Z,2,44,6,"Checking if weaviate similarity_search kwargs contains ""certainty"" and use it accordingly. The minimal level of certainty must be a float, and it is computed by normalized distance.",2,0
1223,2023-02-22T01:44:55Z,,2023-02-27T22:29:32Z,6,106,4,Add a method pformat that takes in a subset of input_variables and adjusts the PromptTemplate's template and input_variables,3,2
1226,2023-02-22T08:31:59Z,,2023-03-12T00:03:17Z,1,6,4,"This allows for the option to self-host a qdrant server and call it from the code as shown below:
qdrant_client = QdrantClient(host='localhost', port=6333)
qdrant = Qdrant.from_documents(docs, embeddings, client=qdrant_client, prefer_grpc=True)",2,3
1228,2023-02-22T09:10:50Z,2023-02-23T15:33:01Z,2023-02-23T15:33:01Z,1,38,0,Adds a Graphsignal ecosystem page,2,0
1231,2023-02-22T13:04:13Z,2023-02-23T15:32:47Z,2023-02-23T15:32:47Z,1,7,2,"Currently youtube loader only seems to support English audio.
Changed to load videos in the specified language.",2,0
1235,2023-02-22T17:30:19Z,2023-02-22T18:36:15Z,2023-02-22T18:36:15Z,1,1,1,Just fixing a typo I found in loading.py,2,0
1240,2023-02-23T00:43:01Z,2023-03-01T07:18:36Z,2023-03-01T07:18:36Z,12,1488,5,"Inspired by the LLMCheckerChain, which is very useful for checking questions and answers, I found a need to check
if the assertions made in a summarization of unknown texts was factually accurate.
LLMSummarizationCheckerChain follows the pattern of LLMCheckerChain and Fact Checker, but builds on it by
conditionally double/triple/n-checking the regeneration of the ""correct"" summary text.   The llm_summarization_checker.ipynb notebook shows a few examples of different
types of text, and the output.
Other small changes

Changed Prompt.from_file to allow relative to the caller file paths.
Added some more context to one of the prompt parameter checks after spending way too long trying to figure out why it was throwing that exception
Added memory support to sequential chains by fixing the param validation


More details
It does a very good job most of the time, but as always with these models, it tends to hallucinate and gets confused.  If max_checks is set too high,
and the LLM temperature is high, it can get stuck in an infinite loop, hallucinating itself to madness.  On the other hand, if you like to live on the edge, you can set max_checks to one and have it behave
like any other SequentialChain.
Depending on the dataset and type of application, I've found through experimenting with it that the LLM temperature is extremely important.  Keeping it at 0 has yielded the most consistent results, but some unstructured
texts require a little more variety to get somewhere good.
Additionally, it really does have problems with more nuanced facts.  For example, ""Mammals can lay eggs, birds can lay eggs, therefore birds are mammals."".  It has no problem correctly identifying that ""Birds are mammals"" is false.
But the statement ""Mammals can lay eggs"" is technically true - see Platypus' (Nature is weird).  The chain gets to a factually correct end point ""'Birds can lay eggs and are not mammals; they belong to a distinct class of their own, separate from mammals.'""
but along the way, it incorrectly asserted ""Mammals can lay eggs: False. Mammals are not capable of laying eggs, as they give birth to live young."".
It gets to the right answer, but in a long and round-a-bout way.
Other strange prompt observations:

Using the word ""Fact"" instead of ""Assertion"" improved the accuracy of the checked_facts generation.
Pre-prompting with ""Fact-checking reporter"" persona also improved accuracy.
Adding a third ""Undetermined"" option when classifying true/false had the largest impact on the final output.  It seems like giving the LLM an ""out"" if it wasn't confident in the answer prevented some hallucinations.

Hopefully this is useful to someone - there's a lot more you can do down this path like abandoning the check loop after a certain number of tries and firing off a chain that does a SERP scrape or docstore lookup to get context and try again.",3,3
1245,2023-02-23T06:26:25Z,,2023-02-23T15:38:34Z,1,17,1,Fix for issue: #1243,5,7
1252,2023-02-23T16:40:56Z,2023-02-24T15:10:35Z,2023-02-24T15:10:35Z,4,270,0,"NotebookLoader.load() loads the .ipynb notebook file into a Document object.
Parameters:

include_outputs (bool): whether to include cell outputs in the resulting document (default is False).
max_output_length (int): the maximum number of characters to include from each cell output (default is 10).
remove_newline (bool): whether to remove newline characters from the cell sources and outputs (default is False).
traceback (bool): whether to include full traceback (default is False).",2,1
1253,2023-02-23T18:17:37Z,2023-02-23T20:34:45Z,2023-02-23T20:34:45Z,1,0,4,"Summary
Updates the docs to remove the nltk download steps from unstructured. As of unstructured 0.4.14, this is handled automatically in the relevant modules within unstructured.",2,0
1256,2023-02-23T19:01:33Z,2023-02-24T15:21:03Z,2023-02-24T15:21:03Z,1,4,0,"As #798, this commit adds the option to truncate the user's inputs larger than what the model can handle.
This defaults to None in the Cohere SDK instead of directly passing ""NONE"", so I maintained the same default value (see: https://github.com/cohere-ai/cohere-python/blob/7f30bfd40c98b88f7d08f8c05db5b91ddb1310d1/cohere/client.py#L127)
PS: I think that both this and #798 should default to the same value.",2,0
1261,2023-02-23T22:45:54Z,2023-02-24T15:06:07Z,2023-02-24T15:06:07Z,2,12,18,-Address TODOs for err handling on missing keys in wolfram & google search utils,2,0
1262,2023-02-23T22:49:51Z,2023-02-24T15:03:50Z,2023-02-24T15:03:50Z,1,4,1,Adding ability to return Source Documents in ChatVectorDBChain async call.,2,0
1263,2023-02-23T23:15:58Z,2023-02-27T05:47:27Z,2023-02-27T05:47:27Z,6,635,1,"Implements AtlasDB vectorstore for integrating with Atlas by Nomic.
Read more about Atlas at docs.nomic.ai",5,5
1286,2023-02-24T21:42:16Z,2023-02-27T01:54:43Z,2023-02-27T01:54:43Z,1,1,1,"While using a SQLiteCache, if there are duplicate (prompt, llm, idx) tuples passed to update_cache(), then an IntegrityError is thrown. This can happen when there are duplicated prompts within the same batch.
This PR changes the SQLAlchemy session.add() to a session.merge() in cache.py, following the solution from this SO thread. I believe this fixes #983, but not entirely sure since that also involves async
Here's a minimal example of the error:
from pathlib import Path

import langchain
from langchain.cache import SQLiteCache

llm = langchain.OpenAI(model_name=""text-ada-001"", openai_api_key=Path(""/.openai_api_key"").read_text().strip())
langchain.llm_cache = SQLiteCache(""test_cache.db"")
llm.generate(['a'] * 5)
>   IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: full_llm_cache.prompt, full_llm_cache.llm, full_llm_cache.idx
    [SQL: INSERT INTO full_llm_cache (prompt, llm, idx, response) VALUES (?, ?, ?, ?)]
    [parameters: ('a', ""[('_type', 'openai'), ('best_of', 1), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'text-ada-001'), ('n', 1), ('presence_penalty', 0), ('request_timeout', None), ('stop', None), ('temperature', 0.7), ('top_p', 1)]"", 0, '\n\nA is for air.\n\nA is for atmosphere.')]
    (Background on this error at: https://sqlalche.me/e/14/gkpj)

After the change, we now have the following
class Output:
    def __init__(self, text):
        self.text = text

# make dummy data
cache = SQLiteCache(""test_cache_2.db"")
cache.update(prompt=""prompt_0"", llm_string=""llm_0"", return_val=[Output(""text_0"")])
cache.engine.execute(""SELECT * FROM full_llm_cache"").fetchall()

# output
>   [('prompt_0', 'llm_0', 0, 'text_0')]
#  update data, before change this would have thrown an `IntegrityError`
cache.update(prompt=""prompt_0"", llm_string=""llm_0"", return_val=[Output(""text_0_new"")])
cache.engine.execute(""SELECT * FROM full_llm_cache"").fetchall()

# output
>   [('prompt_0', 'llm_0', 0, 'text_0_new')]",2,0
1293,2023-02-25T18:23:03Z,2023-03-04T16:01:20Z,2023-03-04T16:01:20Z,9,266,54,"Pulled out of #1240
Working with memory a little bit, it's extremely helpful to attach memory to a sequential chain to pass along unchanging data.  The use case I found was passing along the original text to use as context in each prompt when doing rewrites of text.
It's a lot easier to have a central data store than having to pass along each value as input/output.",2,4
1295,2023-02-25T19:40:39Z,2023-02-27T01:41:03Z,2023-02-27T01:41:03Z,2,94,88,"Thanks for all your hard work!
I noticed a small typo in the bash util doc so here's a quick update. Additionally, my formatter caught some spacing in the .md as well. Happy to revert that if it's an issue.
The main change is just
- A common use case this is for letting it interact with your local file system. 

+ A common use case for this is letting the LLM interact with your local file system.

Testing
make docs_build succeeds locally and the changes show as expected ✌️",2,1
1303,2023-02-26T06:42:46Z,,2023-05-18T23:41:07Z,5,210,0,"In response to #1269, I added an integration using the ArXiv
unofficial SDK.
I don't think it's necessary to use the actual ArXiv API, as lukasschwab's wrapper seems active atm.
Any feedback on the string-like representation is welcome; see line.",4,5
1307,2023-02-26T15:14:45Z,2023-03-15T01:06:03Z,2023-03-15T01:06:03Z,3,457,0,"Description
Add RediSearch vectorstore for LangChain
RediSearch: RediSearch quick start
How to use
from langchain.vectorstores.redisearch import RediSearch

rds = RediSearch.from_documents(docs, embeddings,redisearch_url=""redis://localhost:6379"")",4,17
1308,2023-02-26T16:59:36Z,2023-02-28T16:40:36Z,2023-02-28T16:40:36Z,8,370,14,,2,1
1313,2023-02-27T04:58:21Z,2023-03-03T17:54:18Z,2023-03-03T17:54:18Z,7,174,0,"This PR enables users to transcript audio files and add them to chains using OpenAI's Whisper models on Banana.dev.
TODO

Turn audio_chain_example.py into a notebook",3,1
1325,2023-02-27T16:46:13Z,2023-03-02T05:18:09Z,2023-03-02T05:18:09Z,9,14,14,Fixed typos and links in a few places across documents,2,0
1327,2023-02-27T20:24:18Z,2023-02-27T22:40:44Z,2023-02-27T22:40:44Z,4,11,11,"Fixing a few minor typos in the documentation (and likely introducing other
ones in the process).",2,0
1328,2023-02-27T20:26:45Z,2023-02-27T22:41:13Z,2023-02-27T22:41:13Z,18,90,149,"IMO, it's a bit cleaner to have toolkits in agent_toolkits. Previously, a Toolkit could have imported from agent_toolkit and a create_*_agent could have imported from Toolkit.
example: OpenAPIToolkit needed create_json_agent and create_openapi_agent needed OpenAPIToolkit

Delete unnecessary toolkits (toolkits with only one tool)
Move BaseToolkit to agent_toolkits
Move toolkits to agent_toolkits
update notebooks",2,0
1330,2023-02-27T21:05:02Z,2023-02-27T22:43:32Z,2023-02-27T22:43:32Z,3,160,0,"Summary
Adds a document loader for image files such as .jpg and .png files.
Testing
Run the following using the example document from the unstructured repo.
from langchain.document_loaders.image import UnstructuredImageLoader

loader = UnstructuredImageLoader(""layout-parser-paper-fast.jpg"")
loader.load()",2,0
1333,2023-02-28T01:39:42Z,2023-02-28T04:40:21Z,2023-02-28T04:40:21Z,6,443,0,"iFixit is a wikipedia-like site that has a huge amount of open content on how to fix things, questions/answers for common troubleshooting and ""things"" related content that is more technical in nature.  All content is licensed under CC-BY-SA-NC 3.0
Adding docs from iFixit as context for user questions like ""I dropped my phone in water, what do I do?"" or ""My macbook pro is making a whining noise, what's wrong with it?""  can yield significantly better responses than context free response from LLMs.",2,0
1334,2023-02-28T02:59:34Z,,2023-11-01T22:54:26Z,3,78,15,"Example usage
from langchain.prompts import load_prompt

prompt = load_prompt(""hf://prompts/LLM_Bash/prompt.json"")
prompt.format(question=""Example question"")",5,4
1340,2023-02-28T12:22:43Z,2023-03-01T02:58:23Z,2023-03-01T02:58:24Z,1,2,2,"InvalidTool.run() returns ""{arg}is not a valid tool, try another one."".
However, no function name is actually given in the argument.
This causes LLM to be stuck in a loop, unable to find the right tool.
This may resolve these Issues.
#998
#702",2,0
1347,2023-03-01T00:32:31Z,2023-03-01T02:58:04Z,2023-03-01T02:58:04Z,2,146,1,"Currently, table information is gathered through SQLAlchemy as complete table DDL and a user-selected number of sample rows from each table. This PR adds the option to use user-defined table information instead of automatically collecting it. This will use the provided table information and fall back to the automatic gathering for tables that the user didn't provide information for.
Off the top of my head, there are a few cases where this can be quite useful:

The first n rows of a table are uninformative, or very similar to one another. In this case, hand-crafting example rows for a table such that they provide the good, diverse information can be very helpful. Another approach we can think about later is getting a random sample of n rows instead of the first n rows, but there are some performance considerations that need to be taken there. Even so, hand-crafting the sample rows is useful and can guarantee the model sees informative data.
The user doesn't want every column to be available to the model. This is not an elegant way to fulfill this specific need since the user would have to provide the table definition instead of a simple list of columns to include or ignore, but it does work for this purpose.
For the developers, this makes it a lot easier to compare/benchmark the performance of different prompting structures for providing table information in the prompt.

These are cases I've run into myself (particularly cases 1 and 3) and I've found these changes useful. Personally, I keep custom table info for a few tables in a yaml file for versioning and easy loading.
Definitely open to other opinions/approaches though!",2,1
1356,2023-03-01T07:33:33Z,2023-03-02T04:59:07Z,2023-03-02T04:59:07Z,11,1394,5,,2,1
1357,2023-03-01T08:20:14Z,2023-03-18T20:39:25Z,2023-03-18T20:39:25Z,6,1405,23,"Description
This PR adds a Weights & Biases callback to Langchain, which will allow users to easily track the generations of their interaction with LangChain.

During the duration of the session with Langchain, callback users have the option to log each action performed by the Module to Weights & Biases, alongside the opportunity to analyze the text using complexity metrics such as Flesch-Kincaid, or an option to visualize the text's structure such as Entity Recognition. Details of stdout are automatically saved to the Weights and Biases platform during the context of the callback.
At the end of a session with a module, the generations are logged to Weights and Biases, alongside any selected metrics and visualizations. Users have the ability to also log a table containing all the available information for each callback action that was received, similar to a trace.

As there is no formal finish signal to emit depending on the Langchain Module used to provide generations, we treat the usage of these LLMs similar to a user-defined session, where each session is used to collect many generations for a prompt, test out many different prompts, or different configurations for an LLM.

To serialize the contents of a Langchain module alongside the aforementioned tables to create a contextually tracked model artifact, we add a function to the WandbCallback called flush_tracker which when provided the langchain module to serialize, and some flags controlling the next state of the model for your sessions, will log the aforementioned artifact to Weights and Biases.

This PR includes the following changes:

Added wandb, textstat, spacy en-core-web-sm (3.5.0) and pandas packages to pyproject.toml
Created a new WandbCallback class in wandb_callback.py in langchain/callbacks
Exposed this new callback class in the langchain/callbacks/__.init__.py

Please let me know if there are any issues or concerns with this PR. Thank you for your time and consideration!
Recreating Validation
An example script showing 3 different scenarios using Langchain modules with the WandbCallback have been provided in this colab: https://colab.research.google.com/drive/1DXH4beT4HFaRKy_Vm4PoxhXVDRf7Ym8L?usp=sharing
An example workspace can be found here: https://wandb.ai/a-sh0ts/langchain_callback_demo?workspace=user-a-sh0ts

Under minimal_* it will show the default workspace which will only log during flush, but still track generations and actions during the session.

To recreate the above minimal workspace you can run the collar fully as is


Under full_* it will show the maximal workspace with the tracked values during the session, and the expanded generations table with analysis and visualizations

To recreate features of the above full workspace, set all the appropriate flags to True in the callback of the WandbCallbackHandler



Defaults
Default values for WandbCallbackHandler(...):
visualize: bool = False,
complexity_metrics: bool = False,
stream_logs: bool = False,

Defaults for WandbCallbackHandler.flush_tracker(...):
reset: bool = True,
finish: bool = False,",2,4
1375,2023-03-02T01:55:55Z,,2023-03-09T04:11:26Z,50,3384,477,relevant discussion: #1376,6,0
