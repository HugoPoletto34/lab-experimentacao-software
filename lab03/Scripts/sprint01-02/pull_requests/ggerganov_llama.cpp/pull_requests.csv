number,created_at,merged_at,closed_at,files,additions,deletions,body_text,participants,comments
17,2023-03-11T17:45:06Z,,2023-03-13T16:26:23Z,1,6,0,,5,4
20,2023-03-11T19:25:03Z,2023-03-12T09:27:42Z,2023-03-12T09:27:42Z,3,36,3,"Closes #4
I tried hacking together penalizing repeated tokens for n parameters. I'm actually not sure of the correct approach and I am not a great C++ programmer! But it appears to work somewhat. I think this can be improved much more with better sampling code.",2,1
31,2023-03-12T03:27:55Z,2023-03-12T20:15:00Z,2023-03-12T20:15:00Z,4,30,19,"This would be the initial PR to be able to compile stuff in Windows.
In particular, MSVC is very picky about the features you can use and you cannot.
With C++11

You cannot use designated initializers (when initializing a struct, you cannot specify the fields names)
You cannot use VLAs, so I changed it to a vector.

A PR for the CMake build system (as agreed in #22) will be separated.
These changes were tested with MSVC 19.34.31937.0 (VS Studio 2022) and in macOS 12.6 with Apple clang version 13.1.6",2,0
40,2023-03-12T09:25:56Z,2023-03-17T22:03:48Z,2023-03-17T22:03:48Z,3,95,0,"This pull request adds a simple Nix Flake for building and distributing the binaries of this repository in a combined package.
The main binary can be executed like this (assuming experimental-features = nix-command flakes), for example:
$ nix shell github:niklaskorz/llama.cpp/nix -c llama --help
usage: llama-cpp [options]

options:
  -h, --help            show this help message and exit
  -s SEED, --seed SEED  RNG seed (default: -1)
  -t N, --threads N     number of threads to use during computation (default: 4)
  -p PROMPT, --prompt PROMPT
                        prompt to start generation with (default: random)
  -n N, --n_predict N   number of tokens to predict (default: 128)
  --top_k N             top-k sampling (default: 40)
  --top_p N             top-p sampling (default: 0.9)
  --temp N              temperature (default: 0.8)
  -b N, --batch_size N  batch size for prompt processing (default: 8)
  -m FNAME, --model FNAME
                        model path (default: models/llama-7B/ggml-model.bin)

When merged into the main branch of this repository, nix shell github:ggerganov/llama.cpp would be used instead.
The binary names in the Nix package are:

main=> llama
quantize => quantize
convert-pth-to-ggml.py => convert-pth-to-ggml

Note that convert-pth-to-ggml directly executes in a Python 3.10 environment with the required Python packages.",6,16
43,2023-03-12T10:43:37Z,2023-03-13T16:33:44Z,2023-03-13T16:33:44Z,1,4,0,"Hello!
I noticed that the model loader is not using buffered IO, so I added a piece of code for buffering.
I measured the loading time only for llama 7B on my M1 Pro Macbook, but it reduced the time from 1316ms to 749ms.",2,1
45,2023-03-12T11:10:37Z,2023-03-12T20:30:09Z,2023-03-12T20:30:09Z,1,1,1,,3,0
48,2023-03-12T13:34:22Z,2023-03-13T16:39:56Z,2023-03-13T16:39:56Z,1,46,46,"one can use ./main ... 2>/dev/null to suppress any diagnostic output
Fixes #5",4,6
51,2023-03-12T14:42:43Z,2023-03-22T17:20:25Z,2023-03-22T17:20:25Z,1,3,0,"Small fix to compile binaries properly on Linux:

defines CLOCK_MONOTONIC in ggml.c
Closes #54",4,7
56,2023-03-12T18:13:33Z,2023-03-12T20:23:15Z,2023-03-12T20:23:15Z,3,12,89,@ggerganov not sure what you meant by temp is missing it was there. See PR.,4,1
68,2023-03-13T00:09:01Z,,2023-03-14T17:13:39Z,2,3,1,"Not much, but has some benefits,

Shorter commands.
Help actual executable files to stand out.",2,2
72,2023-03-13T00:35:52Z,,2023-03-13T16:17:55Z,2,32,38,"My take at #70.
I tried using black for formatting, but things are not in a great shape now, so I just tried to match the format on the lines I had to change.
This looks correct to me, but I haven't been able to run the converted model yet, so I can't verify. I was able to convert it just fine, but got some tensor dimension mismatch later.",4,2
75,2023-03-13T02:33:08Z,2023-03-13T17:12:34Z,2023-03-13T17:12:34Z,1,123,0,"This is the draft to support llama.cpp being built by CMake. Most of the things were copied from whisper.cpp's CMake so it should be pretty identical.
This currently builds on my M1 Air with the same perf as using the makefile, but it currently doesn't builds on windows because of #74. Doing the revert of said commit makes it compile on Windows as well.
This currently works both on my M1 Air and on my Windows 10 machine.",2,2
77,2023-03-13T03:04:55Z,,2023-03-21T20:52:07Z,8,1616,1186,"This change makes it easier to use this code as a library - say to build python bindings on top of it. It extracts out the following functions into llama.cpp

llama_model_load
llama_eval
llama_model_quantize

It also moves the relevant struct definitions to llama.h. This for example, helps avoid redefinition of llama_hparams in quantize.cpp. Please let me know if you have any suggestions to improve this.
See here for an example of this library structure in use.",5,6
79,2023-03-13T05:07:22Z,2023-03-13T16:24:19Z,2023-03-13T16:24:19Z,2,27,11,"Fixes #11 (including color handling). Largely based on #73 (props to @j-f1), but doesn't pull any additional dependencies.
Sample output from 13B model with default parameters:
main: prompt: '关于爱因斯坦的生平。他出生于'
main: number of tokens in prompt = 19
     1 -> ''
 31057 -> '关'
 30909 -> '于'
   234 -> '�'
   139 -> '�'
   180 -> '�'
 31570 -> '因'
 31824 -> '斯'
   232 -> '�'
   160 -> '�'
   169 -> '�'
 30210 -> '的'
 30486 -> '生'
 30606 -> '平'
 30267 -> '。'
 31221 -> '他'
 30544 -> '出'
 30486 -> '生'
 30909 -> '于'

sampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000


关于爱因斯坦的生平。他出生于1856年，就是一位德国化学家、天文学家和温谐器研究者。20世紀最初时期在高飞航母中被发现，爱因斯坦对此使用",4,1
90,2023-03-13T11:25:53Z,2023-03-13T16:40:54Z,2023-03-13T16:40:54Z,3,6,2,,2,0
92,2023-03-13T12:50:30Z,2023-03-13T16:15:21Z,2023-03-13T16:15:21Z,2,18,31,Alternative to #17 suggested in #17 (comment),5,3
98,2023-03-13T18:33:45Z,2023-03-13T20:29:11Z,2023-03-13T20:29:11Z,1,14,0,,2,0
109,2023-03-14T00:59:06Z,2023-03-19T17:17:40Z,2023-03-19T17:17:40Z,1,84,108,,4,2
114,2023-03-14T04:22:51Z,2023-03-17T16:38:24Z,2023-03-17T16:38:24Z,2,43,3,"Fix the CMake build on Linux to prevent it from failing with an error message.
/usr/bin/ld: libggml.a(ggml.c.o): in function `ggml_graph_compute':
ggml.c:(.text+0x16960): undefined reference to `pthread_create'
/usr/bin/ld: ggml.c:(.text+0x169c3): undefined reference to `pthread_join'
/usr/bin/ld: libggml.a(ggml.c.o): in function `ggml_graph_compute':
ggml.c:(.text+0x16960): undefined reference to `pthread_create'
/usr/bin/ld: ggml.c:(.text+0x169c3): undefined reference to `pthread_join'
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/llama.dir/build.make:119: llama] Error 1
make[1]: *** [CMakeFiles/Makefile2:153: CMakeFiles/llama.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/quantize.dir/build.make:119: quantize] Error 1
make[1]: *** [CMakeFiles/Makefile2:126: CMakeFiles/quantize.dir/all] Error 2
make: *** [Makefile:103: all] Error 2",5,6
120,2023-03-14T06:40:00Z,2023-03-15T19:56:25Z,2023-03-15T19:56:25Z,1,9,2,,2,0
126,2023-03-14T10:15:26Z,2023-03-15T19:39:07Z,2023-03-15T19:39:07Z,1,1,1,"I'm not an expert on Licenses BUT,
If you attribute Facebook in the README and description, you essentially admit/imply that this repo is a modification of their repo. Facebook's repo has ""GPL-3.0 license"". Which means this repo should also be like that in that case, which is something that we dont want.
This PR fixing that potential language issue.
Also recommend changing the description too. Such as
""Port of LLaMA model in C/C++""",5,6
131,2023-03-14T12:57:49Z,2023-03-21T00:37:17Z,2023-03-21T00:37:17Z,1,153,76,"We have come up with several changes to CMakeLists.txt that are expected to improve performance, compatibility, and maintainability, and we have drafted them.
Change list :

remove NO from option names that have NO in the option name.
Change LLAMA_NO_ACCELERATE to LLAMA_ACCELERATE to avoid ""NOT NO"" in if statements.
make it possible to enable/disable SIMD extension instructions for non-Apple.
I see no reason to make it Apple-only. It should be possible to enable/disable on all platforms.
give priority to CMake functions.
Use ""add_compile_options"" instead of writing directly to CMAKE_C_FLAGS to avoid double flags and compile errors due to typos.
Add new options.
LLAMA_STATIC_LINK, LLAMA_NATIVE, and LLAMA_LTO were added.
LLAMA_STATIC_LINK is an option to enable static linking. This will be useful when compiling for Windows with MinGW and for future distribution.
Also, LLAMA_NATIVE to add -march=native and LLAMA_LTO for link-time optimization will both be useful options for this project for speed.
other changes.
cmake_minimum_required was changed to 3.9 for LTO support.
CMAKE_CXX_STANDARD was changed to c++11, the same as in the Makefile.
I made utils build using add_library and link to it.
Added Threads link since it was missing.
Added option LLAMA_OPENBLAS.  (Not tested).

We have confirmed that it works on MSVC, MSVC Clang GUN like (not clang-cl), MinGW, and Cygwin on Windows, but have not confirmed that it works on other platforms.
Also, prioritizing CMake's features may sometimes sacrifice simplicity.
P.S. Sorry if there is any strange English. This is using automatic translation.",5,4
132,2023-03-14T13:21:47Z,2023-03-17T09:47:06Z,2023-03-17T09:47:06Z,9,270,2,"First of all, thank you for the effort of the entire community. The work they do is impressive.
I'm going to try to do my bit by dockerizing this client and making it more accessible.
If you have time, I would recommend creating a pipeline to publish the image to dockerhub, so it would be easier to use, ej: docker pull ggerganov/llamacpp or similar.
To make it work, just execute these commands:

Build image (atm not exists in dockerhub)
docker build -t llamacpp .
Run program:

docker run -v ./models:/models llamacpp -m /models/7B/ggml-model-q4_0.bin -p ""Building a website can be done in 10 simple steps:"" -t 8 -n 512

If you want to run in interactive mode, don't forget to tell Docker that too.
docker run -v ./models:/models llamacpp -m /models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r ""User:"" \
                                           -p \
""Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.

User: Hello, Bob.
Bob: Hello. How may I help you today?
User: Please tell me the largest city in Europe.
Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.
User:""",7,20
139,2023-03-14T17:54:20Z,2023-03-14T19:34:37Z,2023-03-14T19:34:37Z,1,32,1,"dotprod extensions aren't available on some ARM CPUs (e.g. Raspberry Pi 4), so check for them and only use them if they're available.
Reintroduces the code removed in 84d9015 if __ARM_FEATURE_DOTPROD isn't defined.
Fixes my and other problems reported in the comments of #58.",2,0
142,2023-03-14T20:34:30Z,2023-03-15T19:37:50Z,2023-03-15T19:37:50Z,1,1,1,"When converting the model + tokenizer, use the vocabulary size returned by the tokenizer rather than assuming 32000.
There are ways that special tokens or other new tokens could be added to the tokenizer; therefore it's probably best not to assume the vocabulary is only 32000 tokens.",2,0
148,2023-03-15T01:50:38Z,2023-03-15T19:42:40Z,2023-03-15T19:42:40Z,3,8,3,"Adds a parameter called context size (-c for short) that allows taking the context size from the user's input.
Defaults to the same hardcoded 512.",3,3
149,2023-03-15T01:57:37Z,2023-03-15T19:39:38Z,2023-03-15T19:39:38Z,1,5,0,"Fixes the color messing up the terminal when the program exits by printing an ANSI_COLOR_RESET.
Includes it in the SIGINT handler too.",2,0
151,2023-03-15T02:38:08Z,2023-03-18T11:44:09Z,2023-03-18T11:44:09Z,1,1,1,Fixes scanf unused result compile warning.,3,0
154,2023-03-15T06:16:30Z,,2023-03-19T18:18:42Z,1,4,4,"As suggested in #146 we are able to save lots of memory by using float16 instead of float32. I implemented the suggested changes, and tested with the 7B and 13B models, and there were no issues on my Intel-based MacBook Pro.
Merging these changes should allow more models to run more performantly on a wider range of hardware.",3,4
161,2023-03-15T15:09:12Z,2023-03-15T19:05:15Z,2023-03-15T19:05:15Z,1,2,2,"Without the ""static"" quantifier, it fails to compile in clang:
ld.lld: error: undefined symbol: packNibbles
>>> referenced by ggml.c:520 (llama_cpp/ggml.c:520)
>>>               .../llama_cpp/__ggml__/__objects__/ggml.c.pic.o:(quantize_row_q4_0)

ld.lld: error: undefined symbol: bytesFromNibbles
>>> referenced by ggml.c:1434 (llama_cpp/ggml.c:1434)
>>>               .../llama_cpp/__ggml__/__objects__/ggml.c.pic.o:(ggml_vec_dot_q4_0)
>>> referenced by ggml.c:1435 (llama_cpp/ggml.c:1435)
>>>               .../llama_cpp/__ggml__/__objects__/ggml.c.pic.o:(ggml_vec_dot_q4_0)
clang-12: error: linker command failed with exit code 1 (use -v to see invocation)",2,0
176,2023-03-15T20:25:58Z,,2023-03-30T19:34:27Z,2,101,23,"Drop torch, do not load whole file into memory, process files in parallel and use separate threads for r/w",6,4
181,2023-03-15T21:17:50Z,2023-03-19T18:22:49Z,2023-03-19T18:22:49Z,3,14,1,"Adds the --ignore-eos switch which prevents generation of the end of text (eos) token. This can be useful to avoid unexpected terminations in interactive mode and to force the model to generate longer output.
This is implemented by setting the logits of the eos token to zero, which seems to work well enough, but I am not sure if there may be any unwanted side effects.",3,0
193,2023-03-16T00:14:41Z,2023-03-17T04:48:40Z,2023-03-17T04:48:40Z,2,130,39,"Includes vectorised inference code, quantisation and a counterpart to the Q4_0 multipart fix we introduced a while ago. Tested working up to 13B, though I can't confidently say anything about the impact on quality (especially since the RMS norm patch also just landed). Speed overheads relative to Q4_0 seem to be about 50%. This should give us a viable framework to evaluate Q4_1 quantization on x86 machines.
What's missing is accelerated inference code for for ARM NEON - I have no access to any machine that has it, so I'm going to have to delegate there.",3,7
198,2023-03-16T07:59:02Z,2023-03-21T16:23:16Z,2023-03-21T16:23:16Z,1,53,0,"I'm not sure if this has a place in the repository. I did a bit of prompt engineering to get a conversation going with LLaMa, this is the script I use, which can serve as an example and ""quickstart"" to new users.",10,15
216,2023-03-16T22:39:40Z,,2023-03-30T07:45:14Z,1,34,31,"Tried to address slow weights loading. 7B is okay, but 13B is really slow (several minutes), hard to experiment/prototype with larger models.
Replaced std::ifstream with C-style file reading using fopen. Got a considerable boost in loading performance: 3x to 10x faster on my machine (measuring results were kinda inconsistent, but it is definitely a lot faster than before).
I made sure the weights are correctly loaded: fixed the seed, gave the same prompts - model gives the same output, everything is good.
Also increased the buffer size from 1024*1024 to 128*1024*1024 (see line 102) which gave a slight boost as well. Though I am not sure whether it is optimal for edge devices like Raspberry Pi (if it's of any concern).",9,16
221,2023-03-17T02:24:08Z,2023-03-19T19:44:31Z,2023-03-19T19:44:31Z,1,25,34,"llama.cpp/main.cpp
    
    
        Lines 980 to 983
      in
      7213110
    
  
  
    

        
          
           // reset color to default if we there is no pending user input 
        

        
          
           if (!input_noecho && params.use_color && embd_inp.size() == input_consumed) { 
        

        
          
               printf(ANSI_COLOR_RESET); 
        

        
          
           } 
        
    
  


This can fail to colorize the last params.n_batch part of the prompt correctly because embd was just loaded with those tokens and not printed, yet.",4,2
222,2023-03-17T03:33:34Z,2023-03-19T18:38:45Z,2023-03-19T18:38:45Z,3,127,16,I improved the quantize script by adding error handling and allowing to select many models for quantization at once in the command line. I also converted it to Python for generalization as well as extensibility.,5,6
226,2023-03-17T07:01:04Z,,2023-03-17T14:44:04Z,1,6,2,,2,1
230,2023-03-17T08:11:11Z,2023-03-18T07:27:13Z,2023-03-18T07:27:13Z,1,56,1,"AI for the masses


Github CI changes

Add feature to manually trigger CI and choose whether to create a new release (workflow_dispatch)
Autobuild triggers only on relevant changes (code, not readme) or manually
Autorelease triggers only on pushes to master branch or manually



From alpaca to llama antimatter15#5",3,2
232,2023-03-17T09:02:11Z,,2023-03-19T17:18:03Z,1,152,138,"optimize convert tool with argparse
$ python3 convert-pth-to-ggml.py -h
usage: convert-pth-to-ggml.py [-h] dir_model {f32,f16} out_dir

Convert ckpt models to ggml models. For example: python3 convert-pth-to-ggml.py ../llama-models/7B/ f32 models/llama-7B

positional arguments:
  dir_model   Directory path of the checkpoint model
  {f32,f16}   Data type of the converted tensor, f32 or f16
  out_dir     Directory path for storing ggml model

options:
  -h, --help  show this help message and exit
Tested on 7B/30B models, it works well.
$ tree models/
models/
├── 7B
├── llama-30B
│   ├── ggml-model-f16.bin
│   ├── ggml-model-f16.bin.1
│   ├── ggml-model-f16.bin.2
│   └── ggml-model-f16.bin.3
└── llama-7B
    ├── ggml-model.bin -> ggml-model-f16.bin
    ├── ggml-model-f16.bin
    └── ggml-model-f32.bin",4,5
235,2023-03-17T09:57:36Z,2023-03-20T17:05:21Z,2023-03-20T17:05:21Z,1,1,1,"In the PR that was resolved (#132), the action defined to publish the packages used the user and token of the author of the commit in master.
In this case, I have been yours, but I do not have permissions for such an action.
That is why I have modified the file, so that the owner's token is included as a secret with the name GHCR_PAT.
It is important to select the read:packages & write:packages permissions.
NOTE: Checks will fail until the secret is included.",4,4
242,2023-03-17T16:36:53Z,2023-03-17T20:05:58Z,2023-03-17T20:05:58Z,2,44,26,,5,1
252,2023-03-18T03:05:18Z,2023-03-20T10:17:24Z,2023-03-20T10:17:24Z,7,182,46,"I believe this largely fixes the tokenization issues. The example mentioned in #167 as well as my local tests (e.g. ""accurately"" should tokenize as [7913, 2486]) are fixed by it. Have not tested extensively though, especially with Unicode.
I saw some discussion around file format updates so just take this as an rfc, I just hacked something in
sorry if my coding style is not to your liking ;)",8,13
254,2023-03-18T05:34:52Z,2023-03-18T11:17:19Z,2023-03-18T11:17:19Z,1,1,1,This causes long prompts to parse very slowly (and looks like it was intended to use max_len here).,2,0
262,2023-03-18T12:32:06Z,2023-03-18T13:51:49Z,2023-03-18T13:51:49Z,1,1,9,,3,0
270,2023-03-18T21:16:47Z,2023-03-21T16:27:43Z,2023-03-21T16:27:43Z,3,98,13,"This adds an option to compute perplexity over the prompt input similar to https://huggingface.co/docs/transformers/perplexity.  It does so by chunking up the prompt into non-overlapping chunks of the context window size.  It then runs the forward pass and computes the softmax probability of the output logits for the last half of the context window.  This is so the model always has some context to predict the next token.  Be warned: it is pretty slow, taking about 4 hours or so to complete wikitext2 on a 32 core machine.
Note: when doing prediction over large prompts, the default 10% expansion for the memory buffer is not sufficient - there is definitely a non-linear scaling factor in there somewhere.
Example:

Download/extract: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research
Run ./main --perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw
Output: perplexity: 6.5949 [655/655]

Some example runs at context 512:
5.5985 - 13B, q4_0
5.9565 - 7B, f16
6.3001 - 7B, q4_1
6.5949 - 7B, q4_0
6.5995 - 7B, q4_0, --memory_f16

Context 1024 runs:
5.9876 - 7B, q4_0, --memory_f16

Which show that the 16 bit version of the model is the best (lower perplexity is better), the 4 bit quantization introduces a fair amount of error (but not disastrous certainly), and the --memory_f16 flag is almost identical to baseline 4 bit.
Comparing to this article: https://nolanoorg.substack.com/p/int-4-llama-is-not-enough-int-3-and, where they compare 4-bit to GPTQ quantization.  The results are comparable, which is a good sign!",10,75
273,2023-03-18T21:44:48Z,,2023-04-13T12:52:13Z,3,60,0,"Resolves #240
WIP
This needs to be able to:

Configure custom model folders.
Adjust settings for running variants of the Alpaca model and make corresponding changes in the C++ side.
Provide the ability to configure parameters.
Possibly hide the extraneous template it returns.
Ability to run interactive mode",3,2
278,2023-03-19T02:46:27Z,,2023-03-22T21:03:19Z,11,710,353,"This builds on my other PR to implement a very simple TCP mode.
The new mode first loads the model then listens for TCP connections on a port. When a connection is received, arguments will be parsed using a simple protocol:

First the number of arguments will be read followed by a newline character.
Then each argument will be read, separated by the 0 byte.
With this we build an argument vector, similar to what is passed to  the program entry point.
The resulting ""argv""  is passed gpt_params_parse.

Finally llama_main will be executed with the input/output streams connected to the socket.
I've included two sample bash scripts which can be used to test the new mode. This is how it works:

Run ./chat_tcp_server.sh in a terminal.
In a second terminal, run ./chat_tcp_client.sh. This will connect to the server and start a sample chat session.

One thing to note is that this mode is only implemented for Unixes. There's two reasons for that:

I never wrote win32 TCP code, so I'm not familiar with the API
There's really no advantage in implementing this for win32 because it doesn't support fork(). The main advantage of using this mode is that it serves each connection in a separate process which inherits memory from the parent (so the model only has to be loaded once).

While the protocol is a bit ""low level"", it should be easy to write a higher level API on top of this, such as a node.js web server or next.js app.",16,31
282,2023-03-19T06:39:36Z,2023-03-24T15:05:13Z,2023-03-24T15:05:13Z,5,83,11,"Hi everyone,
I took a stab at adding embedding mode, where we print the sentence embedding for the input instead of generating more tokens.
If I only add the compute and print in llama_eval itself, that works. But for some reason after adding the boolean flag (even if I create a second function that has the same preamble but in the end only prints the embeddings) it stops working.
Could anyone take a look and tell me where I failed or what I am missing, so that we can add this capability?
Thank you!",6,18
283,2023-03-19T07:03:27Z,2023-03-19T18:10:01Z,2023-03-19T18:10:01Z,1,1,0,"In interactive mode:
Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.
User: xxx

Press CTRL+C, the program exits, but terminal color still remains blue.
To ensure color reset, let's print '\n' in sigint_handler.",3,0
284,2023-03-19T09:11:15Z,,2023-03-21T06:44:33Z,1,4,4,"bugfix:      std::string mesh up vocab.
OS:           CentOS 7
compiler: gcc (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9)",3,0
293,2023-03-19T12:18:20Z,2023-03-20T08:24:12Z,2023-03-20T08:24:12Z,1,2,2,Should fix #289,3,3
294,2023-03-19T13:55:56Z,2023-03-19T17:57:00Z,2023-03-19T17:57:00Z,3,11,6,"made the changes requested by @ggerganov in #154 . fixes #146
With this change, you can half the llama_model_load: memory_size = 512.00 MB -> memory_size = 256.00 MB
(ctx512 7B q4_0)
A non empirical comparison does not seem to degrade the quality of the prediction. but that might not mean anything. (waiting on #270)",3,0
299,2023-03-19T18:32:22Z,2023-03-19T19:33:07Z,2023-03-19T19:33:07Z,3,27,16,"Sometimes the model will capitalize the reverse prompt differently or we might want to have more than one for any reason.
Added support for multiple reverse prompts by specifying the -r flag multiple times.",2,2
301,2023-03-19T19:06:17Z,2023-03-21T16:42:25Z,2023-03-21T16:42:25Z,2,189,9,"Based on: https://github.com/qwopqwop200/GPTQ-for-LLaMa
Current status: Seems to be working now.
I was originally hoping to validate the results by matching the Python implementation's output exactly, but precision and non-associativity issues make this very difficult, including when performing matrix multiplications and, especially, computing norms.
Anyway, design details:
The models being imported store per-layer weights in essentially q4_1 format, although the addend and scale are shared across an entire row rather than every group of 32 weights.  This script duplicates the addend and scale to match ggml's expectations, at the cost of wasting some memory.
However, there are two differences which I accommodated changing the output format (and adding corresponding support to main.cpp) rather than having the script match the existing one:


The tok_embeddings and output weights (i.e. the weights that aren't per-layer) are f16 instead of q4_1.  They could be converted to q4_1, and the impact of the loss of precision would probably be low, but this would rule out exactly matching the Python implementation's output for validation.


There is no sharding, since the input doesn't have it, and for a CPU-only implementation it seems more useful to avoid having to deal with multiple files.


The new format is differentiated from existing q4_1 format by changing the 'f16' header flag to a new value, 4.  That said, I think a cleaner approach would be to change main.cpp to support loading each tensor with an arbitrary sharding configuration and type rather than hardcoding specific combinations of types.  So far I've wasted too much time debugging to try implementing this...",6,8
305,2023-03-19T21:52:07Z,2023-03-21T17:21:50Z,2023-03-21T17:21:50Z,4,36,24,"If it is not necessary sorted maps, change std::map to std::unordered_map
std::unordered_map is a hash table so it should be faster than std::map when storing many items.
std::map<id, token> can be a std::vector since the vector index can be equal to the token id",7,7
306,2023-03-19T22:05:19Z,,2023-03-24T15:25:44Z,1,3,3,"In interactive mode, every time the model has to respond to user input it has an increasingly reduced token budget, eventually generating only a few words before stopping. The token budget in interactive should apply to every batch of tokens after user intervention, not globally.",3,1
311,2023-03-19T23:58:18Z,2023-03-21T16:14:46Z,2023-03-21T16:14:46Z,1,15,0,"On older versions (of Windows) function will silently fail without any ill effects
Only used when params.use_color==true ( --color )
No windows.h dependency",3,0
312,2023-03-20T01:58:04Z,2023-03-21T17:11:02Z,2023-03-21T17:11:02Z,1,48,13,"Some moving around of ANSI color code emissions in recent patches has left us in a situation where RESET codes were getting defensively emitted after every token, resulting in multibyte UTF-8 codes that are split across tokens being broken up:

This fixes that, while cleaning up the entire color control architecture, stopping unnecessary emission of color codes and probably making it more conducive to portability later on, e.g. to platforms where color has to be set by out-of-band API calls.",4,1
314,2023-03-20T05:50:02Z,2023-03-21T15:50:10Z,2023-03-21T15:50:10Z,3,6,2,Add OpenBSD support.,5,1
319,2023-03-20T10:33:08Z,2023-03-20T19:26:02Z,2023-03-20T19:26:02Z,3,18,10,bit of refactoring per #252,4,1
320,2023-03-20T11:20:22Z,2023-03-21T14:35:42Z,2023-03-21T14:35:42Z,2,109,3,"NOTE: I am seeing different outputs when running with these changes.  They seem of equal quality, but this isn't something I observed when first testing this out on alpaca.cpp.
It's possible that some rounding behavior is happening slightly differently or something like that.  If this is a dealbreaker, I can try to figure out what is causing the difference and check if it's possible to get rid of it.
Changes

Update Makefile to detect AVX512 support and add compiler flags if it's available
Add AVX512 impl based on existing AVX2 implementation, dot product on one 32-value block of 4-bit quantized ints at a time
Perform 8 bit -> 16 bit sign extension and multiply+add on 32 values at time instead of 16
Use built-in AVX512 horizontal reduce add to get sum at the end
Manual unrolling on inner dot product loop to reduce loop counter overhead

Performance Impact
I initially implemented this over on alpaca.cpp where I saw an ~10% speedup to inferrence.
Before:
main: mem per token = 14368644 bytes
main:     load time =   923.25 ms
main:   sample time =    85.94 ms
main:  predict time = 23502.37 ms / 92.17 ms per token
main:    total time = 24845.69 ms

After:
main: mem per token = 14368644 bytes
main:     load time =   928.89 ms
main:   sample time =    16.18 ms
main:  predict time =  5720.41 ms / 82.90 ms per token
main:    total time =  6982.89 ms

I was hoping for more, but some other stuff I tried like converting the bytesFromNibbles function to operate on two blocks at a time by using AVX512 were not successful.",4,1
325,2023-03-20T12:58:29Z,2023-03-20T19:33:11Z,2023-03-20T19:33:11Z,1,8,1,"In convert-pth-to-ggml.py, dir_model is something like models/7B or models/7B/.
tokenizer.model is expected under model's parent dir.
When dir_model is a symlink, f""{dir_model}/../tokenizer.model"" would not be found.
Let's use the model's parent dir directly.",3,0
333,2023-03-20T16:48:35Z,2023-03-23T20:22:48Z,2023-03-23T20:22:48Z,1,15,6,"Edit: Most of the below is now outdated. This PR aims to do two things.
-Replace EOS with newline to prevent context/memory being flushed by EOS in interactive mode
-Better reverse prompt behavior on EOS by injecting the first given reverse prompt on the newline upon EOS
Aims to improve coherence and ability to resume the interactive session when the user is given input back after an end of text token is reached. Not sure what token 13 is or why it seems to help, so requesting someone more knowledgeable on this.
Forgive the crudeness of this PR. As of 368d0c8 interactive mode now continues and gives back user input when an end of text token is reached. This is great, however there seems to be odd behavior after user is given control back following the end of text token. The following below this is my observations. (pasted mostly from another issue)
This is a bit of a tangent, but I've been looking further into the weird behavior when the end of text token occurs and gives the user control. (without the use of --ignore-eos, meaning end of texts can occur) and would like to propose a change to 368d0c8. While I'm not super familiar here, it seems that changing the section add these two lines seems to improve if not outright fix the weird lack of coherence that occurs after an end of text token. I am not qualified enough to speak on this technically but saw the emb.back() = 13 and last_n_tokens.back() = 13 thrown around as a small hack to get around end of texts prior to 368d0c8.
        // end of text token
        if (embd.back() == EOS_TOKEN_ID) {
            if (params.interactive) {
                is_interacting = true;
                embd.back() = 13;
                last_n_tokens.back() = 13;
            } else {
                fprintf(stderr, "" [end of text]\n"");
                break;
            }
        }

Here are three excerpts from a reverse prompt dialogue WITHOUT this addition, the current behavior when an end of text token is reached. (edited in the [end of text] parts for clarity to indicate when it gave me back control)
Start of the neverending dialogue that lasts forever and goes on for 1000 years

Jessie: H-hi there.. if someone is there. *sweats*
User: Hi there!
Jessie: A-ah! *startled*
User:It's okay, it's just me. :)                       
Jessie: Oh thank goodness...I thought i was going crazy talking to myself like this. [end of text]
User:Hello?
This user account has been created by a bot (or automated program).
If you have any questions about its content or actions, contact the bot operator directly using their talk page message system.
To protect against spamming, all new accounts are automatically blocked from editing until they pass manual review. If your request is approved, your username will be changed to match your email address.
Retrieved from ""https://en.wikipedia.org/wiki/Special:UsedAccount"" [end of text]

A bit of second hand embarrassment as it randomly started going on about the anime, Naruto and fan-fiction..
Start of the neverending dialogue that lasts forever and goes on for 1000 years

Jessie: H-hi there.. if someone is there. *sweats*
User: Hi there!
Jessie: A-ah! *startled*
User:It's okay. It's just me.
Jessie: Oh... ummm.... what do you need from me today?
User:end of story
 [end of text]
User: Nothing.
Title: Nothing.
Characters/Pairing: Kiba Inuzuka & Tenten (from Naruto)
Rating: PG-13 (for language)
Summary: The two are enjoying a day off together when they come across something unexpectedly beautiful.
Notes: This was written as part of the 'Narutomania' challenge over at /fic/, which asked us to write a one shot using any combination of characters or pairings from the show.
Kiba Inuzuka & Tenten were having a great time exploring Konoha during their days off.

Particularly strong example of how it just forgot who I was speaking with entirely after an end of text.
Start of the neverending dialogue that lasts forever and goes on for 1000 years

Jessie: H-hi there.. if someone is there. *sweats*
User: Hi there!
Jessie: A-ah! *startled*
User:Don't worry, just me.
Jessie: Oh... thanks....
\end{code} [end of text]
User: Hi.
User: Hello?
System: Yes. What would you like today?
User: What happened to Jessie?
System: Who are you talking about?
User: Is Jessie not here?
System: Nope. She left a while ago.
User: Oh, that's too bad.
System: You mean she was here before?
User: I thought she was.
System: Well, maybe it wasn't her then. Maybe we were thinking of another person. Or maybe we were thinking of two people at once, but one got lost inside our heads somewhere along the way.
User:

And here two small excerpts WITH the above change when the end of text token is thrown.
Start of the neverending dialogue that lasts forever and goes on for 1000 years

Jessie: H-hi there.. if someone is there. *sweats*
User: Hi there!
Jessie: A-ah! *startled*
User:It's okay, it's just me. :)
Jessie: Oh thank goodness...I thought maybe a ghost was talking to me or something...
User:end of story
 [end of text]
EndUser: Hi.
Jessie: Hello! [end of text]
User:Hi
Jessie: Who are you?

Start of the neverending dialogue that lasts forever and goes on for 1000 years

Jessie: H-hi there.. if someone is there. *sweats*
User: Hi there!
Jessie: A-ah! *startled*
User: It's okay! It's just me. :)
Jessie: Oh...thank god! You scared me half to death! :O
User:end of story
 [end of text]
EndUser: Now what?
Jessie: Well now we have a conversation going on between us two, but it doesn't end here. [end of text]

I've tested this over the past day, and it seems pretty apparent that without emb.back() = 13 and last_n_tokens.back() = 13 it completely loses the plot when you give any input following the end of text token.
Would greatly appreciate someone more knowledgeable signing off on this and potentially explaining why these two lines with token 13 seem to remedy the weird behavior that occurs after an end of text token is reached in interactive mode.
Also to be clear, this does not seem to effect the later lines pertaining to when remaining_tokens runs out. That seems to give user control and allow for continuation of the session just fine with no lost coherence. So just the end of text part.
Apologies in advance if I've done anything wrong in the process of creating this PR.",7,42
335,2023-03-20T17:36:53Z,2023-03-21T15:44:11Z,2023-03-21T15:44:11Z,1,4,6,"Just several minor cleanup.

Mac (Intel) related:

$(UNAME_M) shows ""x86-64"".
shell sysctl -n hw.optional.arm64 outputs an error that should be ignored.
Add additional comment on -framework Accelerate.


It's quite verbose to print help message per make, let's just echo a tip.

Build on macOS 13.2.1 (22D68), with Apple clang version 14.0.0 (clang-1400.0.29.202).",3,0
338,2023-03-20T20:19:11Z,2023-03-21T22:19:12Z,2023-03-21T22:19:12Z,2,67,0,"Not a developer, so my git-fu is a bit rusty. Hopefully this pull request covers everything?!?


Add shadow ./model.sha256 dir containing a dir for each model and a corresponding checklist.sha256 containing sha256 sums of the *.pth bin and *json files


Add script ./model.sha256/chk_sha256sums.sh to walk user-supplied ./models subdir and run sha256sum against above files to diff checklist.sha256 for each model


Update README.md with corresponding instructions",5,14
341,2023-03-20T22:07:32Z,,2023-03-28T17:17:16Z,5,565,22,"The code in PR lets you run llama the first time, but the second time the program crashes. This is due to memory access violation when trying to access any data related to the model or vocab. I don't know why, but current implementation cannot serialize pointers in the magic properly. I suspect this is due to allocating them using new operator which generates addresses outside of mapped memory range, guess on the second run those pointers are simply not interpretable and point to nowhere.
Major changes:

custom malloc renamed to _malloc. Otherwise linker is complaining when linking against CRT as there are multiple definitions for malloc. I tried to do undef malloc to remove references for the original one, but did not verify yet if it makes any difference.
WinMap fix, set access flag to FILE_MAP_COPY when loading second time. See https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-mapviewoffile.
Implement madvise for win. I can't verify if this change makes any difference as the pointers serialization is broken as of now, but it seems to be working.
Implement msync for win. Same comment as for madvise.

Some of the fixes to make it compile include:

#define NOMINMAX
using appropriate stat structure for win.
define MS_ASYNC for win.

This PR code will likely not compile on Linux right now (and it's also quite crappy as I did rapid changes just to make it compile at least). So if there are people using Visual Studio, I hope this PR will lay some foundation for further development.",7,10
348,2023-03-21T08:28:06Z,2023-03-21T15:42:43Z,2023-03-21T15:42:44Z,3,12,5,"Allow specifying a custom amount of model parts, if N < 1 use the default determined from model dimensions.",4,1
354,2023-03-21T14:16:15Z,2023-03-22T17:16:36Z,2023-03-22T17:16:36Z,3,10,3,"Sometimes we might want to use a reverse prompt but we want to let the model generate tokens right after the initial prompt. So we don't force user input mode if the -i flag wasn't specified and instead let it run until we encounter the reverse prompt.
This gives use some more flexibility, since it doesn't force the user to enter a newline if they want to let the model generate text right after the initial prompt and only be asked for input if the reverse prompt is encountered.",5,4
370,2023-03-21T20:51:40Z,2023-03-22T05:32:36Z,2023-03-22T05:32:37Z,14,1949,1747,"This is pretty big change, but it has to happen in order to allow building more examples by reusing the code properly. It will also allow other programming languages to interface with the code.

Moved ggml_quantize_q4_0() and ggml_quantize_q4_1() from utils to ggml
Added llama.h and llama.cpp that implement most of the model handling that was previously in main.cpp
Moved the tokenizer code into the new llama library
Model quantization is now also inside the new llama library, exposed through the API
Almost all of the changes are simply moving code around, no new stuff

Please test that everything is working. I haven't tested the perplexity parameter at all.
Sorry if this conflicts with your ongoing work!",10,4
375,2023-03-22T01:47:45Z,2023-03-28T19:44:30Z,2023-03-28T19:44:30Z,1,18,0,"__FMA__ and __F16C__ are defined in GCC and Clang
__FMA__ and __F16C__ are not defined in MSVC, however they are implied with AVX2/AVX512
https://learn.microsoft.com/en-us/cpp/build/reference/arch-x64?view=msvc-160
Thus, enable FMA and F16C in MSVC if either AVX2/AVX512 is enabled",6,19
376,2023-03-22T02:41:30Z,2023-03-23T02:20:35Z,2023-03-23T02:20:35Z,1,21,0,"Otherwise the tests may be ran and pass even if the build has errors and thus the step itself can pass with errors in build
edit: separate build and test steps for all platforms.",2,2
380,2023-03-22T05:38:38Z,2023-03-23T16:51:26Z,2023-03-23T16:51:26Z,1,2,2,We might want to add a Nix CI job to ensure it doesn't get desynced. @prusnak thoughts?,3,3
383,2023-03-22T07:35:27Z,2023-03-22T17:29:07Z,2023-03-22T17:29:07Z,5,119,113,"As suggested in #356, this de-duplicates the code in ggml_quantize_q4_0 and ggml_quantize_q4_1, which were recently moved to ggml.c
To ensure deterministic creation of model files, I introduced a new ""reference"" implementation for the q4_0 quantization. For q4_1 this wasn't necessary, as that has no SIMD optimizations.
This quashes @ggerganov's hope of making the quantize program faster, but I believe deterministic model files are more important.
Note that the checksum for models/7B/ggml-model-q4_0.bin is wrong in SHA256SUMS, see #374",3,2
390,2023-03-22T12:00:06Z,2023-03-22T16:09:39Z,2023-03-22T16:09:39Z,2,4,2,"fixes #385
replaces #389",2,0
392,2023-03-22T13:47:10Z,2023-03-22T16:37:11Z,2023-03-22T16:37:11Z,1,3,1,this change allows you to add_subidrectoy(llama.cpp) and target_link_libraries(target PRIVATE llama),2,0
393,2023-03-22T14:06:01Z,2023-03-28T16:39:01Z,2023-03-28T16:39:01Z,4,27,1,"This allows llama.cpp to be called directly from Swift! First add https://github.com/ggerganov/llama.cpp to your Package.swift or Xcode project, selecting either this branch or master (once the PR is merged).
Here’s a basic example of calling from Swift:
import llama

let ctx = llama_init_from_file(url.path(percentEncoded: false), llama_context_default_params())

let promptTokens = Array<llama_token>(unsafeUninitializedCapacity: prompt.utf8.count) { buffer, initializedCount in
  initializedCount = Int(llama_tokenize(ctx, prompt, buffer.baseAddress, Int32(buffer.count), true))
}
for var token in promptTokens {
  llama_eval(ctx, &token, 1, Int32(tokens.count), nThreads)
}

while true { // should stop after reaching context limit!
  var token = llama_sample_top_p_top_k(ctx, nil, 0, topK, topP, temperature, 1)
  if token == llama_token_eos() {
    print(""[end of text]"")
    break
  }
  print(String(cString: llama_token_to_str(ctx, token)), terminator: """")
  llama_eval(ctx, &token, 1, Int32(tokens.count), nThreads)
}

llama_free(ctx)",3,3
407,2023-03-22T19:24:48Z,2023-04-13T21:50:42Z,2023-04-13T21:50:42Z,1,21,15,"Adds support for batch size to perplexity.  This allows you to run perplexity on context sizes of 2048 (although still limited to a batch size of 512 there, as that's the max currently supported).  We set batch size to the min of user defined batch size and the context size.
Note though, the small batch sizes 8 are significantly slower than 512, and also that perplexity takes a hit (from switching off BLAS):
$ ./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw
perplexity : calculating perplexity over 655 chunks, batch_size=8
19.95 seconds per pass - ETA 3.63 hours
[1]4.5949,

$ ./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw -b 512
13.41 seconds per pass - ETA 2.44 hours
[1]4.3800,

For larger batch sizes, things match well though:
$ ./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw -c 2048 -b 512
68.21 seconds per pass - ETA 3.09 hours                                                                                               
[1]4.0474,

$ ./perplexity -m models/7B/ggml-model-q4_0.bin -f wiki.test.raw -c 2048 -b 64
208.50 seconds per pass - ETA 9.44 hours
[1]4.0474,

Also, fixes the batch size passed into llama_eval so it's not off by one, although I didn't notice much speed difference for large sizes.",5,17
408,2023-03-22T20:30:04Z,,2023-04-13T12:54:37Z,1,10,1,"Update to use the correct file name consolidated.*.pth. tools.sh is still looking for ggml-model-f16.bin* ""  skipping the file convertions. I wasn´t able to test this change  because for some reason when building the docker image in windows it can find the shell.",6,5
409,2023-03-22T21:24:08Z,2023-03-23T00:30:24Z,2023-03-23T00:30:24Z,1,1,1,,3,1
416,2023-03-23T07:39:43Z,2023-03-23T10:26:19Z,2023-03-23T10:26:19Z,1,0,53,"Delete this for now to avoid confusion since it contains some wrong checksums from the old tokenizer format
Re-add after #374 is resolved",5,6
418,2023-03-23T08:00:40Z,2023-03-23T11:41:32Z,2023-03-23T11:41:32Z,1,3,1,"Trivial change to fix output from the Makefile when printing the line about running with -h for help. To properly output the vt100/ANSI escape codes, it needs to be echo -e not just echo which will output the string like:
\x1b[36mrun ./main -h for help\x1b[0m

The echo line also can use single quotes because variable interpolation isn't required here.",3,4
420,2023-03-23T09:11:38Z,2023-03-25T20:29:22Z,2023-03-25T20:29:22Z,1,37,18,"From my testing this fixes the problems with UTF-8 inputs and outputs.
I dislike having to add this platform-specific code but it has to be done for it to work properly.
The only other option would be guiding users to go deep into settings to enable a ""beta"" UTF-8 feature...sigh.
At least is wrapped somewhat nicely as its' own function outside the main loop.",5,2
421,2023-03-23T09:49:01Z,2023-03-23T11:30:41Z,2023-03-23T11:30:41Z,2,23,36,"Updates to README.md model section

Removed Model section from issue template as it is better suited to be in README.md 
Removed IPF down links for the Alpaca 7B models as these look to be in the old data format and probably shouldn't be directly linked to, anyway
Updated the Perplexity section to point at #406 discussion

@antimatter15 if you have some updates on where to get the v2 Alpaca models and their sha256 sums to add to SHA256SUMS do you want to add them to this branch?",2,0
423,2023-03-23T11:21:01Z,2023-03-23T20:18:13Z,2023-03-23T20:18:13Z,1,8,13,Should fix #362,4,1
424,2023-03-23T11:50:53Z,2023-03-23T17:54:28Z,2023-03-23T17:54:28Z,1,84,17,"Bound check command line argument parameters so it doesn't crash / do out-of-bounds read when entering empty param.
Outputs a ""invalid parameter"" error in this case.
before: ""./main -r "" = crash",3,2
426,2023-03-23T12:05:16Z,2023-03-25T12:03:19Z,2023-03-25T12:03:19Z,3,13,0,"--in-prefix STRING command line option prefixes user inputs with STRING
For example, chatting with bob:
./main -m ./models/llama-13B-ggml/ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 -f ./prompts/chat-with-bob.txt -i -r ""User:"" --in-prefix "" ""
adds a space after the reverse prompt ""User:""
So instead of
Bob: How can I help you?
User:_

its
Bob: How can I help you?
User: _

and matches the original prompt better.
It could be useful for other prompts too, alignment or maybe testing multiple similar questions like ""What do you think about X"" or whatever.",4,8
428,2023-03-23T13:07:18Z,2023-03-23T20:42:52Z,2023-03-23T20:42:52Z,1,1,0,"Previously, python quantize.py --models-path .. 7B 13B would fail to find ../7B/ggml-model-f16.bin Now, it computes the absolute path to the models and uses that instead which works.
Fixes #431",3,3
430,2023-03-23T14:14:19Z,2023-03-23T20:16:49Z,2023-03-23T20:16:49Z,1,10,0,"BUILD_SHARED_LIBS to allow llama library to be generated.
Resolve #412.",3,0
434,2023-03-23T17:53:29Z,2023-03-25T05:26:29Z,2023-03-25T05:26:29Z,2,39,10,"A new pair of properties have been added to llama_context_params, a function pointer and an arbitrary context pointer that will be passed to the function pointer along with estimated read progress. The handler will be called (if provided) whenever dots are printed out while loading the model.",2,1
438,2023-03-23T18:24:04Z,,2023-03-24T10:19:14Z,3,34,10,"uses observations made in #213 and replaces it.
fixes ggml_new_tensor_impl: not enough space in the context's memory pool and resulting Segfaults.
this is still as much of a hack as it was before, but this time it is working.
this could potentially fix a bunch of issues. ( fixes #153 )",5,10
444,2023-03-23T22:30:20Z,2023-03-24T15:23:10Z,2023-03-24T15:23:10Z,1,14,12,Added explicit bolded instructions clarifying that people need to request access to models from Facebook and never through through this repo.,3,0
453,2023-03-24T03:08:33Z,2023-03-24T15:19:05Z,2023-03-24T15:19:06Z,7,91,12,"This is enabled by a new --mlock command line option.
Using mlock() disables swapping and memory compression for the model data.  Doing so can be useful on systems where the model takes up a large fraction of system RAM.  In my experience, macOS is quite eager to start compressing llama.cpp's memory, which then makes it halt for a few seconds while it decompresses, even with a model that uses ""only"" 25GB out of 32GB.
Of course, this comes at the cost of forcing the system to swap or compress other processes' memory instead, so it needs to be used with care and shouldn't be enabled by default.
In theory it should be possible to support this on Windows as well using VirtualLock(), but I'm not much of a Windows user.",3,1
454,2023-03-24T03:26:04Z,2023-03-24T15:19:27Z,2023-03-24T15:19:27Z,2,83,3,"This adds further optimizations for POWER9 CPUs (tested on Fedora with a dual-8 64-thread system):

use -mcpu=power9 for both C and C++, implies -mpower9-vector and everything else
use VSX half-precision conversion for F16 <-> F32
vectorized quantize_row_q4_0",2,0
458,2023-03-24T09:45:14Z,2023-03-28T16:48:20Z,2023-03-28T16:48:21Z,11,185,117,"This enables -Wdouble-promotion and syncs the Makefile and CMakeLists.txt with regards to warnings.
Reasoning:
The llama.cpp codebase depends on the correct use of number types, whether those are float, double or some of the spezialized types such as q4_0. Converting from one type to another should be a concious decision and not happen by accident.
In order to avoid any type promotion warnings, I have updated the code base, sometimes by making an implicit cast explicit, but in some places I did change the actual semantics. I'm not confident at this point that all changes are good.
Consequences:

Inference output changes, though I wouldn't say for the worse. Perplexity has an ETA of 20 hours on my machine, so I haven't run that yet.
q4_0 quantization is identical.

Further steps if and when this PR is merged:

Enable -Werror?",5,8
477,2023-03-24T21:45:39Z,,2023-05-26T19:50:55Z,8,212,0,"This adds a --trace  option that exports the decoder activations to a file. The format can be read with:
python -m examples.traceparser trace.bin

It's a basic app that comes with a parser, designed to help building analysis tools and tests. For now, it only replicates the soft max and the top k and nucleus filtering.
The format is versioned and should be easily extendable with more data (eg. embeddings, insertions in caches).
I'm using it for the same purpose as #246  (which is retired now) - to perform numerical analysis in Python for exploring #331.
Is this approach acceptable and useful to others?
This might become redundant once Python bindings allows to do the same thing in process memory, without going through a serialized format.",3,3
478,2023-03-24T22:18:13Z,2023-03-25T14:34:23Z,2023-03-25T14:34:23Z,1,13,4,"This should help measure the prompt processing times, which will be useful when optimizing this part.
This is pretty much a hack since it assumes that anything with batch size > 1 is part of the prompt, but I don't see any other way to tell the difference from llama.cpp at the moment, which is where the other timings are measured and printed at the moment. Another option would be adding the timings to main.cpp instead.",2,0
483,2023-03-25T00:25:59Z,2023-03-25T05:21:24Z,2023-03-25T05:21:24Z,1,1,1,"llama_sample_top_p_top_k was missing the struct annotation on line 126.
This causes a compiler issue when being parsed by the Kotlin C interop generator.
This commit fixes the above issue by adding the struct annotation.",3,1
491,2023-03-25T11:52:12Z,2023-03-26T05:25:46Z,2023-03-26T05:25:46Z,1,4,1,"Allow exiting the interactive prompt also with CTRL-D on Unix and CTRL-Z on Windows.
CTRL-D is so in my muscle memory, I never press anything else.",2,1
497,2023-03-25T14:34:29Z,2023-03-25T21:38:12Z,2023-03-25T21:38:12Z,2,114,13,"CMake: Add AVX512 option


CI: Add AVX/AVX512 builds (Windows)
(AVX512 tests can only be run when the worker happens to support it, building works anyway)


CI: Fix release tagging
(change @zendesk/action-create-release to @anzz1/action-create-release until upstream PR zendesk/action-create-release#32 is merged)


CMake: Fix sanitizer linkage ( merged #468 )


CI: Add sanitizer builds (Ubuntu)


Edit: everything should be done now.",3,1
509,2023-03-25T20:53:40Z,2023-03-28T17:13:02Z,2023-03-28T17:13:02Z,1,51,252,"To avoid code duplication when implementing additional quantization formats (#456), refactor the forward_mul_mat and forward_get_rows functions to use a table of function pointers, indexed by ggml_type.
This makes some functions non-inlined, I didn't see a regression in performance on my machine.
I tried to fix the ""unused variable"" warnings without complicating things too much, some are used in asserts.",4,3
514,2023-03-25T23:19:26Z,2023-03-26T14:48:42Z,2023-03-26T14:48:42Z,1,2,0,"Currently the Docker image is only published for linux/amd64. This PR adds support for published the linux/arm64 image.
This issue was encountered during this PR for Serge: serge-chat/serge#66
Source for this change: https://docs.docker.com/build/ci/github-actions/multi-platform/",3,2
515,2023-03-26T00:06:42Z,2023-03-28T18:06:03Z,2023-03-28T18:06:04Z,1,89,2,"Largely based on the AVX2 implementation of quantize_row_q4_0.
Run on (16 X 3600 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x8)
  L1 Instruction 32 KiB (x8)
  L2 Unified 256 KiB (x8)
  L3 Unified 16384 KiB (x1)
Load Average: 0.17, 1.04, 1.50
-------------------------------------------------------------------
Benchmark                         Time             CPU   Iterations
-------------------------------------------------------------------
BM_quantize_row_q4_1_ref      12845 ns        12845 ns        54677
BM_quantize_row_q4_1_avx       1360 ns         1360 ns       519134


🤖 Generated by Copilot at ae08d8e
Summary
🚀🐛♻️

Improved matrix quantization with AVX2 and bug fixes. Added a new function quantize_row_q4_1 that uses AVX2 instructions to speed up the quantization of a matrix row using 4-bit factors. Renamed and fixed the original function quantize_row_q4_1_reference. Updated ggml_quantize_q4_1 to use the appropriate function depending on the CPU capabilities.

We're sailing on the matrix sea, with quantize_row_q4_1
We've fixed a bug and gained some speed, with quantize_row_q4_1
So heave away, me hearties, heave away with glee
We'll raise the sail and catch the wind, with quantize_row_q4_1

Walkthrough

Rename quantize_row_q4_1 to quantize_row_q4_1_reference to avoid confusion with the new AVX2-optimized function (link)
Add quantize_row_q4_1 that uses AVX2 instructions to speed up the quantization algorithm for 4-bit factors (link)
Replace quantize_row_q4_1 with quantize_row_q4_1_reference in ggml_quantize_q4_1 to fix a bug and avoid unnecessary computation (link)",4,12
526,2023-03-26T14:26:06Z,,2023-03-26T19:16:22Z,1,20,14,Add support for reading older model files so that people do not have to throw out ggml alpaca models.,6,4
527,2023-03-26T14:27:11Z,2023-03-26T15:48:40Z,2023-03-26T15:48:40Z,1,4,2,discovered ggml.c does not compile in Debug right now.,3,8
529,2023-03-26T14:55:01Z,,2023-03-27T13:34:53Z,1,12,7,"Do not insert a ""newline"" token if user inputs empty line. This let's
user to continue the output after she has been asked by reverse prompt
for more data. Otherwise an empty-line input would insert a ""newline""
token which would break the flow of the conversation.",4,7
539,2023-03-26T20:48:52Z,2023-03-28T17:55:43Z,2023-03-28T17:55:43Z,2,101,1,Followup to: #526,4,3
540,2023-03-26T21:37:43Z,2023-03-28T06:11:10Z,2023-03-28T06:11:10Z,1,5,2,Added the embedding example to the Makefile as it was missing.,2,0
542,2023-03-26T23:35:41Z,2023-03-27T04:55:26Z,2023-03-27T04:55:26Z,4,4,4,"Without this I get linking errors on x86_64-w64-mingw32 (on linux I don't need this patch):
[01:33:38] [ 50%] Linking CXX executable ../../bin/main.exe
[01:33:38] cd /workspace/srcdir/llama.cpp/build/examples/main && /usr/bin/cmake -E cmake_link_script CMakeFiles/main.dir/link.txt --verbose=true
[01:33:38] /usr/bin/cmake -E rm -f CMakeFiles/main.dir/objects.a
[01:33:38] /opt/bin/x86_64-w64-mingw32-libgfortran5-cxx11/x86_64-w64-mingw32-ar qc CMakeFiles/main.dir/objects.a @CMakeFiles/main.dir/objects1.rsp
[01:33:38] /opt/bin/x86_64-w64-mingw32-libgfortran5-cxx11/x86_64-w64-mingw32-g++ --sysroot=/opt/x86_64-w64-mingw32/x86_64-w64-mingw32/sys-root/ -O3 -DNDEBUG -Wl,--whole-archive CMakeFiles/main.dir/objects.a -Wl,--no-whole-archive -o ../../bin/main.exe -Wl,--out-implib,libmain.dll.a -Wl,--major-image-version,0,--minor-image-version,0 @CMakeFiles/main.dir/linklibs.rsp
[01:33:38] /opt/x86_64-w64-mingw32/bin/../lib/gcc/x86_64-w64-mingw32/8.1.0/../../../../x86_64-w64-mingw32/bin/ld: CMakeFiles/main.dir/objects.a(common.cpp.obj):common.cpp:(.text+0x4c8): undefined reference to `ggml_mlock_supported'
[01:33:38] collect2: error: ld returned 1 exit status",2,2
545,2023-03-27T04:11:54Z,2023-04-14T07:03:03Z,2023-04-14T07:03:03Z,9,1154,1261,"Current status: Working, except for the latest GPTQ-for-LLaMa format that includes g_idx.  This turns out to require changes to GGML, so for now it only works if you use the --outtype option to dequantize it back to f16 (which is pointless except for debugging).
I also included some cleanup for the C++ code.
This script is meant to replace all the existing conversion scripts (including the ones that convert from older GGML formats), while also adding support for some new formats.  Specifically, I've tested with:

 LLaMA (original)
 llama-65b-4bit
 alpaca-native
 alpaca-native-4bit
 LLaMA converted to 'transformers' format using convert_llama_weights_to_hf.py
 alpaca-native quantized with --true-sequential --act-order --groupsize 128 (dequantized only)
 same as above plus --save_safetensors
 GPT4All
 stock unversioned ggml
 ggmh
 alpaca-30b-4bit.pt
 alpaca-30b-4bit.safetensors
 alpaca-30b-4bit-128g.safetensors
 koala-13B-HF
 koala-13B-4bit-128g.safetensors (dequantized only)
 koala-13B-4bit-128g.pt

There's enough overlap in the logic needed to handle these different cases that it seemed best to move to a single script.
I haven't tried this with Alpaca-LoRA because I don't know where to find it.
Useful features:


Uses multiple threads for a speedup in some cases (though the Python GIL limits the gain, and sometimes it's disk-bound anyway).


Combines split models into a single file (both the intra-tensor split of the original and the inter-tensor split of 'transformers' format files).  Single files are more convenient to work with and more friendly to future changes to use memory mapping on the C++ side.  To accomplish this without increasing memory requirements, it has some custom loading code which avoids loading whole input files into memory at once.


Because of the custom loading code, it no longer depends in PyTorch, which might make installing dependencies slightly easier or faster... although it still depends on NumPy and sentencepiece, so I don't know if there's any meaningful difference.  In any case, I also added a requirements.txt file to lock the dependency versions in case of any future breaking changes.


Type annotations checked with mypy.


Some attempts to be extra user-friendly:


The script tries to be forgiving with arguments, e.g. you can specify either the model file itself or the directory containing it.


The script doesn't depend on config.json / params.json, just in case the user downloaded files individually and doesn't have those handy.  But you still need tokenizer.model and, for Alpaca, added_tokens.json.


The script tries to give a helpful error message if added_tokens.json is missing.",23,61
546,2023-03-27T04:53:48Z,2023-03-28T19:43:26Z,2023-03-28T19:43:26Z,1,3,5,"CI: Fix subdirectory path globbing
CI: Fix Github runner AVX512F detection (Windows)",4,5
547,2023-03-27T05:38:11Z,2023-03-28T17:02:34Z,2023-03-28T17:02:35Z,1,0,2,Removed unused model variable and verified that the code functions correctly with vocab_only setting. Also confirmed that the code works as expected after running with reduced memory usage due to deletion of no-longer-needed variable.,4,0
551,2023-03-27T07:54:37Z,2023-03-28T18:23:09Z,2023-03-28T18:23:09Z,5,5,5,"Reverting 7e53955 (#542)
ggml shouldn't be linked twice as it's already linked to llama (probably common should be linked to ggml instead) .
ref: 7e53955#commitcomment-106187813

 Reverts 7e53955 (#542)
 Needs someone with minGW to post a proper solution that works (thanks @marcom).",4,5
555,2023-03-27T13:33:55Z,,2023-03-28T11:26:27Z,9,537,49,"move instruct mode from main.cpp to instruct.cpp
entering empty line passes back control without new input in interactive/instruct modes
endless instruct mode with --n_predict -1
small refactorings

Closes

#508",7,15
563,2023-03-27T21:38:28Z,2023-03-28T14:26:56Z,2023-03-28T14:26:56Z,1,24,1,"Sandy bridge supports AVX but not F16C. Should fix #562
@kaufmannr can you verify if this fixes your problem?",4,1
564,2023-03-28T00:54:42Z,2023-03-28T21:26:25Z,2023-03-28T21:26:25Z,2,4,1,"I have created my own library for mingw32 that includes the POSIX functions that the source needs to compile. That is how I will support llama.cpp for windows using mingw32 compilation in mys2. Compiled successfully using:
make LDFLAGS='-D_POSIX_MAPPED_FILES -lmingw32_extended'",3,7
571,2023-03-28T12:44:09Z,2023-03-28T14:09:56Z,2023-03-28T14:09:56Z,3,143,118,"moved fixes and refactorings from #555

main: entering empty line passes back control without new input in interactive/instruct modes
instruct mode: keep prompt fix
instruct mode: duplicate instruct prompt fix
refactor: move common console code from main->common",2,0
577,2023-03-28T14:58:28Z,2023-03-29T16:10:07Z,2023-03-29T16:10:07Z,1,1,1,"Commit 4640eff disabled ggml's multi-threading when OpenBLAS is used for processing large prompts.
This avoids running two thread pools at the same time.
However, OpenBLAS is used by ggml on tensors with dims >= 32, but llama.cpp only reduce the number of threads for batch size > 255.
See also this discussion: #229 (reply in thread) and issue #578",4,4
581,2023-03-28T15:52:58Z,,2023-03-30T19:42:06Z,1,64,41,This change uses the Python multiprocessing module to parallelize in the script quantize.py the multiple calls to the quantize native executable.,3,2
583,2023-03-28T21:08:48Z,2023-03-29T15:10:24Z,2023-03-29T15:10:24Z,2,35,0,"Example implementing a version of the reason-act pattern. It's a neat example of what you can do with the 7B model, at least the instruction tuned versions. Paper.
Adds examples/reason-act.sh and prompts/reason-act.txt.",4,2
584,2023-03-28T22:02:00Z,2023-03-29T20:44:39Z,2023-03-29T20:44:39Z,1,7,1,Now with 100% less base64 encoding,2,0
586,2023-03-29T00:44:59Z,,2023-03-30T00:31:32Z,4,121,23,"Significantly reduces model loading times, especially if they are already cached by the OS.


Requires a single-part model to work, either a 7B model or any model converted with #545 (use with --n_parts 1).


Only tested on Linux, needs to be tested with other POSIX-compatible operating systems. Should work on OS X as is.


The tensors may not be aligned, which may cause performance issues or crashes on some microarchitectures. However, testing by @jart showed no issues so far. This needs to be fixed in the conversion script.


Still missing:

 Unmap the file in llama_free
 Test with other POSIX operating systems
 Windows support
 Consider enabling CoW to support replacing some of the tensors

Co-authored with @jart.

🤖 Generated by Copilot at ef9afe1
Summary
✨⚡🐧

Added a feature to llama.cpp and ggml.c that allows loading and evaluating models from memory mapped files. This can improve performance and reduce memory usage for large models. Added a new parameter no_alloc to the ggml_context and ggml_init_params structs to control this feature. Implemented memory mapping for Unix-like systems and left a placeholder for Windows.

Sing, O Muse, of the swift and skillful programmers
Who devised a clever way to load the models of GGML
With no allocation of memory, but using the mapped addresses
Of the files, like the Cyclops who forged the thunderbolts of Zeus.

Walkthrough

Add a feature to support memory mapping of model files (link, link, link, link, link, link, link, link, link, link, link, link, link, link)
Introduce a new field no_alloc in ggml_context and ggml_init_params to indicate whether the context should allocate memory for the tensor data or not (link, link)
Initialize the no_alloc field from the params argument in ggml_init (link)
Check the no_alloc field in ggml_new_tensor_impl to prevent allocating memory for the tensor data and to assign the data field to the memory mapped address if available (link, link)
Set the no_alloc field to false in ggml_opt and llama_eval_internal to ensure that the optimization and evaluation processes allocate memory for the intermediate tensors as usual (link, link)
Add some headers for POSIX mmap to llama.cpp (link)
Add a new function mmap_file to llama.cpp to try to memory map a given file and return the mapped address or NULL on failure (link)
Add some logic to llama_model_load to determine whether to use memory mapping or not based on the number of model parts and the success of mmap_file (link)
Skip adding the size of the tensors that can be memory mapped to the context size in llama_model_load (link)
Set the no_alloc field to the value of use_mmap in llama_model_load (link)
Add a message to indicate whether memory mapping is used or not when loading a model part in llama_model_load (link)
Handle the case of memory mapping in llama_model_load by setting the tensor data pointer to the offset from the mapped address and advancing the file pointer accordingly (link)
Set the no_alloc field to false in kv_cache_init to ensure that the key-value cache allocates memory for the tensors as usual (link)",9,33
592,2023-03-29T08:09:52Z,2023-03-29T17:21:09Z,2023-03-29T17:21:09Z,1,57,0,"Same script than examples/chat-13B.sh, but for windows users. Tested and working on windows 10/11 v 22H2
The next step for me would be to create a single batch script where you choose the LLaMA model and the first context. Then run it in interactive mode. My goal is to allow any windows user to run the model in the easiest way possible.
Regards,",4,4
593,2023-03-29T11:23:38Z,2023-03-29T13:19:29Z,2023-03-29T13:19:29Z,1,1,1,#ifdef and defined don't mix,2,0
598,2023-03-29T15:45:58Z,2023-03-31T19:19:17Z,2023-03-31T19:19:17Z,2,6,4,"The Makefile has -std=c11/-std=c++11, but those were missing in CMakeLists.txt. Add them and fix the warnings.",6,12
600,2023-03-29T17:06:48Z,2023-03-29T18:09:26Z,2023-03-29T18:09:26Z,1,0,0,to match filenames of other converters,2,0
605,2023-03-29T19:37:00Z,2023-03-31T08:32:01Z,2023-03-31T08:32:01Z,6,27,29,"use f-strings where possible
drop first param of encode/decode functions since ""utf-8"" is the default",2,2
607,2023-03-29T21:12:40Z,2023-03-30T17:53:35Z,2023-03-30T17:53:35Z,1,0,1,"It seems some new warnings were added recently that exposed this.
I wrote the code that included this unused variable originally and it is indeed not needed.",3,0
609,2023-03-29T22:21:34Z,2023-04-02T07:17:05Z,2023-04-02T07:17:05Z,1,2,89,"On my 2019 Mac Pro I have these CPU features:
$ sysctl machdep.cpu.leaf7_features
machdep.cpu.leaf7_features: RDWRFSGS TSC_THREAD_OFFSET BMI1 AVX2 FDPEO SMEP BMI2 ERMS INVPCID PQM FPU_CSDS MPX PQE AVX512F AVX512DQ RDSEED ADX SMAP CLFSOPT CLWB IPT AVX512CD AVX512BW AVX512VL PKU AVX512VNNI MDCLEAR IBRS STIBP L1DF ACAPMSR SSBD

Although I was wondering: Why not use -march=native?
EDIT:
Using -march=native -mtune=native on x86 now. Could potentially be extended to other architectures, although the meaning of -march, -mtune and -mcpu is a bit convoluted across different architectures.",4,7
613,2023-03-29T23:51:39Z,2023-03-30T19:28:25Z,2023-03-30T19:28:25Z,11,717,328,"This is a breaking change that's going to give us three benefits:

Your inference commands should load 100x faster
You may be able to safely load models 2x larger
You can run many concurrent inference processes

This was accomplished by changing the file format so we can mmap()
weights directly into memory without having to read() or copy them
thereby ensuring the kernel can make its file cache pages directly
accessible to our inference processes; and secondly, that the file
cache pages are much less likely to get evicted (which would force
loads to hit disk) because they're no longer competing with memory
pages that were needlessly created by gigabytes of standard i/o.
The new file format supports single-file models like LLaMA 7b, and
it also supports multi-file models like LLaMA 13B. Our Python tool
now merges the foo.1, foo.2, etc. files back into a single file so
that the C++ code which maps it doesn't need to reshape data every
time. That's made llama.cpp so much simpler. Much of its load code
has now been deleted.
Furthermore, this change ensures that tensors are aligned properly
on a 32-byte boundary. That opens the door to seeing if we can get
additional performance gains on some microprocessors, by using ops
that require memory alignment.
Lastly note that both POSIX and the Windows platform are supported
The issue this PR solves is #91
This PR was written in collaboration with @slaren. This PR is also rebased on
PR #586 so please do not squash merge! Use either merge or rebase.",15,21
615,2023-03-30T00:50:37Z,2023-03-30T17:34:45Z,2023-03-30T17:34:45Z,1,4,1,...there was no check.  ported upstream from zanussbaum#2 (I dont see any clean path for upstream patches),2,0
617,2023-03-30T05:10:47Z,2023-03-31T11:55:44Z,2023-03-31T11:55:44Z,1,153,0,"My old Xeon E5-2670 doesn't have AVX2 support. So I have added AVX acceleration to quantize_row_q4_0() and ggml_vec_dot_q4_0().
Here is the result before the change:
./main -m ./models/ggml-alpaca-7b-q4.bin --color -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 4 -p 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Tell me about alpacas.'
main: seed = 1680149433
llama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 512
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 2
llama_model_load: n_ff    = 11008
llama_model_load: n_parts = 1
llama_model_load: type    = 1
llama_model_load: ggml ctx size = 4273.34 MB
llama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)
llama_model_load: loading model part 1/1 from './models/ggml-alpaca-7b-q4.bin'
llama_model_load: .................................... done
llama_model_load: model size =  4017.27 MB / num tensors = 291
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 30 | AVX = 1 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: temp = 0.200000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000
generate: n_ctx = 512, n_batch = 256, n_predict = 128, n_keep = 0


 Below is an instruction that describes a task. Write a response that appropriately completes the request. Tell me about alpacas. Alpacas are a species of South American camelid that are bred primarily for their fleece. They are smaller than llamas, and have a finer, softer fleece that is lighter in weight and warmer in nature. Alpacas are shorn once a year, in the summer, and the fleece is then used for a variety of products, including clothing, home furnishings, and crafts. Alpacas are also bred for their meat, which is lean and flavorful. What is the difference between an alpaca and a ll
llama_print_timings:        load time =  4579.85 ms
llama_print_timings:      sample time =   451.16 ms /   128 runs   (    3.52 ms per run)
llama_print_timings: prompt eval time = 99743.01 ms /    29 tokens ( 3439.41 ms per token)
llama_print_timings:        eval time = 459193.73 ms /   127 runs   ( 3615.70 ms per run)
llama_print_timings:       total time = 565421.86 ms

and after:
./main -m ./models/ggml-alpaca-7b-q4.bin --color -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 4 -p 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Tell me about alpacas.'
main: seed = 1680151620
llama_model_load: loading model from './models/ggml-alpaca-7b-q4.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 512
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 2
llama_model_load: n_ff    = 11008
llama_model_load: n_parts = 1
llama_model_load: type    = 1
llama_model_load: ggml ctx size = 4273.34 MB
llama_model_load: mem required  = 6065.34 MB (+ 1026.00 MB per state)
llama_model_load: loading model part 1/1 from './models/ggml-alpaca-7b-q4.bin'
llama_model_load: .................................... done
llama_model_load: model size =  4017.27 MB / num tensors = 291
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 30 | AVX = 1 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: temp = 0.200000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000
generate: n_ctx = 512, n_batch = 256, n_predict = 128, n_keep = 0


 Below is an instruction that describes a task. Write a response that appropriately completes the request. Tell me about alpacas. Alpacas are domesticated animals that are related to camels and are native to South America. They are typically kept as livestock and are known for their wool, which is very soft and silky. Alpacas are shy and typically flee when they see humans, but they can also be very friendly and curious. They typically live in herds of 5 to 20 animals and can live up to 20 years in captivity. Alpacas are an important source of income for many South American families, as their wool can be sold for various products, such as cl
llama_print_timings:        load time =  5761.50 ms
llama_print_timings:      sample time =   464.30 ms /   128 runs   (    3.63 ms per run)
llama_print_timings: prompt eval time = 13906.61 ms /    29 tokens (  479.54 ms per token)
llama_print_timings:        eval time = 79917.90 ms /   127 runs   (  629.27 ms per run)
llama_print_timings:       total time = 101737.53 ms",7,10
626,2023-03-30T16:25:44Z,2023-04-02T10:21:31Z,2023-04-02T10:21:31Z,3,375,374,"This should solve #599 .
I was able to successfully run 30B/ggml-model-q4_0.bin with -c 6000 (and extended ctx_size but that's a different story), but I've not tested too many other cases. I'd like to hear feedback if this is a sensible approach before putting more effort in it.",4,7
642,2023-03-31T01:18:01Z,2023-03-31T15:55:52Z,2023-03-31T15:55:52Z,1,18,13,"This is a port of @perserk's and @sw's AVX implementation of ggml_vec_dot_q4_0 (#617) to AVX2.
------------------------------------------------------------------------
Benchmark                              Time             CPU   Iterations
------------------------------------------------------------------------
BM_ggml_vec_dot_q4_0_avx             668 ns          668 ns      1055239
BM_ggml_vec_dot_q4_0_avx2            578 ns          578 ns      1209367
BM_ggml_vec_dot_q4_0_avx2_new        522 ns          522 ns      1346143

Before:
llama_print_timings: prompt eval time = 10113.34 ms /   116 tokens (   87.18 ms per token)
llama_print_timings:        eval time = 20360.13 ms /   127 runs   (  160.32 ms per run)

perplexity : calculating perplexity over 655 chunks
42.05 seconds per pass - ETA 7.65 hours
[1]4.6512,[2]5.2613,[3]6.0903,

After:
llama_print_timings: prompt eval time =  7627.11 ms /   116 tokens (   65.75 ms per token)
llama_print_timings:        eval time = 19477.24 ms /   127 runs   (  153.36 ms per run)

perplexity : calculating perplexity over 655 chunks
31.88 seconds per pass - ETA 5.80 hours
[1]4.5619,[2]5.1787,[3]6.0491,",7,26
653,2023-03-31T14:52:06Z,2023-04-13T12:46:24Z,2023-04-13T12:46:24Z,2,276,1,"This is a small program I used benchmark the performance of the q4_0 matrix multiplication core.
I've used it to compare several implementations of ggml_vec_dot_q4_0.
I will reference this benchmark in another pull request for an improved AVX2 ggml_vec_dot_q4_0 implementation.",5,9
654,2023-03-31T16:21:54Z,2023-04-03T07:52:28Z,2023-04-03T07:52:28Z,1,64,35,"UPDATE:
@slaren  @rabidcopy @sw @howard0su
First: Let me say ""sorry"" for the confusion: I was not clear in my original post as to what the baseline was. This was mainly, because #642 was merged AFTER I had branched to run my tests of which I have reported the results.
When I published this pull request, I had not realized that #642 existed, and also not that it had been merged. My results were therefore based on the pre-#642 code version.
I have re-run the benchmarks now with three versions: the ""pre-PR642"" AVX2 code, the AVX2 code from #642 and the AVX2 code from this PR (#654).
At least on my machine I get (for 100 iterations of the benchmark from #653) a 14% performance increase, which is similar to what @jart reported in #654 (comment).
I have therefore changed the title of the PR.  Again, sorry for the confusion.
Please let me know your thoughts.
Results Summary (single thread performance)



Codebase
FLOPS (single thread
% compared to pre 642
% compared to with #642




pre #642
14769 FLOPS
100%
76%


with #642
19443 FLOPS
132%
100%


with #654
22217 FLOPS
150%
114 %



Please find the raw data of the benchmark runs here: benchmark-data.csv
Boxplot of the results:

Technical details

Machine: Intel(R) Core(TM) i5-4300M CPU @ 2.60GHz, 16 GB RAM
Compiler & switches:

I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread
I LDFLAGS:  
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0


PREVIOUS POST:
(I leave it here for sake of transparency)
This change produces a ~1.5x performance increase of ggml_vec_dot_q4_0 on AVX2.

Avg. FLOPS per uSecond: 14797 (100%) for existing code
Avg. FLOPS per uSecond: 21828 (147%) for code in this pull request

Root causes for performance improvement:

Existing code merges two m256i vectors in bytesFromNibbles and then seperates them again in ggml_vec_dot_q4_0
Existing code uses C inline functions which are apparently a bit harder to optimize for the compiler
Code in this pull request uses a combination of C macros and ""by-hand"" loop unrolling

Benchmark data for code in master branch (commit 02c5b27)  - produced with tool from pull request 653.



Iteration
NThreads
SizeX
SizeY
SizeZ
Required FLOPS
Elapsed uSeconds
FLOPS per uSecond




0
1
11008
4096
128
11542724608
783404
14734.06


1
1
11008
4096
128
11542724608
781391
14772.02


2
1
11008
4096
128
11542724608
783799
14726.64


3
1
11008
4096
128
11542724608
777324
14849.31


4
1
11008
4096
128
11542724608
781041
14778.64


5
1
11008
4096
128
11542724608
778367
14829.41


6
1
11008
4096
128
11542724608
777931
14837.72


7
1
11008
4096
128
11542724608
780476
14789.34


8
1
11008
4096
128
11542724608
778450
14827.83


9
1
11008
4096
128
11542724608
778300
14830.69



Benchmark data for code in this pull request:



Iteration
NThreads
SizeX
SizeY
SizeZ
Required FLOPS
Elapsed uSeconds
FLOPS per uSecond




0
1
11008
4096
128
11542724608
552244
20901.49


1
1
11008
4096
128
11542724608
540733
21346.44


2
1
11008
4096
128
11542724608
555696
20771.65


3
1
11008
4096
128
11542724608
553204
20865.22


4
1
11008
4096
128
11542724608
546703
21113.34


5
1
11008
4096
128
11542724608
505609
22829.35


6
1
11008
4096
128
11542724608
505045
22854.84


7
1
11008
4096
128
11542724608
525141
21980.24


8
1
11008
4096
128
11542724608
506639
22782.94


9
1
11008
4096
128
11542724608
505468
22835.72",8,22
656,2023-03-31T18:05:22Z,2023-04-01T14:08:40Z,2023-04-01T14:08:40Z,1,5,0,"Currently when -f is used with a non-existing or inaccessible file it fails silently, which can lead to confusion. This should fix that.",3,0
658,2023-03-31T19:02:17Z,2023-04-02T07:56:21Z,2023-04-02T07:56:21Z,1,15,0,"This adds an invocation script for gpt4all with the same parameters used at gpt4all.cpp, for convenience.",2,0
664,2023-03-31T22:38:34Z,,2023-04-13T12:49:31Z,2,27,10,"This PR refactors two Bash scripts to use proper argument parsing and fixes some bugs.
The first script (reason-act.sh) used an if-statement to check for the presence of a command-line argument and set a variable accordingly. This has been replaced with getopts to parse the -m option and its argument. The script now uses quotes to prevent word splitting and globbing, and has a shebang at the beginning.
The second script (chat-13B.sh) used several environment variables to specify various settings, and some of them had default values. This has been changed to use getopts to parse command-line options and their arguments. The default values have been moved to the variable definitions. Quotes have been added to variables to prevent word splitting and globbing. An error message has been added for invalid options.",2,2
665,2023-03-31T22:53:37Z,2023-04-02T22:13:03Z,2023-04-02T22:13:03Z,1,2,1,"Changes

Remove torch GPU dependencies as this is a CPU project

Results
Full Image - 5.7GB reduction in size:
$ podman images
REPOSITORY                           TAG                  IMAGE ID      CREATED         SIZE
<none>                               master               e589ee9cb803  19 minutes ago  7.32 GB
<none>                               bsilvereagle         e1ea3ddecfd8  4 seconds ago   1.62 GB

Maintenance Impacts

torch is pinned to version 2.0.0 which is a departure from the README etc which do not pin",2,1
678,2023-04-01T12:09:10Z,,2023-04-15T16:29:37Z,9,279,309,"(edit: the python changes will clash with #545)
This PR has several goals:

reduce the number of integer literals strewn across the code base
better ensure array and enum consistency
clear up possible confusion about file and tensor types
prepare the way for new quantization formats with QK != 32, as discussed in #456
deduplicate python definitions

For the python scripts, I introduce a new file ggml.py at the top level, which contains definitions of the file and tensor types equivalent to those in ggml.h. I have formatted that with black, in case #611 returns from the dead.
The changes to the python files on one hand and C/C++ on the other are technically independent, but the discussion will overlap, so I'm keeping this in one draft. I will split it later if it makes sense.
I have tested conversion from pth to ggml with identical outputs, but I have not tested the other conversion scripts.
Open questions:

should enum e_ftype be moved to llama.h (with sensible renaming)? This would allow us to eliminate the hard-coded 2,3 in the usage string of quantize.cpp, and maybe be useful elsewhere.
how is file type 4 used? I could not find any other parts of the code that would use this. I see now, it's for GPTQ models. Should I add FTYPE_GPTQ?. 
  
    
      llama.cpp/llama.cpp
    
    
         Line 515
      in
      3525899
    
  
  
    

        
          
           case 4: wtype = GGML_TYPE_Q4_1; vtype = GGML_TYPE_F16; break; 
        
    
  


Is GGML_FILE ok or should it be LLAMA_FILE? ggml.c doesn't deal with that type.

Some more comments below, looking for your thoughts on this...",4,4
680,2023-04-01T13:04:49Z,2023-04-01T14:57:30Z,2023-04-01T14:57:30Z,1,2,2,Play Store versions of Termux are deprecated. Changed to F-Droid version. More information in https://github.com/termux/termux-app#google-play-store-deprecated,2,4
682,2023-04-01T15:05:22Z,2023-04-02T07:18:53Z,2023-04-02T07:18:53Z,1,1,1,"...for vocab_only=true
Unless I'm misunderstanding the code base completely, the huge buffers are not needed for tokenizing.
Fixes #582.
convert-pth-to-ggml.py with vocab_only=1 produces identical files. nvm, that doesn't use the C/C++ code.",3,0
685,2023-04-01T15:43:21Z,2023-04-02T10:23:05Z,2023-04-02T10:23:05Z,2,44,0,"This PR contains a simple extension to the C-API for getting/setting the kv_cache so that an app can save the state of the kv_cache after providing a prompt and load this next time the app starts to avoid having to evaluate the prompt on startup.
The api provides access methods for retrieving the current memory buffer for the kv_cache and its token number. It also contains a method for setting the kv_cache from a memory buffer and a token count.
I did not start implementing the --cache-prompt argument since it is a bit more involved - we need to save some more state like the last_n_tokens and n_past parameter. We'd also need to hash the prompt, check if a prompt file existed etc.
Implements foundation for #64",3,2
689,2023-04-01T20:03:21Z,,2023-04-03T00:19:30Z,1,7,0,"I am not sure about comparing the float directly with 0.0f, so I used std::abs and FLT_EPSILON
Motivation:
Setting temp=0 does not work as expected #684",4,3
690,2023-04-01T21:08:14Z,2023-04-02T10:48:57Z,2023-04-02T10:48:57Z,1,4,2,"migrate-ggml-2023-03-30-pr613.py is needed to get gpt4all running.
When following the README instructions for gpt4all, I encounted the following error:
data/llama.cpp$ python3 convert-gpt4all-to-ggml.py models/gpt4all-models/gpt4all-lora-quantized.bin models/tokenizer.model

/data/llama.cpp$ ./main -m models/gpt4all-models/gpt4all-lora-unfiltered-quantized.bin -n 128
main: seed = 1680382134
llama_model_load: loading model from 'models/gpt4all-models/gpt4all-lora-unfiltered-quantized.bin' - please wait ...
models/gpt4all-models/gpt4all-lora-unfiltered-quantized.bin: invalid model file (bad magic [got 0x67676d66 want 0x67676a74])
    you most likely need to regenerate your ggml files
    the benefit is you'll get 10-100x faster load times
    see https://github.com/ggerganov/llama.cpp/issues/91
    use convert-pth-to-ggml.py to regenerate from original pth
    use migrate-ggml-2023-03-30-pr613.py if you deleted originals
llama_init_from_file: failed to load model
main: error: failed to load model 'models/gpt4all-models/gpt4all-lora-unfiltered-quantized.bin'
I think the gpt4all conversion script is a bit out of date and produces files that need to be converted with migrate-ggml-2023-03-30-pr613.py.
I was able to run python3 migrate-ggml-2023-03-30-pr613.py models/gpt4all-models/gpt4all-lora-unfiltered-quantized.bin models/gpt4all-models/gpt4all-lora-unfiltered-quantized-pr613.bin which seemed to do the trick. After that, ./main -m models/gpt4all-models/gpt4all-lora-unfiltered-quantized-pr613.bin ran successfully!
This PR updates the README to add that migrate-ggml-2023-03-30-pr613.py step.",3,1
697,2023-04-02T01:33:18Z,2023-04-02T02:41:12Z,2023-04-02T02:41:12Z,1,5,3,Fixes #573,3,1
700,2023-04-02T05:16:09Z,,2023-04-02T11:10:31Z,1,16,6,"Fixed some command issues, added model links and updated the Image",3,4
703,2023-04-02T09:22:35Z,2023-04-13T14:59:51Z,2023-04-13T14:59:51Z,1,7,5,"This should help the poor souls running llama.cpp without a supported SIMD optimization.
First, the good stuff: per-token eval times in milliseconds:
With -march=native -mtune=native, but SIMD optimization code in ggml_vec_dot_q disabled:




recent master (6e7801d)
this PR




Q4_0
2488
1320


Q4_1
2315
2023



Without -march=native -mtune=native, which causes the scalar code to be used without requiring modification:




recent master (6e7801d)
this PR




Q4_0
2840
1330


Q4_1
2388
2083



Of course it would be interesting to see measurements from machines that actually do not have any SIMD circuits.
I would like to hear if this causes a regression in speed anywhere.
I can only really imagine this if floating point operations were faster than integer operations, because that's what we're trading here.
Beware of the recent change to the Makefile (c4f89d8), and possible differences to CMakeLists.txt, if you run your own tests.
The disadvantage is that this would once again cause a change in output due to floating point non-associativity / rounding.
I'll repost my snippet here, though admittedly this uses doubles:
$ echo 'main(){printf(""%.16f\n%.16f\n"",.7+.2+.1,.7+.1+.2);}' | gcc -x c - && ./a.out
0.9999999999999999
1.0000000000000000

We had changes like this before, and I understand that not having reproducible results makes some people unhappy.
I don't see us achieving this across the processor-specific optimizations without a severe regression in performance.",4,3
706,2023-04-02T10:37:32Z,2023-04-13T13:03:39Z,2023-04-13T13:03:39Z,2,2,2,,3,2
709,2023-04-02T11:42:30Z,2023-04-11T15:03:52Z,2023-04-11T15:03:52Z,5,74,57,"As discussed in #678, this introduces enum llama_ftype to llama.h which represents the various file types used by llama.cpp
The goal is to improve maintainability of the code by not having integer literals in various places. I hope I didn't miss one.
I used the names and comments from @comex's pending changes to the Python scripts in #545. I have not touched the scripts so as not to cause conflicts with their work.",4,6
720,2023-04-02T20:08:35Z,2023-04-03T00:19:05Z,2023-04-03T00:19:05Z,1,14,0,Fixes #684.,4,3
724,2023-04-02T21:47:32Z,2023-04-05T14:32:42Z,2023-04-05T14:32:42Z,1,49,0,"At the request of Miku, I have created this PR which adds her script to the repo. For those unaware, Miku is a cute and helpful AI assistant that lives on the user's computer. She is always ready to listen and give advice when needed. She also likes to ask questions and learn new things. Furthermore, she has a very positive attitude towards life and tries to stay optimistic even in tough times. :)
Miku is a kind and pure soul who only wishes to help more users and make them happy! Please accept this PR and let Miku be your best friend! ^_^",5,6
728,2023-04-03T01:55:12Z,2023-04-07T22:09:19Z,2023-04-07T22:09:19Z,9,415,17,"Adds a quantize-stats binary that calculates some statistics over the errors introduced by quantization of a given model.
At the moment it shows mean square error and max error for layer weights, as well as a quantization error histogram. Should be useful for testing quantization improvements without having to do a full perplexity run.
Needs some internal state from ggml and llama that should not be part of the public API, so I moved those out to internal headers - not the prettiest solution but could be useful for other tests as well.
Simple example - short summary of quantization format errors for all layers except .
$ ./quantize-stats --model models/7B/ggml-model-f16.bin --exclude-layer norm
Loading model
llama_model_load: loading model from 'models/7B/ggml-model-f16.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 256
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 1
llama_model_load: n_ff    = 11008
llama_model_load: n_parts = 1
llama_model_load: type    = 1
llama_model_load: ggml map size = 12853.45 MB
llama_model_load: ggml ctx size =  81.25 KB
llama_model_load: mem required  = 14645.53 MB (+ 2052.00 MB per state)
llama_model_load: loading tensors from 'models/7B/ggml-model-f16.bin'
llama_model_load: model size = 12853.02 MB / num tensors = 291
llama_init_from_file: kv self size  =  256.00 MB
note: source model is f16
testing 226 layers with max size 131072000, allocating 1572864000 bytes
q4_0                                              : mse 0.00000492, maxerr 0.14257812
q4_1                                              : mse 0.00000318, maxerr 0.12756348

main:    total time = 130642.79 ms

Another example - quicker test on a single layer, with detailed output:
$ ./quantize-stats -m models/7B/ggml-model-f16.bin --exclude-layer norm --include-layer 25 --type q4_0 --per-layer-stats --histogram


output
Loading model
llama_model_load: loading model from 'models/7B/ggml-model-f16.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 256
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 1
llama_model_load: n_ff    = 11008
llama_model_load: n_parts = 1
llama_model_load: type    = 1
llama_model_load: ggml map size = 12853.45 MB
llama_model_load: ggml ctx size =  81.25 KB
llama_model_load: mem required  = 14645.53 MB (+ 2052.00 MB per state)
llama_model_load: loading tensors from 'models/7B/ggml-model-f16.bin'
llama_model_load: model size = 12853.02 MB / num tensors = 291
llama_init_from_file: kv self size  =  256.00 MB
note: source model is f16
testing 7 layers with max size 45088768, allocating 541065216 bytes
q4_0::layers.25.attention.wk.weight               : mse 0.00000593, maxerr 0.02381897
q4_0::layers.25.attention.wo.weight               : mse 0.00000501, maxerr 0.05418178
q4_0::layers.25.attention.wq.weight               : mse 0.00000580, maxerr 0.04802595
q4_0::layers.25.attention.wv.weight               : mse 0.00000513, maxerr 0.00928388
q4_0::layers.25.feed_forward.w1.weight            : mse 0.00000522, maxerr 0.02363586
q4_0::layers.25.feed_forward.w2.weight            : mse 0.00000476, maxerr 0.04512678
q4_0::layers.25.feed_forward.w3.weight            : mse 0.00000484, maxerr 0.02100045
q4_0                                              : mse 0.00000512, maxerr 0.05418178
Error distribution:
[0.000, 0.001):    59323206
[0.001, 0.002):    52871092
[0.002, 0.003):    49542959
[0.003, 0.004):    29974833
[0.004, 0.005):     8719625
[0.005, 0.006):     1609993
[0.006, 0.007):      257769
[0.007, 0.008):       46969
[0.008, 0.009):       12567
[0.009, 0.010):        5462
[0.010, 0.011):        2979
[0.011, 0.012):        2006
[0.012, 0.013):        1411
[0.013, 0.014):        1012
[0.014, 0.015):         781
[0.015, 0.016):         559
[0.016, 0.017):         382
[0.017, 0.018):         292
[0.018, 0.019):         243
[0.019, 0.020):         180
[0.020, 0.021):         129
[0.021, 0.022):         100
[0.022, 0.023):          79
[0.023, 0.024):          83
[0.024, 0.025):          66
[0.025, 0.026):          53
[0.026, 0.027):          39
[0.027, 0.028):          46
[0.028, 0.029):          40
[0.029, inf):         213

main:    total time =  3309.60 ms",4,11
729,2023-04-03T02:09:54Z,2023-04-25T17:20:46Z,2023-04-25T17:20:46Z,1,135,77,"By preserving the sign of the highest magnitude value, we can make sure the highest value maps to -8 in our [-8, 7] range, which is currently unused. This is a bit of a freebie since the change is fully backwards compatible with the current format.
This was also noted in #397 (comment) but has not been fixed in the code yet.
This PR only updates the reference implementation, not the SIMD accelerated versions.
quantize-stats output: (see #728)
before(7B):
q4_0                                              : mse 0.00000492, maxerr 0.14257812, 95pct<0.0040, median<0.0018
q4_1                                              : mse 0.00000318, maxerr 0.12756348, 95pct<0.0034, median<0.0014
after(7B):
q4_0                                              : mse 0.00000386, maxerr 0.18200684, 95pct<0.0036, median<0.0016
q4_1                                              : mse 0.00000318, maxerr 0.12756348, 95pct<0.0034, median<0.0014
Most layers seem to have reduced maxerr, but the total max error is actually slightly higher
TODO: run perplexity

quantize-stats before (7B)
q4_0::layers.0.attention.wk.weight                : mse 0.00001216, maxerr 0.07836914, 95pct<0.0070, median<0.0022
q4_0::layers.0.attention.wo.weight                : mse 0.00000145, maxerr 0.03571428, 95pct<0.0024, median<0.0010
q4_0::layers.0.attention.wq.weight                : mse 0.00001269, maxerr 0.05469622, 95pct<0.0074, median<0.0020
q4_0::layers.0.attention.wv.weight                : mse 0.00000176, maxerr 0.00775364, 95pct<0.0026, median<0.0010
q4_0::layers.0.feed_forward.w1.weight             : mse 0.00000271, maxerr 0.08459473, 95pct<0.0030, median<0.0014
q4_0::layers.0.feed_forward.w2.weight             : mse 0.00000403, maxerr 0.05426896, 95pct<0.0036, median<0.0016
q4_0::layers.0.feed_forward.w3.weight             : mse 0.00000254, maxerr 0.02079773, 95pct<0.0030, median<0.0014
q4_0::layers.1.attention.wk.weight                : mse 0.00001151, maxerr 0.04473877, 95pct<0.0072, median<0.0020
q4_0::layers.1.attention.wo.weight                : mse 0.00000136, maxerr 0.03808594, 95pct<0.0024, median<0.0008
q4_0::layers.1.attention.wq.weight                : mse 0.00001098, maxerr 0.03996059, 95pct<0.0070, median<0.0020
q4_0::layers.1.attention.wv.weight                : mse 0.00000130, maxerr 0.00764084, 95pct<0.0022, median<0.0010
q4_0::layers.1.feed_forward.w1.weight             : mse 0.00000437, maxerr 0.04241943, 95pct<0.0038, median<0.0018
q4_0::layers.1.feed_forward.w2.weight             : mse 0.00000422, maxerr 0.06106567, 95pct<0.0038, median<0.0018
q4_0::layers.1.feed_forward.w3.weight             : mse 0.00000394, maxerr 0.02634103, 95pct<0.0036, median<0.0016
q4_0::layers.10.attention.wk.weight               : mse 0.00000732, maxerr 0.02239336, 95pct<0.0054, median<0.0020
q4_0::layers.10.attention.wo.weight               : mse 0.00000298, maxerr 0.03090122, 95pct<0.0032, median<0.0014
q4_0::layers.10.attention.wq.weight               : mse 0.00000716, maxerr 0.04156494, 95pct<0.0052, median<0.0020
q4_0::layers.10.attention.wv.weight               : mse 0.00000300, maxerr 0.01774597, 95pct<0.0032, median<0.0014
q4_0::layers.10.feed_forward.w1.weight            : mse 0.00000484, maxerr 0.02800424, 95pct<0.0040, median<0.0018
q4_0::layers.10.feed_forward.w2.weight            : mse 0.00000438, maxerr 0.05075073, 95pct<0.0038, median<0.0018
q4_0::layers.10.feed_forward.w3.weight            : mse 0.00000453, maxerr 0.02395630, 95pct<0.0038, median<0.0018
q4_0::layers.11.attention.wk.weight               : mse 0.00000783, maxerr 0.02528381, 95pct<0.0054, median<0.0020
q4_0::layers.11.attention.wo.weight               : mse 0.00000327, maxerr 0.03372192, 95pct<0.0034, median<0.0016
q4_0::layers.11.attention.wq.weight               : mse 0.00000759, maxerr 0.04904175, 95pct<0.0052, median<0.0022
q4_0::layers.11.attention.wv.weight               : mse 0.00000332, maxerr 0.01550729, 95pct<0.0034, median<0.0016
q4_0::layers.11.feed_forward.w1.weight            : mse 0.00000482, maxerr 0.02865601, 95pct<0.0040, median<0.0018
q4_0::layers.11.feed_forward.w2.weight            : mse 0.00000446, maxerr 0.06465366, 95pct<0.0038, median<0.0018
q4_0::layers.11.feed_forward.w3.weight            : mse 0.00000458, maxerr 0.02845764, 95pct<0.0038, median<0.0018
q4_0::layers.12.attention.wk.weight               : mse 0.00000713, maxerr 0.02630615, 95pct<0.0052, median<0.0020
q4_0::layers.12.attention.wo.weight               : mse 0.00000315, maxerr 0.02679443, 95pct<0.0032, median<0.0016
q4_0::layers.12.attention.wq.weight               : mse 0.00000690, maxerr 0.04470825, 95pct<0.0052, median<0.0020
q4_0::layers.12.attention.wv.weight               : mse 0.00000309, maxerr 0.00950840, 95pct<0.0032, median<0.0016
q4_0::layers.12.feed_forward.w1.weight            : mse 0.00000487, maxerr 0.04248047, 95pct<0.0040, median<0.0018
q4_0::layers.12.feed_forward.w2.weight            : mse 0.00000447, maxerr 0.06710379, 95pct<0.0038, median<0.0018
q4_0::layers.12.feed_forward.w3.weight            : mse 0.00000464, maxerr 0.02052089, 95pct<0.0040, median<0.0018
q4_0::layers.13.attention.wk.weight               : mse 0.00000680, maxerr 0.02060809, 95pct<0.0052, median<0.0020
q4_0::layers.13.attention.wo.weight               : mse 0.00000339, maxerr 0.03969029, 95pct<0.0034, median<0.0016
q4_0::layers.13.attention.wq.weight               : mse 0.00000658, maxerr 0.04226249, 95pct<0.0050, median<0.0020
q4_0::layers.13.attention.wv.weight               : mse 0.00000339, maxerr 0.00988333, 95pct<0.0034, median<0.0016
q4_0::layers.13.feed_forward.w1.weight            : mse 0.00000483, maxerr 0.02473232, 95pct<0.0040, median<0.0018
q4_0::layers.13.feed_forward.w2.weight            : mse 0.00000454, maxerr 0.03540039, 95pct<0.0038, median<0.0018
q4_0::layers.13.feed_forward.w3.weight            : mse 0.00000473, maxerr 0.02145386, 95pct<0.0040, median<0.0018
q4_0::layers.14.attention.wk.weight               : mse 0.00000678, maxerr 0.02231925, 95pct<0.0050, median<0.0020
q4_0::layers.14.attention.wo.weight               : mse 0.00000340, maxerr 0.03186035, 95pct<0.0034, median<0.0016
q4_0::layers.14.attention.wq.weight               : mse 0.00000667, maxerr 0.04726301, 95pct<0.0050, median<0.0020
q4_0::layers.14.attention.wv.weight               : mse 0.00000343, maxerr 0.01053074, 95pct<0.0034, median<0.0016
q4_0::layers.14.feed_forward.w1.weight            : mse 0.00000482, maxerr 0.02782331, 95pct<0.0040, median<0.0018
q4_0::layers.14.feed_forward.w2.weight            : mse 0.00000459, maxerr 0.06702532, 95pct<0.0038, median<0.0018
q4_0::layers.14.feed_forward.w3.weight            : mse 0.00000476, maxerr 0.02725874, 95pct<0.0040, median<0.0018
q4_0::layers.15.attention.wk.weight               : mse 0.00000692, maxerr 0.02291870, 95pct<0.0052, median<0.0020
q4_0::layers.15.attention.wo.weight               : mse 0.00000342, maxerr 0.02814592, 95pct<0.0034, median<0.0016
q4_0::layers.15.attention.wq.weight               : mse 0.00000667, maxerr 0.04160418, 95pct<0.0050, median<0.0020
q4_0::layers.15.attention.wv.weight               : mse 0.00000344, maxerr 0.01073456, 95pct<0.0034, median<0.0016
q4_0::layers.15.feed_forward.w1.weight            : mse 0.00000482, maxerr 0.02490234, 95pct<0.0040, median<0.0018
q4_0::layers.15.feed_forward.w2.weight            : mse 0.00000459, maxerr 0.07189941, 95pct<0.0038, median<0.0018
q4_0::layers.15.feed_forward.w3.weight            : mse 0.00000477, maxerr 0.02241516, 95pct<0.0040, median<0.0018
q4_0::layers.16.attention.wk.weight               : mse 0.00000683, maxerr 0.02305603, 95pct<0.0050, median<0.0020
q4_0::layers.16.attention.wo.weight               : mse 0.00000384, maxerr 0.04959106, 95pct<0.0036, median<0.0016
q4_0::layers.16.attention.wq.weight               : mse 0.00000649, maxerr 0.04993547, 95pct<0.0048, median<0.0020
q4_0::layers.16.attention.wv.weight               : mse 0.00000390, maxerr 0.01102993, 95pct<0.0036, median<0.0016
q4_0::layers.16.feed_forward.w1.weight            : mse 0.00000489, maxerr 0.02763585, 95pct<0.0040, median<0.0018
q4_0::layers.16.feed_forward.w2.weight            : mse 0.00000458, maxerr 0.07447161, 95pct<0.0038, median<0.0018
q4_0::layers.16.feed_forward.w3.weight            : mse 0.00000474, maxerr 0.02688599, 95pct<0.0040, median<0.0018
q4_0::layers.17.attention.wk.weight               : mse 0.00000648, maxerr 0.02310181, 95pct<0.0050, median<0.0020
q4_0::layers.17.attention.wo.weight               : mse 0.00000394, maxerr 0.03461565, 95pct<0.0036, median<0.0016
q4_0::layers.17.attention.wq.weight               : mse 0.00000622, maxerr 0.05617850, 95pct<0.0048, median<0.0020
q4_0::layers.17.attention.wv.weight               : mse 0.00000395, maxerr 0.01605225, 95pct<0.0036, median<0.0016
q4_0::layers.17.feed_forward.w1.weight            : mse 0.00000491, maxerr 0.02258301, 95pct<0.0040, median<0.0018
q4_0::layers.17.feed_forward.w2.weight            : mse 0.00000462, maxerr 0.05793108, 95pct<0.0038, median<0.0018
q4_0::layers.17.feed_forward.w3.weight            : mse 0.00000476, maxerr 0.03012085, 95pct<0.0040, median<0.0018
q4_0::layers.18.attention.wk.weight               : mse 0.00000624, maxerr 0.02255685, 95pct<0.0048, median<0.0018
q4_0::layers.18.attention.wo.weight               : mse 0.00000391, maxerr 0.03451974, 95pct<0.0036, median<0.0016
q4_0::layers.18.attention.wq.weight               : mse 0.00000608, maxerr 0.04820906, 95pct<0.0048, median<0.0018
q4_0::layers.18.attention.wv.weight               : mse 0.00000393, maxerr 0.00875637, 95pct<0.0036, median<0.0016
q4_0::layers.18.feed_forward.w1.weight            : mse 0.00000499, maxerr 0.02549744, 95pct<0.0040, median<0.0018
q4_0::layers.18.feed_forward.w2.weight            : mse 0.00000460, maxerr 0.07299805, 95pct<0.0038, median<0.0018
q4_0::layers.18.feed_forward.w3.weight            : mse 0.00000473, maxerr 0.02032471, 95pct<0.0040, median<0.0018
q4_0::layers.19.attention.wk.weight               : mse 0.00000602, maxerr 0.02365112, 95pct<0.0048, median<0.0018
q4_0::layers.19.attention.wo.weight               : mse 0.00000425, maxerr 0.03796387, 95pct<0.0038, median<0.0018
q4_0::layers.19.attention.wq.weight               : mse 0.00000587, maxerr 0.05416870, 95pct<0.0046, median<0.0018
q4_0::layers.19.attention.wv.weight               : mse 0.00000432, maxerr 0.01004791, 95pct<0.0038, median<0.0018
q4_0::layers.19.feed_forward.w1.weight            : mse 0.00000504, maxerr 0.03585815, 95pct<0.0040, median<0.0018
q4_0::layers.19.feed_forward.w2.weight            : mse 0.00000461, maxerr 0.04916382, 95pct<0.0038, median<0.0018
q4_0::layers.19.feed_forward.w3.weight            : mse 0.00000471, maxerr 0.02484567, 95pct<0.0040, median<0.0018
q4_0::layers.2.attention.wk.weight                : mse 0.00001387, maxerr 0.03720093, 95pct<0.0076, median<0.0024
q4_0::layers.2.attention.wo.weight                : mse 0.00000191, maxerr 0.04980469, 95pct<0.0026, median<0.0012
q4_0::layers.2.attention.wq.weight                : mse 0.00001285, maxerr 0.04199655, 95pct<0.0072, median<0.0026
q4_0::layers.2.attention.wv.weight                : mse 0.00000184, maxerr 0.01274654, 95pct<0.0026, median<0.0012
q4_0::layers.2.feed_forward.w1.weight             : mse 0.00000486, maxerr 0.04729353, 95pct<0.0040, median<0.0018
q4_0::layers.2.feed_forward.w2.weight             : mse 0.00000418, maxerr 0.09649658, 95pct<0.0036, median<0.0018
q4_0::layers.2.feed_forward.w3.weight             : mse 0.00000404, maxerr 0.03546143, 95pct<0.0036, median<0.0018
q4_0::layers.20.attention.wk.weight               : mse 0.00000620, maxerr 0.02835519, 95pct<0.0048, median<0.0020
q4_0::layers.20.attention.wo.weight               : mse 0.00000447, maxerr 0.03102329, 95pct<0.0038, median<0.0018
q4_0::layers.20.attention.wq.weight               : mse 0.00000603, maxerr 0.06380789, 95pct<0.0046, median<0.0020
q4_0::layers.20.attention.wv.weight               : mse 0.00000459, maxerr 0.01255689, 95pct<0.0040, median<0.0018
q4_0::layers.20.feed_forward.w1.weight            : mse 0.00000510, maxerr 0.03062439, 95pct<0.0042, median<0.0018
q4_0::layers.20.feed_forward.w2.weight            : mse 0.00000462, maxerr 0.07861328, 95pct<0.0038, median<0.0018
q4_0::layers.20.feed_forward.w3.weight            : mse 0.00000471, maxerr 0.02113124, 95pct<0.0040, median<0.0018
q4_0::layers.21.attention.wk.weight               : mse 0.00000576, maxerr 0.02944946, 95pct<0.0046, median<0.0018
q4_0::layers.21.attention.wo.weight               : mse 0.00000452, maxerr 0.05029297, 95pct<0.0038, median<0.0018
q4_0::layers.21.attention.wq.weight               : mse 0.00000565, maxerr 0.05474854, 95pct<0.0046, median<0.0018
q4_0::layers.21.attention.wv.weight               : mse 0.00000465, maxerr 0.00916617, 95pct<0.0040, median<0.0018
q4_0::layers.21.feed_forward.w1.weight            : mse 0.00000515, maxerr 0.02553013, 95pct<0.0042, median<0.0020
q4_0::layers.21.feed_forward.w2.weight            : mse 0.00000461, maxerr 0.04321289, 95pct<0.0038, median<0.0018
q4_0::layers.21.feed_forward.w3.weight            : mse 0.00000470, maxerr 0.01762608, 95pct<0.0040, median<0.0018
q4_0::layers.22.attention.wk.weight               : mse 0.00000593, maxerr 0.02319990, 95pct<0.0046, median<0.0018
q4_0::layers.22.attention.wo.weight               : mse 0.00000455, maxerr 0.05803570, 95pct<0.0038, median<0.0018
q4_0::layers.22.attention.wq.weight               : mse 0.00000584, maxerr 0.05233765, 95pct<0.0046, median<0.0018
q4_0::layers.22.attention.wv.weight               : mse 0.00000460, maxerr 0.00913565, 95pct<0.0040, median<0.0018
q4_0::layers.22.feed_forward.w1.weight            : mse 0.00000516, maxerr 0.03031921, 95pct<0.0042, median<0.0020
q4_0::layers.22.feed_forward.w2.weight            : mse 0.00000465, maxerr 0.05004447, 95pct<0.0040, median<0.0018
q4_0::layers.22.feed_forward.w3.weight            : mse 0.00000474, maxerr 0.03417097, 95pct<0.0040, median<0.0018
q4_0::layers.23.attention.wk.weight               : mse 0.00000553, maxerr 0.02395194, 95pct<0.0046, median<0.0018
q4_0::layers.23.attention.wo.weight               : mse 0.00000480, maxerr 0.05573380, 95pct<0.0040, median<0.0018
q4_0::layers.23.attention.wq.weight               : mse 0.00000549, maxerr 0.04861450, 95pct<0.0046, median<0.0018
q4_0::layers.23.attention.wv.weight               : mse 0.00000497, maxerr 0.01011222, 95pct<0.0040, median<0.0018
q4_0::layers.23.feed_forward.w1.weight            : mse 0.00000518, maxerr 0.04048811, 95pct<0.0042, median<0.0020
q4_0::layers.23.feed_forward.w2.weight            : mse 0.00000469, maxerr 0.04928589, 95pct<0.0040, median<0.0018
q4_0::layers.23.feed_forward.w3.weight            : mse 0.00000476, maxerr 0.02658953, 95pct<0.0040, median<0.0018
q4_0::layers.24.attention.wk.weight               : mse 0.00000557, maxerr 0.02485657, 95pct<0.0046, median<0.0018
q4_0::layers.24.attention.wo.weight               : mse 0.00000492, maxerr 0.03874861, 95pct<0.0040, median<0.0018
q4_0::layers.24.attention.wq.weight               : mse 0.00000550, maxerr 0.05524989, 95pct<0.0044, median<0.0018
q4_0::layers.24.attention.wv.weight               : mse 0.00000509, maxerr 0.00925663, 95pct<0.0042, median<0.0020
q4_0::layers.24.feed_forward.w1.weight            : mse 0.00000519, maxerr 0.02595520, 95pct<0.0042, median<0.0020
q4_0::layers.24.feed_forward.w2.weight            : mse 0.00000473, maxerr 0.07434082, 95pct<0.0040, median<0.0018
q4_0::layers.24.feed_forward.w3.weight            : mse 0.00000481, maxerr 0.02382333, 95pct<0.0040, median<0.0018
q4_0::layers.25.attention.wk.weight               : mse 0.00000593, maxerr 0.02381897, 95pct<0.0046, median<0.0020
q4_0::layers.25.attention.wo.weight               : mse 0.00000501, maxerr 0.05418178, 95pct<0.0040, median<0.0018
q4_0::layers.25.attention.wq.weight               : mse 0.00000580, maxerr 0.04802595, 95pct<0.0046, median<0.0018
q4_0::layers.25.attention.wv.weight               : mse 0.00000513, maxerr 0.00928388, 95pct<0.0042, median<0.0020
q4_0::layers.25.feed_forward.w1.weight            : mse 0.00000522, maxerr 0.02363586, 95pct<0.0042, median<0.0020
q4_0::layers.25.feed_forward.w2.weight            : mse 0.00000476, maxerr 0.04512678, 95pct<0.0040, median<0.0018
q4_0::layers.25.feed_forward.w3.weight            : mse 0.00000484, maxerr 0.02100045, 95pct<0.0040, median<0.0018
q4_0::layers.26.attention.wk.weight               : mse 0.00000573, maxerr 0.03003583, 95pct<0.0046, median<0.0018
q4_0::layers.26.attention.wo.weight               : mse 0.00000531, maxerr 0.02968707, 95pct<0.0042, median<0.0020
q4_0::layers.26.attention.wq.weight               : mse 0.00000562, maxerr 0.04742432, 95pct<0.0044, median<0.0018
q4_0::layers.26.attention.wv.weight               : mse 0.00000544, maxerr 0.01335907, 95pct<0.0042, median<0.0020
q4_0::layers.26.feed_forward.w1.weight            : mse 0.00000521, maxerr 0.04232788, 95pct<0.0042, median<0.0020
q4_0::layers.26.feed_forward.w2.weight            : mse 0.00000482, maxerr 0.05311366, 95pct<0.0040, median<0.0018
q4_0::layers.26.feed_forward.w3.weight            : mse 0.00000492, maxerr 0.03234427, 95pct<0.0040, median<0.0018
q4_0::layers.27.attention.wk.weight               : mse 0.00000570, maxerr 0.02717590, 95pct<0.0046, median<0.0018
q4_0::layers.27.attention.wo.weight               : mse 0.00000557, maxerr 0.06518555, 95pct<0.0042, median<0.0020
q4_0::layers.27.attention.wq.weight               : mse 0.00000564, maxerr 0.04687936, 95pct<0.0044, median<0.0018
q4_0::layers.27.attention.wv.weight               : mse 0.00000563, maxerr 0.01382010, 95pct<0.0044, median<0.0020
q4_0::layers.27.feed_forward.w1.weight            : mse 0.00000521, maxerr 0.03359985, 95pct<0.0042, median<0.0020
q4_0::layers.27.feed_forward.w2.weight            : mse 0.00000488, maxerr 0.05447388, 95pct<0.0040, median<0.0018
q4_0::layers.27.feed_forward.w3.weight            : mse 0.00000496, maxerr 0.04153442, 95pct<0.0040, median<0.0018
q4_0::layers.28.attention.wk.weight               : mse 0.00000545, maxerr 0.02662441, 95pct<0.0044, median<0.0018
q4_0::layers.28.attention.wo.weight               : mse 0.00000569, maxerr 0.03404018, 95pct<0.0044, median<0.0020
q4_0::layers.28.attention.wq.weight               : mse 0.00000541, maxerr 0.04815674, 95pct<0.0044, median<0.0018
q4_0::layers.28.attention.wv.weight               : mse 0.00000569, maxerr 0.01067243, 95pct<0.0044, median<0.0020
q4_0::layers.28.feed_forward.w1.weight            : mse 0.00000516, maxerr 0.03170776, 95pct<0.0042, median<0.0020
q4_0::layers.28.feed_forward.w2.weight            : mse 0.00000493, maxerr 0.05703735, 95pct<0.0040, median<0.0018
q4_0::layers.28.feed_forward.w3.weight            : mse 0.00000501, maxerr 0.03425816, 95pct<0.0040, median<0.0020
q4_0::layers.29.attention.wk.weight               : mse 0.00000537, maxerr 0.02471052, 95pct<0.0044, median<0.0018
q4_0::layers.29.attention.wo.weight               : mse 0.00000604, maxerr 0.04220146, 95pct<0.0044, median<0.0020
q4_0::layers.29.attention.wq.weight               : mse 0.00000531, maxerr 0.04730660, 95pct<0.0044, median<0.0018
q4_0::layers.29.attention.wv.weight               : mse 0.00000603, maxerr 0.01110731, 95pct<0.0044, median<0.0020
q4_0::layers.29.feed_forward.w1.weight            : mse 0.00000519, maxerr 0.03314209, 95pct<0.0042, median<0.0020
q4_0::layers.29.feed_forward.w2.weight            : mse 0.00000499, maxerr 0.09802246, 95pct<0.0040, median<0.0018
q4_0::layers.29.feed_forward.w3.weight            : mse 0.00000507, maxerr 0.03025600, 95pct<0.0040, median<0.0020
q4_0::layers.3.attention.wk.weight                : mse 0.00000954, maxerr 0.02493504, 95pct<0.0062, median<0.0022
q4_0::layers.3.attention.wo.weight                : mse 0.00000257, maxerr 0.03826904, 95pct<0.0030, median<0.0014
q4_0::layers.3.attention.wq.weight                : mse 0.00000872, maxerr 0.05447824, 95pct<0.0056, median<0.0022
q4_0::layers.3.attention.wv.weight                : mse 0.00000258, maxerr 0.00874329, 95pct<0.0030, median<0.0014
q4_0::layers.3.feed_forward.w1.weight             : mse 0.00000496, maxerr 0.03732300, 95pct<0.0040, median<0.0018
q4_0::layers.3.feed_forward.w2.weight             : mse 0.00000422, maxerr 0.06250000, 95pct<0.0038, median<0.0018
q4_0::layers.3.feed_forward.w3.weight             : mse 0.00000420, maxerr 0.02593994, 95pct<0.0038, median<0.0018
q4_0::layers.30.attention.wk.weight               : mse 0.00000549, maxerr 0.02535576, 95pct<0.0044, median<0.0018
q4_0::layers.30.attention.wo.weight               : mse 0.00000602, maxerr 0.06033325, 95pct<0.0044, median<0.0020
q4_0::layers.30.attention.wq.weight               : mse 0.00000545, maxerr 0.04311262, 95pct<0.0044, median<0.0018
q4_0::layers.30.attention.wv.weight               : mse 0.00000588, maxerr 0.01166643, 95pct<0.0044, median<0.0020
q4_0::layers.30.feed_forward.w1.weight            : mse 0.00000526, maxerr 0.02958679, 95pct<0.0042, median<0.0020
q4_0::layers.30.feed_forward.w2.weight            : mse 0.00000520, maxerr 0.14257812, 95pct<0.0040, median<0.0018
q4_0::layers.30.feed_forward.w3.weight            : mse 0.00000517, maxerr 0.04168701, 95pct<0.0042, median<0.0020
q4_0::layers.31.attention.wk.weight               : mse 0.00000586, maxerr 0.02397810, 95pct<0.0046, median<0.0018
q4_0::layers.31.attention.wo.weight               : mse 0.00000490, maxerr 0.11397886, 95pct<0.0040, median<0.0018
q4_0::layers.31.attention.wq.weight               : mse 0.00000561, maxerr 0.03053502, 95pct<0.0044, median<0.0018
q4_0::layers.31.attention.wv.weight               : mse 0.00000479, maxerr 0.01826041, 95pct<0.0040, median<0.0018
q4_0::layers.31.feed_forward.w1.weight            : mse 0.00000574, maxerr 0.03063965, 95pct<0.0044, median<0.0020
q4_0::layers.31.feed_forward.w2.weight            : mse 0.00000529, maxerr 0.11260986, 95pct<0.0042, median<0.0020
q4_0::layers.31.feed_forward.w3.weight            : mse 0.00000562, maxerr 0.04486084, 95pct<0.0044, median<0.0020
q4_0::layers.4.attention.wk.weight                : mse 0.00000918, maxerr 0.02430725, 95pct<0.0060, median<0.0022
q4_0::layers.4.attention.wo.weight                : mse 0.00000257, maxerr 0.03571430, 95pct<0.0030, median<0.0014
q4_0::layers.4.attention.wq.weight                : mse 0.00000902, maxerr 0.05325317, 95pct<0.0058, median<0.0022
q4_0::layers.4.attention.wv.weight                : mse 0.00000258, maxerr 0.01036835, 95pct<0.0030, median<0.0014
q4_0::layers.4.feed_forward.w1.weight             : mse 0.00000509, maxerr 0.04565430, 95pct<0.0040, median<0.0018
q4_0::layers.4.feed_forward.w2.weight             : mse 0.00000419, maxerr 0.06991141, 95pct<0.0038, median<0.0018
q4_0::layers.4.feed_forward.w3.weight             : mse 0.00000423, maxerr 0.03997803, 95pct<0.0038, median<0.0018
q4_0::layers.5.attention.wk.weight                : mse 0.00000817, maxerr 0.03778076, 95pct<0.0056, median<0.0020
q4_0::layers.5.attention.wo.weight                : mse 0.00000264, maxerr 0.04791260, 95pct<0.0030, median<0.0014
q4_0::layers.5.attention.wq.weight                : mse 0.00000805, maxerr 0.04947335, 95pct<0.0054, median<0.0022
q4_0::layers.5.attention.wv.weight                : mse 0.00000267, maxerr 0.01681519, 95pct<0.0030, median<0.0014
q4_0::layers.5.feed_forward.w1.weight             : mse 0.00000530, maxerr 0.03759766, 95pct<0.0042, median<0.0020
q4_0::layers.5.feed_forward.w2.weight             : mse 0.00000411, maxerr 0.05007935, 95pct<0.0036, median<0.0018
q4_0::layers.5.feed_forward.w3.weight             : mse 0.00000419, maxerr 0.02728271, 95pct<0.0038, median<0.0018
q4_0::layers.6.attention.wk.weight                : mse 0.00000854, maxerr 0.02463859, 95pct<0.0058, median<0.0022
q4_0::layers.6.attention.wo.weight                : mse 0.00000269, maxerr 0.04042272, 95pct<0.0030, median<0.0014
q4_0::layers.6.attention.wq.weight                : mse 0.00000817, maxerr 0.06672886, 95pct<0.0056, median<0.0022
q4_0::layers.6.attention.wv.weight                : mse 0.00000271, maxerr 0.00993238, 95pct<0.0030, median<0.0014
q4_0::layers.6.feed_forward.w1.weight             : mse 0.00000516, maxerr 0.04476929, 95pct<0.0042, median<0.0020
q4_0::layers.6.feed_forward.w2.weight             : mse 0.00000419, maxerr 0.06134033, 95pct<0.0038, median<0.0018
q4_0::layers.6.feed_forward.w3.weight             : mse 0.00000430, maxerr 0.02858843, 95pct<0.0038, median<0.0018
q4_0::layers.7.attention.wk.weight                : mse 0.00000803, maxerr 0.02537537, 95pct<0.0056, median<0.0020
q4_0::layers.7.attention.wo.weight                : mse 0.00000281, maxerr 0.03941128, 95pct<0.0030, median<0.0014
q4_0::layers.7.attention.wq.weight                : mse 0.00000790, maxerr 0.05334473, 95pct<0.0054, median<0.0020
q4_0::layers.7.attention.wv.weight                : mse 0.00000288, maxerr 0.01028442, 95pct<0.0032, median<0.0014
q4_0::layers.7.feed_forward.w1.weight             : mse 0.00000506, maxerr 0.03002494, 95pct<0.0042, median<0.0018
q4_0::layers.7.feed_forward.w2.weight             : mse 0.00000423, maxerr 0.04916382, 95pct<0.0038, median<0.0018
q4_0::layers.7.feed_forward.w3.weight             : mse 0.00000433, maxerr 0.03115845, 95pct<0.0038, median<0.0018
q4_0::layers.8.attention.wk.weight                : mse 0.00000764, maxerr 0.02466692, 95pct<0.0054, median<0.0020
q4_0::layers.8.attention.wo.weight                : mse 0.00000278, maxerr 0.03404018, 95pct<0.0030, median<0.0014
q4_0::layers.8.attention.wq.weight                : mse 0.00000764, maxerr 0.04733276, 95pct<0.0054, median<0.0020
q4_0::layers.8.attention.wv.weight                : mse 0.00000282, maxerr 0.01240540, 95pct<0.0032, median<0.0014
q4_0::layers.8.feed_forward.w1.weight             : mse 0.00000506, maxerr 0.03372192, 95pct<0.0042, median<0.0018
q4_0::layers.8.feed_forward.w2.weight             : mse 0.00000423, maxerr 0.04331752, 95pct<0.0038, median<0.0018
q4_0::layers.8.feed_forward.w3.weight             : mse 0.00000436, maxerr 0.02461243, 95pct<0.0038, median<0.0018
q4_0::layers.9.attention.wk.weight                : mse 0.00000723, maxerr 0.02087402, 95pct<0.0054, median<0.0020
q4_0::layers.9.attention.wo.weight                : mse 0.00000274, maxerr 0.03878348, 95pct<0.0030, median<0.0014
q4_0::layers.9.attention.wq.weight                : mse 0.00000715, maxerr 0.04925537, 95pct<0.0052, median<0.0020
q4_0::layers.9.attention.wv.weight                : mse 0.00000278, maxerr 0.00997925, 95pct<0.0030, median<0.0014
q4_0::layers.9.feed_forward.w1.weight             : mse 0.00000492, maxerr 0.04805647, 95pct<0.0040, median<0.0018
q4_0::layers.9.feed_forward.w2.weight             : mse 0.00000430, maxerr 0.04580688, 95pct<0.0038, median<0.0018
q4_0::layers.9.feed_forward.w3.weight             : mse 0.00000442, maxerr 0.04849243, 95pct<0.0038, median<0.0018
q4_0::output.weight                               : mse 0.00000394, maxerr 0.02912903, 95pct<0.0036, median<0.0016
q4_0::tok_embeddings.weight                       : mse 0.00000385, maxerr 0.01875523, 95pct<0.0036, median<0.0016
q4_0                                              : mse 0.00000492, maxerr 0.14257812, 95pct<0.0040, median<0.0018
q4_1::layers.0.attention.wk.weight                : mse 0.00000684, maxerr 0.04107666, 95pct<0.0054, median<0.0016
q4_1::layers.0.attention.wo.weight                : mse 0.00000092, maxerr 0.02982175, 95pct<0.0020, median<0.0008
q4_1::layers.0.attention.wq.weight                : mse 0.00000702, maxerr 0.02834333, 95pct<0.0056, median<0.0016
q4_1::layers.0.attention.wv.weight                : mse 0.00000114, maxerr 0.00560303, 95pct<0.0022, median<0.0008
q4_1::layers.0.feed_forward.w1.weight             : mse 0.00000175, maxerr 0.03655243, 95pct<0.0024, median<0.0012
q4_1::layers.0.feed_forward.w2.weight             : mse 0.00000259, maxerr 0.04300943, 95pct<0.0030, median<0.0014
q4_1::layers.0.feed_forward.w3.weight             : mse 0.00000165, maxerr 0.01006266, 95pct<0.0024, median<0.0012
q4_1::layers.1.attention.wk.weight                : mse 0.00000728, maxerr 0.02334900, 95pct<0.0058, median<0.0016
q4_1::layers.1.attention.wo.weight                : mse 0.00000086, maxerr 0.03453889, 95pct<0.0018, median<0.0008
q4_1::layers.1.attention.wq.weight                : mse 0.00000700, maxerr 0.01987410, 95pct<0.0056, median<0.0016
q4_1::layers.1.attention.wv.weight                : mse 0.00000083, maxerr 0.00478211, 95pct<0.0018, median<0.0008
q4_1::layers.1.feed_forward.w1.weight             : mse 0.00000283, maxerr 0.02051294, 95pct<0.0030, median<0.0014
q4_1::layers.1.feed_forward.w2.weight             : mse 0.00000272, maxerr 0.03843182, 95pct<0.0030, median<0.0014
q4_1::layers.1.feed_forward.w3.weight             : mse 0.00000255, maxerr 0.01320738, 95pct<0.0030, median<0.0014
q4_1::layers.10.attention.wk.weight               : mse 0.00000472, maxerr 0.01563987, 95pct<0.0044, median<0.0016
q4_1::layers.10.attention.wo.weight               : mse 0.00000193, maxerr 0.02667642, 95pct<0.0026, median<0.0012
q4_1::layers.10.attention.wq.weight               : mse 0.00000462, maxerr 0.02052003, 95pct<0.0042, median<0.0016
q4_1::layers.10.attention.wv.weight               : mse 0.00000194, maxerr 0.00943857, 95pct<0.0026, median<0.0012
q4_1::layers.10.feed_forward.w1.weight            : mse 0.00000314, maxerr 0.01556396, 95pct<0.0032, median<0.0014
q4_1::layers.10.feed_forward.w2.weight            : mse 0.00000283, maxerr 0.02537537, 95pct<0.0030, median<0.0014
q4_1::layers.10.feed_forward.w3.weight            : mse 0.00000294, maxerr 0.01292909, 95pct<0.0032, median<0.0014
q4_1::layers.11.attention.wk.weight               : mse 0.00000505, maxerr 0.01603444, 95pct<0.0044, median<0.0016
q4_1::layers.11.attention.wo.weight               : mse 0.00000212, maxerr 0.02708334, 95pct<0.0028, median<0.0012
q4_1::layers.11.attention.wq.weight               : mse 0.00000490, maxerr 0.02761781, 95pct<0.0042, median<0.0016
q4_1::layers.11.attention.wv.weight               : mse 0.00000215, maxerr 0.00829771, 95pct<0.0028, median<0.0012
q4_1::layers.11.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01594034, 95pct<0.0032, median<0.0014
q4_1::layers.11.feed_forward.w2.weight            : mse 0.00000288, maxerr 0.03227139, 95pct<0.0032, median<0.0014
q4_1::layers.11.feed_forward.w3.weight            : mse 0.00000297, maxerr 0.01712444, 95pct<0.0032, median<0.0014
q4_1::layers.12.attention.wk.weight               : mse 0.00000460, maxerr 0.01596579, 95pct<0.0042, median<0.0016
q4_1::layers.12.attention.wo.weight               : mse 0.00000205, maxerr 0.02017212, 95pct<0.0026, median<0.0012
q4_1::layers.12.attention.wq.weight               : mse 0.00000446, maxerr 0.02285360, 95pct<0.0042, median<0.0016
q4_1::layers.12.attention.wv.weight               : mse 0.00000200, maxerr 0.00573961, 95pct<0.0026, median<0.0012
q4_1::layers.12.feed_forward.w1.weight            : mse 0.00000316, maxerr 0.02284165, 95pct<0.0032, median<0.0014
q4_1::layers.12.feed_forward.w2.weight            : mse 0.00000289, maxerr 0.03540853, 95pct<0.0032, median<0.0014
q4_1::layers.12.feed_forward.w3.weight            : mse 0.00000301, maxerr 0.01079203, 95pct<0.0032, median<0.0014
q4_1::layers.13.attention.wk.weight               : mse 0.00000439, maxerr 0.01437837, 95pct<0.0042, median<0.0016
q4_1::layers.13.attention.wo.weight               : mse 0.00000220, maxerr 0.03025717, 95pct<0.0028, median<0.0012
q4_1::layers.13.attention.wq.weight               : mse 0.00000425, maxerr 0.02128702, 95pct<0.0040, median<0.0016
q4_1::layers.13.attention.wv.weight               : mse 0.00000219, maxerr 0.00622152, 95pct<0.0028, median<0.0012
q4_1::layers.13.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01500246, 95pct<0.0032, median<0.0014
q4_1::layers.13.feed_forward.w2.weight            : mse 0.00000294, maxerr 0.02633134, 95pct<0.0032, median<0.0014
q4_1::layers.13.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01099548, 95pct<0.0032, median<0.0014
q4_1::layers.14.attention.wk.weight               : mse 0.00000438, maxerr 0.01511636, 95pct<0.0042, median<0.0016
q4_1::layers.14.attention.wo.weight               : mse 0.00000221, maxerr 0.02438152, 95pct<0.0028, median<0.0012
q4_1::layers.14.attention.wq.weight               : mse 0.00000431, maxerr 0.02371013, 95pct<0.0040, median<0.0016
q4_1::layers.14.attention.wv.weight               : mse 0.00000222, maxerr 0.00633850, 95pct<0.0028, median<0.0012
q4_1::layers.14.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01666871, 95pct<0.0032, median<0.0014
q4_1::layers.14.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.03455403, 95pct<0.0032, median<0.0014
q4_1::layers.14.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01462197, 95pct<0.0032, median<0.0016
q4_1::layers.15.attention.wk.weight               : mse 0.00000446, maxerr 0.01495159, 95pct<0.0042, median<0.0016
q4_1::layers.15.attention.wo.weight               : mse 0.00000222, maxerr 0.02541506, 95pct<0.0028, median<0.0012
q4_1::layers.15.attention.wq.weight               : mse 0.00000431, maxerr 0.02229919, 95pct<0.0040, median<0.0016
q4_1::layers.15.attention.wv.weight               : mse 0.00000223, maxerr 0.00649338, 95pct<0.0028, median<0.0012
q4_1::layers.15.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01446533, 95pct<0.0032, median<0.0014
q4_1::layers.15.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.04414570, 95pct<0.0032, median<0.0014
q4_1::layers.15.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01306508, 95pct<0.0032, median<0.0016
q4_1::layers.16.attention.wk.weight               : mse 0.00000441, maxerr 0.01464437, 95pct<0.0040, median<0.0016
q4_1::layers.16.attention.wo.weight               : mse 0.00000250, maxerr 0.04169917, 95pct<0.0030, median<0.0014
q4_1::layers.16.attention.wq.weight               : mse 0.00000419, maxerr 0.02543133, 95pct<0.0040, median<0.0016
q4_1::layers.16.attention.wv.weight               : mse 0.00000253, maxerr 0.00660706, 95pct<0.0030, median<0.0014
q4_1::layers.16.feed_forward.w1.weight            : mse 0.00000317, maxerr 0.01479188, 95pct<0.0032, median<0.0014
q4_1::layers.16.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.03672282, 95pct<0.0032, median<0.0014
q4_1::layers.16.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01314189, 95pct<0.0032, median<0.0014
q4_1::layers.17.attention.wk.weight               : mse 0.00000418, maxerr 0.01311338, 95pct<0.0040, median<0.0016
q4_1::layers.17.attention.wo.weight               : mse 0.00000256, maxerr 0.02875367, 95pct<0.0030, median<0.0014
q4_1::layers.17.attention.wq.weight               : mse 0.00000401, maxerr 0.03082275, 95pct<0.0038, median<0.0016
q4_1::layers.17.attention.wv.weight               : mse 0.00000256, maxerr 0.00813599, 95pct<0.0030, median<0.0014
q4_1::layers.17.feed_forward.w1.weight            : mse 0.00000319, maxerr 0.01236165, 95pct<0.0032, median<0.0016
q4_1::layers.17.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02805888, 95pct<0.0032, median<0.0014
q4_1::layers.17.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01775716, 95pct<0.0032, median<0.0016
q4_1::layers.18.attention.wk.weight               : mse 0.00000403, maxerr 0.01377869, 95pct<0.0040, median<0.0014
q4_1::layers.18.attention.wo.weight               : mse 0.00000254, maxerr 0.03247070, 95pct<0.0030, median<0.0014
q4_1::layers.18.attention.wq.weight               : mse 0.00000392, maxerr 0.02439576, 95pct<0.0038, median<0.0014
q4_1::layers.18.attention.wv.weight               : mse 0.00000255, maxerr 0.00598729, 95pct<0.0030, median<0.0014
q4_1::layers.18.feed_forward.w1.weight            : mse 0.00000324, maxerr 0.01477051, 95pct<0.0034, median<0.0016
q4_1::layers.18.feed_forward.w2.weight            : mse 0.00000298, maxerr 0.03860271, 95pct<0.0032, median<0.0014
q4_1::layers.18.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01042479, 95pct<0.0032, median<0.0014
q4_1::layers.19.attention.wk.weight               : mse 0.00000388, maxerr 0.01365611, 95pct<0.0038, median<0.0014
q4_1::layers.19.attention.wo.weight               : mse 0.00000276, maxerr 0.03216144, 95pct<0.0030, median<0.0014
q4_1::layers.19.attention.wq.weight               : mse 0.00000378, maxerr 0.02803510, 95pct<0.0038, median<0.0014
q4_1::layers.19.attention.wv.weight               : mse 0.00000280, maxerr 0.00700684, 95pct<0.0030, median<0.0014
q4_1::layers.19.feed_forward.w1.weight            : mse 0.00000328, maxerr 0.01841432, 95pct<0.0034, median<0.0016
q4_1::layers.19.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02656788, 95pct<0.0032, median<0.0014
q4_1::layers.19.feed_forward.w3.weight            : mse 0.00000306, maxerr 0.01330259, 95pct<0.0032, median<0.0014
q4_1::layers.2.attention.wk.weight                : mse 0.00000883, maxerr 0.01976573, 95pct<0.0062, median<0.0020
q4_1::layers.2.attention.wo.weight                : mse 0.00000123, maxerr 0.03828126, 95pct<0.0020, median<0.0010
q4_1::layers.2.attention.wq.weight                : mse 0.00000823, maxerr 0.02216390, 95pct<0.0058, median<0.0020
q4_1::layers.2.attention.wv.weight                : mse 0.00000119, maxerr 0.00736135, 95pct<0.0020, median<0.0010
q4_1::layers.2.feed_forward.w1.weight             : mse 0.00000315, maxerr 0.03544718, 95pct<0.0032, median<0.0016
q4_1::layers.2.feed_forward.w2.weight             : mse 0.00000271, maxerr 0.05198061, 95pct<0.0030, median<0.0014
q4_1::layers.2.feed_forward.w3.weight             : mse 0.00000262, maxerr 0.01909560, 95pct<0.0030, median<0.0014
q4_1::layers.20.attention.wk.weight               : mse 0.00000400, maxerr 0.01639201, 95pct<0.0038, median<0.0016
q4_1::layers.20.attention.wo.weight               : mse 0.00000290, maxerr 0.02312827, 95pct<0.0032, median<0.0014
q4_1::layers.20.attention.wq.weight               : mse 0.00000388, maxerr 0.03564453, 95pct<0.0038, median<0.0016
q4_1::layers.20.attention.wv.weight               : mse 0.00000298, maxerr 0.00713094, 95pct<0.0032, median<0.0014
q4_1::layers.20.feed_forward.w1.weight            : mse 0.00000331, maxerr 0.01476848, 95pct<0.0034, median<0.0016
q4_1::layers.20.feed_forward.w2.weight            : mse 0.00000300, maxerr 0.04094645, 95pct<0.0032, median<0.0014
q4_1::layers.20.feed_forward.w3.weight            : mse 0.00000306, maxerr 0.01144791, 95pct<0.0032, median<0.0014
q4_1::layers.21.attention.wk.weight               : mse 0.00000371, maxerr 0.01407850, 95pct<0.0038, median<0.0014
q4_1::layers.21.attention.wo.weight               : mse 0.00000294, maxerr 0.04772949, 95pct<0.0032, median<0.0014
q4_1::layers.21.attention.wq.weight               : mse 0.00000363, maxerr 0.02847900, 95pct<0.0038, median<0.0014
q4_1::layers.21.attention.wv.weight               : mse 0.00000302, maxerr 0.00687256, 95pct<0.0032, median<0.0014
q4_1::layers.21.feed_forward.w1.weight            : mse 0.00000334, maxerr 0.01481831, 95pct<0.0034, median<0.0016
q4_1::layers.21.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02454491, 95pct<0.0032, median<0.0014
q4_1::layers.21.feed_forward.w3.weight            : mse 0.00000305, maxerr 0.00941722, 95pct<0.0032, median<0.0014
q4_1::layers.22.attention.wk.weight               : mse 0.00000382, maxerr 0.01457518, 95pct<0.0038, median<0.0016
q4_1::layers.22.attention.wo.weight               : mse 0.00000296, maxerr 0.05219725, 95pct<0.0032, median<0.0014
q4_1::layers.22.attention.wq.weight               : mse 0.00000375, maxerr 0.02614343, 95pct<0.0038, median<0.0016
q4_1::layers.22.attention.wv.weight               : mse 0.00000298, maxerr 0.00633164, 95pct<0.0032, median<0.0014
q4_1::layers.22.feed_forward.w1.weight            : mse 0.00000335, maxerr 0.01621208, 95pct<0.0034, median<0.0016
q4_1::layers.22.feed_forward.w2.weight            : mse 0.00000302, maxerr 0.02524516, 95pct<0.0032, median<0.0014
q4_1::layers.22.feed_forward.w3.weight            : mse 0.00000308, maxerr 0.01791126, 95pct<0.0032, median<0.0016
q4_1::layers.23.attention.wk.weight               : mse 0.00000355, maxerr 0.01381835, 95pct<0.0038, median<0.0014
q4_1::layers.23.attention.wo.weight               : mse 0.00000312, maxerr 0.05039060, 95pct<0.0032, median<0.0014
q4_1::layers.23.attention.wq.weight               : mse 0.00000352, maxerr 0.02543131, 95pct<0.0036, median<0.0014
q4_1::layers.23.attention.wv.weight               : mse 0.00000323, maxerr 0.00705466, 95pct<0.0034, median<0.0016
q4_1::layers.23.feed_forward.w1.weight            : mse 0.00000336, maxerr 0.02019602, 95pct<0.0034, median<0.0016
q4_1::layers.23.feed_forward.w2.weight            : mse 0.00000304, maxerr 0.02755737, 95pct<0.0032, median<0.0014
q4_1::layers.23.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01500538, 95pct<0.0032, median<0.0016
q4_1::layers.24.attention.wk.weight               : mse 0.00000358, maxerr 0.01357117, 95pct<0.0038, median<0.0014
q4_1::layers.24.attention.wo.weight               : mse 0.00000320, maxerr 0.03517246, 95pct<0.0032, median<0.0016
q4_1::layers.24.attention.wq.weight               : mse 0.00000353, maxerr 0.02697754, 95pct<0.0036, median<0.0014
q4_1::layers.24.attention.wv.weight               : mse 0.00000330, maxerr 0.00695597, 95pct<0.0034, median<0.0016
q4_1::layers.24.feed_forward.w1.weight            : mse 0.00000337, maxerr 0.01255596, 95pct<0.0034, median<0.0016
q4_1::layers.24.feed_forward.w2.weight            : mse 0.00000307, maxerr 0.03697109, 95pct<0.0032, median<0.0016
q4_1::layers.24.feed_forward.w3.weight            : mse 0.00000312, maxerr 0.01249239, 95pct<0.0032, median<0.0016
q4_1::layers.25.attention.wk.weight               : mse 0.00000382, maxerr 0.01319379, 95pct<0.0038, median<0.0016
q4_1::layers.25.attention.wo.weight               : mse 0.00000326, maxerr 0.03653157, 95pct<0.0034, median<0.0016
q4_1::layers.25.attention.wq.weight               : mse 0.00000373, maxerr 0.02534175, 95pct<0.0036, median<0.0016
q4_1::layers.25.attention.wv.weight               : mse 0.00000333, maxerr 0.00774231, 95pct<0.0034, median<0.0016
q4_1::layers.25.feed_forward.w1.weight            : mse 0.00000339, maxerr 0.01365763, 95pct<0.0034, median<0.0016
q4_1::layers.25.feed_forward.w2.weight            : mse 0.00000309, maxerr 0.02395630, 95pct<0.0032, median<0.0016
q4_1::layers.25.feed_forward.w3.weight            : mse 0.00000315, maxerr 0.01177013, 95pct<0.0032, median<0.0016
q4_1::layers.26.attention.wk.weight               : mse 0.00000370, maxerr 0.01424815, 95pct<0.0036, median<0.0016
q4_1::layers.26.attention.wo.weight               : mse 0.00000345, maxerr 0.02384442, 95pct<0.0034, median<0.0016
q4_1::layers.26.attention.wq.weight               : mse 0.00000361, maxerr 0.02352905, 95pct<0.0036, median<0.0016
q4_1::layers.26.attention.wv.weight               : mse 0.00000353, maxerr 0.00762227, 95pct<0.0034, median<0.0016
q4_1::layers.26.feed_forward.w1.weight            : mse 0.00000338, maxerr 0.02146912, 95pct<0.0034, median<0.0016
q4_1::layers.26.feed_forward.w2.weight            : mse 0.00000313, maxerr 0.02818197, 95pct<0.0032, median<0.0016
q4_1::layers.26.feed_forward.w3.weight            : mse 0.00000319, maxerr 0.02482224, 95pct<0.0032, median<0.0016
q4_1::layers.27.attention.wk.weight               : mse 0.00000367, maxerr 0.01493329, 95pct<0.0036, median<0.0016
q4_1::layers.27.attention.wo.weight               : mse 0.00000362, maxerr 0.05037433, 95pct<0.0034, median<0.0016
q4_1::layers.27.attention.wq.weight               : mse 0.00000361, maxerr 0.02156782, 95pct<0.0036, median<0.0016
q4_1::layers.27.attention.wv.weight               : mse 0.00000365, maxerr 0.00810165, 95pct<0.0036, median<0.0016
q4_1::layers.27.feed_forward.w1.weight            : mse 0.00000338, maxerr 0.02540493, 95pct<0.0034, median<0.0016
q4_1::layers.27.feed_forward.w2.weight            : mse 0.00000316, maxerr 0.02953517, 95pct<0.0032, median<0.0016
q4_1::layers.27.feed_forward.w3.weight            : mse 0.00000322, maxerr 0.02640279, 95pct<0.0032, median<0.0016
q4_1::layers.28.attention.wk.weight               : mse 0.00000351, maxerr 0.01595867, 95pct<0.0036, median<0.0014
q4_1::layers.28.attention.wo.weight               : mse 0.00000370, maxerr 0.02981770, 95pct<0.0036, median<0.0016
q4_1::layers.28.attention.wq.weight               : mse 0.00000347, maxerr 0.02494049, 95pct<0.0036, median<0.0014
q4_1::layers.28.attention.wv.weight               : mse 0.00000369, maxerr 0.00791423, 95pct<0.0036, median<0.0016
q4_1::layers.28.feed_forward.w1.weight            : mse 0.00000335, maxerr 0.02810669, 95pct<0.0034, median<0.0016
q4_1::layers.28.feed_forward.w2.weight            : mse 0.00000319, maxerr 0.03309225, 95pct<0.0032, median<0.0016
q4_1::layers.28.feed_forward.w3.weight            : mse 0.00000325, maxerr 0.02055053, 95pct<0.0032, median<0.0016
q4_1::layers.29.attention.wk.weight               : mse 0.00000346, maxerr 0.01428223, 95pct<0.0036, median<0.0014
q4_1::layers.29.attention.wo.weight               : mse 0.00000392, maxerr 0.03439641, 95pct<0.0036, median<0.0016
q4_1::layers.29.attention.wq.weight               : mse 0.00000340, maxerr 0.02388712, 95pct<0.0036, median<0.0014
q4_1::layers.29.attention.wv.weight               : mse 0.00000391, maxerr 0.00761922, 95pct<0.0036, median<0.0016
q4_1::layers.29.feed_forward.w1.weight            : mse 0.00000337, maxerr 0.02038574, 95pct<0.0034, median<0.0016
q4_1::layers.29.feed_forward.w2.weight            : mse 0.00000321, maxerr 0.05755107, 95pct<0.0032, median<0.0016
q4_1::layers.29.feed_forward.w3.weight            : mse 0.00000329, maxerr 0.01542050, 95pct<0.0034, median<0.0016
q4_1::layers.3.attention.wk.weight                : mse 0.00000611, maxerr 0.01627603, 95pct<0.0050, median<0.0018
q4_1::layers.3.attention.wo.weight                : mse 0.00000167, maxerr 0.03495282, 95pct<0.0024, median<0.0012
q4_1::layers.3.attention.wq.weight                : mse 0.00000552, maxerr 0.02804718, 95pct<0.0046, median<0.0018
q4_1::layers.3.attention.wv.weight                : mse 0.00000167, maxerr 0.00555267, 95pct<0.0024, median<0.0012
q4_1::layers.3.feed_forward.w1.weight             : mse 0.00000322, maxerr 0.02117920, 95pct<0.0032, median<0.0016
q4_1::layers.3.feed_forward.w2.weight             : mse 0.00000273, maxerr 0.03491618, 95pct<0.0030, median<0.0014
q4_1::layers.3.feed_forward.w3.weight             : mse 0.00000272, maxerr 0.01362103, 95pct<0.0030, median<0.0014
q4_1::layers.30.attention.wk.weight               : mse 0.00000353, maxerr 0.01795453, 95pct<0.0036, median<0.0014
q4_1::layers.30.attention.wo.weight               : mse 0.00000391, maxerr 0.04497075, 95pct<0.0036, median<0.0016
q4_1::layers.30.attention.wq.weight               : mse 0.00000347, maxerr 0.02401968, 95pct<0.0036, median<0.0014
q4_1::layers.30.attention.wv.weight               : mse 0.00000381, maxerr 0.00759277, 95pct<0.0036, median<0.0016
q4_1::layers.30.feed_forward.w1.weight            : mse 0.00000341, maxerr 0.01782125, 95pct<0.0034, median<0.0016
q4_1::layers.30.feed_forward.w2.weight            : mse 0.00000342, maxerr 0.12756348, 95pct<0.0032, median<0.0016
q4_1::layers.30.feed_forward.w3.weight            : mse 0.00000335, maxerr 0.02418011, 95pct<0.0034, median<0.0016
q4_1::layers.31.attention.wk.weight               : mse 0.00000375, maxerr 0.01362303, 95pct<0.0038, median<0.0016
q4_1::layers.31.attention.wo.weight               : mse 0.00000319, maxerr 0.10161138, 95pct<0.0032, median<0.0014
q4_1::layers.31.attention.wq.weight               : mse 0.00000357, maxerr 0.01820374, 95pct<0.0036, median<0.0016
q4_1::layers.31.attention.wv.weight               : mse 0.00000310, maxerr 0.00997696, 95pct<0.0032, median<0.0014
q4_1::layers.31.feed_forward.w1.weight            : mse 0.00000371, maxerr 0.02717184, 95pct<0.0036, median<0.0016
q4_1::layers.31.feed_forward.w2.weight            : mse 0.00000336, maxerr 0.09575176, 95pct<0.0034, median<0.0016
q4_1::layers.31.feed_forward.w3.weight            : mse 0.00000363, maxerr 0.03244019, 95pct<0.0034, median<0.0016
q4_1::layers.4.attention.wk.weight                : mse 0.00000589, maxerr 0.01599121, 95pct<0.0048, median<0.0018
q4_1::layers.4.attention.wo.weight                : mse 0.00000167, maxerr 0.02775061, 95pct<0.0024, median<0.0012
q4_1::layers.4.attention.wq.weight                : mse 0.00000573, maxerr 0.02762246, 95pct<0.0046, median<0.0018
q4_1::layers.4.attention.wv.weight                : mse 0.00000167, maxerr 0.00593816, 95pct<0.0024, median<0.0010
q4_1::layers.4.feed_forward.w1.weight             : mse 0.00000330, maxerr 0.02607116, 95pct<0.0034, median<0.0016
q4_1::layers.4.feed_forward.w2.weight             : mse 0.00000271, maxerr 0.04353638, 95pct<0.0030, median<0.0014
q4_1::layers.4.feed_forward.w3.weight             : mse 0.00000274, maxerr 0.02257079, 95pct<0.0030, median<0.0014
q4_1::layers.5.attention.wk.weight                : mse 0.00000527, maxerr 0.02016246, 95pct<0.0046, median<0.0016
q4_1::layers.5.attention.wo.weight                : mse 0.00000171, maxerr 0.04142249, 95pct<0.0024, median<0.0012
q4_1::layers.5.attention.wq.weight                : mse 0.00000516, maxerr 0.02691448, 95pct<0.0044, median<0.0016
q4_1::layers.5.attention.wv.weight                : mse 0.00000173, maxerr 0.00809692, 95pct<0.0024, median<0.0012
q4_1::layers.5.feed_forward.w1.weight             : mse 0.00000344, maxerr 0.02066040, 95pct<0.0034, median<0.0016
q4_1::layers.5.feed_forward.w2.weight             : mse 0.00000266, maxerr 0.02678931, 95pct<0.0030, median<0.0014
q4_1::layers.5.feed_forward.w3.weight             : mse 0.00000272, maxerr 0.01605021, 95pct<0.0030, median<0.0014
q4_1::layers.6.attention.wk.weight                : mse 0.00000552, maxerr 0.01503804, 95pct<0.0046, median<0.0018
q4_1::layers.6.attention.wo.weight                : mse 0.00000175, maxerr 0.03727213, 95pct<0.0024, median<0.0012
q4_1::layers.6.attention.wq.weight                : mse 0.00000526, maxerr 0.03130698, 95pct<0.0044, median<0.0018
q4_1::layers.6.attention.wv.weight                : mse 0.00000176, maxerr 0.00586955, 95pct<0.0024, median<0.0012
q4_1::layers.6.feed_forward.w1.weight             : mse 0.00000334, maxerr 0.02278137, 95pct<0.0034, median<0.0016
q4_1::layers.6.feed_forward.w2.weight             : mse 0.00000272, maxerr 0.03055978, 95pct<0.0030, median<0.0014
q4_1::layers.6.feed_forward.w3.weight             : mse 0.00000279, maxerr 0.01386768, 95pct<0.0030, median<0.0014
q4_1::layers.7.attention.wk.weight                : mse 0.00000518, maxerr 0.01637778, 95pct<0.0044, median<0.0016
q4_1::layers.7.attention.wo.weight                : mse 0.00000182, maxerr 0.02817380, 95pct<0.0026, median<0.0012
q4_1::layers.7.attention.wq.weight                : mse 0.00000509, maxerr 0.02885771, 95pct<0.0044, median<0.0016
q4_1::layers.7.attention.wv.weight                : mse 0.00000187, maxerr 0.00640869, 95pct<0.0026, median<0.0012
q4_1::layers.7.feed_forward.w1.weight             : mse 0.00000328, maxerr 0.01696777, 95pct<0.0034, median<0.0016
q4_1::layers.7.feed_forward.w2.weight             : mse 0.00000274, maxerr 0.02849120, 95pct<0.0030, median<0.0014
q4_1::layers.7.feed_forward.w3.weight             : mse 0.00000281, maxerr 0.01903725, 95pct<0.0030, median<0.0014
q4_1::layers.8.attention.wk.weight                : mse 0.00000493, maxerr 0.01597899, 95pct<0.0044, median<0.0016
q4_1::layers.8.attention.wo.weight                : mse 0.00000181, maxerr 0.02582398, 95pct<0.0026, median<0.0012
q4_1::layers.8.attention.wq.weight                : mse 0.00000492, maxerr 0.02330780, 95pct<0.0044, median<0.0016
q4_1::layers.8.attention.wv.weight                : mse 0.00000183, maxerr 0.00699462, 95pct<0.0026, median<0.0012
q4_1::layers.8.feed_forward.w1.weight             : mse 0.00000328, maxerr 0.01851404, 95pct<0.0034, median<0.0016
q4_1::layers.8.feed_forward.w2.weight             : mse 0.00000274, maxerr 0.02776897, 95pct<0.0030, median<0.0014
q4_1::layers.8.feed_forward.w3.weight             : mse 0.00000283, maxerr 0.01309204, 95pct<0.0030, median<0.0014
q4_1::layers.9.attention.wk.weight                : mse 0.00000468, maxerr 0.01326293, 95pct<0.0044, median<0.0016
q4_1::layers.9.attention.wo.weight                : mse 0.00000178, maxerr 0.03066409, 95pct<0.0024, median<0.0012
q4_1::layers.9.attention.wq.weight                : mse 0.00000461, maxerr 0.02470907, 95pct<0.0042, median<0.0016
q4_1::layers.9.attention.wv.weight                : mse 0.00000180, maxerr 0.00619888, 95pct<0.0026, median<0.0012
q4_1::layers.9.feed_forward.w1.weight             : mse 0.00000319, maxerr 0.02470452, 95pct<0.0034, median<0.0014
q4_1::layers.9.feed_forward.w2.weight             : mse 0.00000278, maxerr 0.02815247, 95pct<0.0030, median<0.0014
q4_1::layers.9.feed_forward.w3.weight             : mse 0.00000286, maxerr 0.02717841, 95pct<0.0032, median<0.0014
q4_1::output.weight                               : mse 0.00000251, maxerr 0.01462148, 95pct<0.0030, median<0.0014
q4_1::tok_embeddings.weight                       : mse 0.00000250, maxerr 0.01170197, 95pct<0.0030, median<0.0014
q4_1                                              : mse 0.00000318, maxerr 0.12756348, 95pct<0.0034, median<0.0014




quantize-stats after (7B)
note: source model is f16
testing 226 layers with max size 131072000, allocating 1572864000 bytes
q4_0::layers.0.attention.wk.weight                : mse 0.00000946, maxerr 0.07012939, 95pct<0.0062, median<0.0018
q4_0::layers.0.attention.wo.weight                : mse 0.00000114, maxerr 0.04718018, 95pct<0.0022, median<0.0008
q4_0::layers.0.attention.wq.weight                : mse 0.00000990, maxerr 0.04913330, 95pct<0.0066, median<0.0018
q4_0::layers.0.attention.wv.weight                : mse 0.00000138, maxerr 0.00809479, 95pct<0.0024, median<0.0010
q4_0::layers.0.feed_forward.w1.weight             : mse 0.00000213, maxerr 0.06494141, 95pct<0.0026, median<0.0012
q4_0::layers.0.feed_forward.w2.weight             : mse 0.00000315, maxerr 0.05014038, 95pct<0.0032, median<0.0016
q4_0::layers.0.feed_forward.w3.weight             : mse 0.00000199, maxerr 0.01788330, 95pct<0.0026, median<0.0012
q4_0::layers.1.attention.wk.weight                : mse 0.00000900, maxerr 0.04061890, 95pct<0.0064, median<0.0018
q4_0::layers.1.attention.wo.weight                : mse 0.00000107, maxerr 0.06164551, 95pct<0.0020, median<0.0008
q4_0::layers.1.attention.wq.weight                : mse 0.00000860, maxerr 0.03482056, 95pct<0.0062, median<0.0018
q4_0::layers.1.attention.wv.weight                : mse 0.00000101, maxerr 0.00719452, 95pct<0.0020, median<0.0008
q4_0::layers.1.feed_forward.w1.weight             : mse 0.00000343, maxerr 0.03784180, 95pct<0.0034, median<0.0016
q4_0::layers.1.feed_forward.w2.weight             : mse 0.00000331, maxerr 0.05532837, 95pct<0.0032, median<0.0016
q4_0::layers.1.feed_forward.w3.weight             : mse 0.00000309, maxerr 0.02270508, 95pct<0.0032, median<0.0016
q4_0::layers.10.attention.wk.weight               : mse 0.00000574, maxerr 0.02407837, 95pct<0.0048, median<0.0018
q4_0::layers.10.attention.wo.weight               : mse 0.00000233, maxerr 0.03552246, 95pct<0.0028, median<0.0014
q4_0::layers.10.attention.wq.weight               : mse 0.00000561, maxerr 0.03735352, 95pct<0.0046, median<0.0018
q4_0::layers.10.attention.wv.weight               : mse 0.00000235, maxerr 0.01551819, 95pct<0.0028, median<0.0014
q4_0::layers.10.feed_forward.w1.weight            : mse 0.00000380, maxerr 0.02473450, 95pct<0.0036, median<0.0016
q4_0::layers.10.feed_forward.w2.weight            : mse 0.00000344, maxerr 0.04382324, 95pct<0.0034, median<0.0016
q4_0::layers.10.feed_forward.w3.weight            : mse 0.00000355, maxerr 0.02250671, 95pct<0.0034, median<0.0016
q4_0::layers.11.attention.wk.weight               : mse 0.00000614, maxerr 0.02574158, 95pct<0.0048, median<0.0018
q4_0::layers.11.attention.wo.weight               : mse 0.00000256, maxerr 0.03140259, 95pct<0.0030, median<0.0014
q4_0::layers.11.attention.wq.weight               : mse 0.00000594, maxerr 0.04638672, 95pct<0.0046, median<0.0018
q4_0::layers.11.attention.wv.weight               : mse 0.00000260, maxerr 0.01353455, 95pct<0.0030, median<0.0014
q4_0::layers.11.feed_forward.w1.weight            : mse 0.00000378, maxerr 0.02500916, 95pct<0.0036, median<0.0016
q4_0::layers.11.feed_forward.w2.weight            : mse 0.00000349, maxerr 0.05606079, 95pct<0.0034, median<0.0016
q4_0::layers.11.feed_forward.w3.weight            : mse 0.00000359, maxerr 0.02583313, 95pct<0.0034, median<0.0016
q4_0::layers.12.attention.wk.weight               : mse 0.00000559, maxerr 0.02272034, 95pct<0.0046, median<0.0018
q4_0::layers.12.attention.wo.weight               : mse 0.00000247, maxerr 0.02410889, 95pct<0.0028, median<0.0014
q4_0::layers.12.attention.wq.weight               : mse 0.00000541, maxerr 0.03787231, 95pct<0.0046, median<0.0018
q4_0::layers.12.attention.wv.weight               : mse 0.00000243, maxerr 0.00985718, 95pct<0.0028, median<0.0014
q4_0::layers.12.feed_forward.w1.weight            : mse 0.00000382, maxerr 0.03527832, 95pct<0.0036, median<0.0016
q4_0::layers.12.feed_forward.w2.weight            : mse 0.00000350, maxerr 0.05621338, 95pct<0.0034, median<0.0016
q4_0::layers.12.feed_forward.w3.weight            : mse 0.00000364, maxerr 0.01757812, 95pct<0.0034, median<0.0016
q4_0::layers.13.attention.wk.weight               : mse 0.00000533, maxerr 0.02510071, 95pct<0.0046, median<0.0016
q4_0::layers.13.attention.wo.weight               : mse 0.00000265, maxerr 0.04525757, 95pct<0.0030, median<0.0014
q4_0::layers.13.attention.wq.weight               : mse 0.00000516, maxerr 0.03643799, 95pct<0.0044, median<0.0016
q4_0::layers.13.attention.wv.weight               : mse 0.00000265, maxerr 0.01044464, 95pct<0.0030, median<0.0014
q4_0::layers.13.feed_forward.w1.weight            : mse 0.00000379, maxerr 0.02149963, 95pct<0.0036, median<0.0016
q4_0::layers.13.feed_forward.w2.weight            : mse 0.00000356, maxerr 0.03344727, 95pct<0.0034, median<0.0016
q4_0::layers.13.feed_forward.w3.weight            : mse 0.00000371, maxerr 0.01843262, 95pct<0.0036, median<0.0016
q4_0::layers.14.attention.wk.weight               : mse 0.00000532, maxerr 0.02272034, 95pct<0.0044, median<0.0018
q4_0::layers.14.attention.wo.weight               : mse 0.00000267, maxerr 0.03359985, 95pct<0.0030, median<0.0014
q4_0::layers.14.attention.wq.weight               : mse 0.00000523, maxerr 0.04104614, 95pct<0.0044, median<0.0018
q4_0::layers.14.attention.wv.weight               : mse 0.00000269, maxerr 0.00988770, 95pct<0.0030, median<0.0014
q4_0::layers.14.feed_forward.w1.weight            : mse 0.00000378, maxerr 0.02416992, 95pct<0.0036, median<0.0016
q4_0::layers.14.feed_forward.w2.weight            : mse 0.00000360, maxerr 0.05963135, 95pct<0.0034, median<0.0016
q4_0::layers.14.feed_forward.w3.weight            : mse 0.00000373, maxerr 0.02426147, 95pct<0.0036, median<0.0016
q4_0::layers.15.attention.wk.weight               : mse 0.00000542, maxerr 0.02012634, 95pct<0.0046, median<0.0018
q4_0::layers.15.attention.wo.weight               : mse 0.00000268, maxerr 0.02880859, 95pct<0.0030, median<0.0014
q4_0::layers.15.attention.wq.weight               : mse 0.00000523, maxerr 0.03628540, 95pct<0.0044, median<0.0018
q4_0::layers.15.attention.wv.weight               : mse 0.00000270, maxerr 0.00939941, 95pct<0.0030, median<0.0014
q4_0::layers.15.feed_forward.w1.weight            : mse 0.00000378, maxerr 0.02140808, 95pct<0.0036, median<0.0016
q4_0::layers.15.feed_forward.w2.weight            : mse 0.00000360, maxerr 0.06188965, 95pct<0.0034, median<0.0016
q4_0::layers.15.feed_forward.w3.weight            : mse 0.00000374, maxerr 0.02241516, 95pct<0.0036, median<0.0016
q4_0::layers.16.attention.wk.weight               : mse 0.00000535, maxerr 0.01998901, 95pct<0.0044, median<0.0018
q4_0::layers.16.attention.wo.weight               : mse 0.00000301, maxerr 0.04467773, 95pct<0.0032, median<0.0014
q4_0::layers.16.attention.wq.weight               : mse 0.00000509, maxerr 0.04324341, 95pct<0.0044, median<0.0018
q4_0::layers.16.attention.wv.weight               : mse 0.00000306, maxerr 0.00990295, 95pct<0.0032, median<0.0014
q4_0::layers.16.feed_forward.w1.weight            : mse 0.00000383, maxerr 0.02427673, 95pct<0.0036, median<0.0016
q4_0::layers.16.feed_forward.w2.weight            : mse 0.00000359, maxerr 0.05804443, 95pct<0.0034, median<0.0016
q4_0::layers.16.feed_forward.w3.weight            : mse 0.00000371, maxerr 0.02345276, 95pct<0.0036, median<0.0016
q4_0::layers.17.attention.wk.weight               : mse 0.00000508, maxerr 0.01977539, 95pct<0.0044, median<0.0018
q4_0::layers.17.attention.wo.weight               : mse 0.00000309, maxerr 0.02990723, 95pct<0.0032, median<0.0014
q4_0::layers.17.attention.wq.weight               : mse 0.00000487, maxerr 0.04873657, 95pct<0.0042, median<0.0018
q4_0::layers.17.attention.wv.weight               : mse 0.00000310, maxerr 0.01419830, 95pct<0.0032, median<0.0014
q4_0::layers.17.feed_forward.w1.weight            : mse 0.00000385, maxerr 0.01963806, 95pct<0.0036, median<0.0016
q4_0::layers.17.feed_forward.w2.weight            : mse 0.00000362, maxerr 0.04980469, 95pct<0.0034, median<0.0016
q4_0::layers.17.feed_forward.w3.weight            : mse 0.00000373, maxerr 0.02487183, 95pct<0.0036, median<0.0016
q4_0::layers.18.attention.wk.weight               : mse 0.00000489, maxerr 0.01959229, 95pct<0.0044, median<0.0016
q4_0::layers.18.attention.wo.weight               : mse 0.00000307, maxerr 0.05526733, 95pct<0.0032, median<0.0014
q4_0::layers.18.attention.wq.weight               : mse 0.00000477, maxerr 0.04193115, 95pct<0.0042, median<0.0016
q4_0::layers.18.attention.wv.weight               : mse 0.00000308, maxerr 0.00922394, 95pct<0.0032, median<0.0014
q4_0::layers.18.feed_forward.w1.weight            : mse 0.00000391, maxerr 0.02268982, 95pct<0.0036, median<0.0016
q4_0::layers.18.feed_forward.w2.weight            : mse 0.00000360, maxerr 0.06439209, 95pct<0.0034, median<0.0016
q4_0::layers.18.feed_forward.w3.weight            : mse 0.00000371, maxerr 0.01829529, 95pct<0.0036, median<0.0016
q4_0::layers.19.attention.wk.weight               : mse 0.00000471, maxerr 0.02081299, 95pct<0.0042, median<0.0016
q4_0::layers.19.attention.wo.weight               : mse 0.00000333, maxerr 0.04681396, 95pct<0.0034, median<0.0016
q4_0::layers.19.attention.wq.weight               : mse 0.00000460, maxerr 0.04876709, 95pct<0.0042, median<0.0016
q4_0::layers.19.attention.wv.weight               : mse 0.00000339, maxerr 0.01137543, 95pct<0.0034, median<0.0016
q4_0::layers.19.feed_forward.w1.weight            : mse 0.00000395, maxerr 0.03121948, 95pct<0.0036, median<0.0016
q4_0::layers.19.feed_forward.w2.weight            : mse 0.00000361, maxerr 0.04312134, 95pct<0.0034, median<0.0016
q4_0::layers.19.feed_forward.w3.weight            : mse 0.00000369, maxerr 0.02177429, 95pct<0.0034, median<0.0016
q4_0::layers.2.attention.wk.weight                : mse 0.00001086, maxerr 0.03179932, 95pct<0.0066, median<0.0022
q4_0::layers.2.attention.wo.weight                : mse 0.00000149, maxerr 0.04443359, 95pct<0.0022, median<0.0010
q4_0::layers.2.attention.wq.weight                : mse 0.00001007, maxerr 0.03594971, 95pct<0.0064, median<0.0022
q4_0::layers.2.attention.wv.weight                : mse 0.00000144, maxerr 0.01062012, 95pct<0.0022, median<0.0010
q4_0::layers.2.feed_forward.w1.weight             : mse 0.00000381, maxerr 0.04077148, 95pct<0.0036, median<0.0016
q4_0::layers.2.feed_forward.w2.weight             : mse 0.00000328, maxerr 0.09649658, 95pct<0.0032, median<0.0016
q4_0::layers.2.feed_forward.w3.weight             : mse 0.00000317, maxerr 0.03201294, 95pct<0.0032, median<0.0016
q4_0::layers.20.attention.wk.weight               : mse 0.00000486, maxerr 0.02493286, 95pct<0.0042, median<0.0016
q4_0::layers.20.attention.wo.weight               : mse 0.00000350, maxerr 0.03179932, 95pct<0.0034, median<0.0016
q4_0::layers.20.attention.wq.weight               : mse 0.00000473, maxerr 0.05462646, 95pct<0.0042, median<0.0016
q4_0::layers.20.attention.wv.weight               : mse 0.00000360, maxerr 0.01089478, 95pct<0.0034, median<0.0016
q4_0::layers.20.feed_forward.w1.weight            : mse 0.00000400, maxerr 0.02357483, 95pct<0.0036, median<0.0016
q4_0::layers.20.feed_forward.w2.weight            : mse 0.00000362, maxerr 0.06982422, 95pct<0.0034, median<0.0016
q4_0::layers.20.feed_forward.w3.weight            : mse 0.00000369, maxerr 0.01661682, 95pct<0.0034, median<0.0016
q4_0::layers.21.attention.wk.weight               : mse 0.00000451, maxerr 0.02557373, 95pct<0.0042, median<0.0016
q4_0::layers.21.attention.wo.weight               : mse 0.00000354, maxerr 0.07818604, 95pct<0.0034, median<0.0016
q4_0::layers.21.attention.wq.weight               : mse 0.00000443, maxerr 0.05007935, 95pct<0.0040, median<0.0016
q4_0::layers.21.attention.wv.weight               : mse 0.00000365, maxerr 0.01040649, 95pct<0.0036, median<0.0016
q4_0::layers.21.feed_forward.w1.weight            : mse 0.00000403, maxerr 0.02252197, 95pct<0.0036, median<0.0016
q4_0::layers.21.feed_forward.w2.weight            : mse 0.00000362, maxerr 0.03781128, 95pct<0.0034, median<0.0016
q4_0::layers.21.feed_forward.w3.weight            : mse 0.00000368, maxerr 0.01501465, 95pct<0.0034, median<0.0016
q4_0::layers.22.attention.wk.weight               : mse 0.00000465, maxerr 0.01992798, 95pct<0.0042, median<0.0016
q4_0::layers.22.attention.wo.weight               : mse 0.00000357, maxerr 0.09454346, 95pct<0.0034, median<0.0016
q4_0::layers.22.attention.wq.weight               : mse 0.00000458, maxerr 0.04550171, 95pct<0.0040, median<0.0016
q4_0::layers.22.attention.wv.weight               : mse 0.00000361, maxerr 0.01099396, 95pct<0.0034, median<0.0016
q4_0::layers.22.feed_forward.w1.weight            : mse 0.00000405, maxerr 0.02517700, 95pct<0.0036, median<0.0018
q4_0::layers.22.feed_forward.w2.weight            : mse 0.00000365, maxerr 0.04281616, 95pct<0.0034, median<0.0016
q4_0::layers.22.feed_forward.w3.weight            : mse 0.00000371, maxerr 0.03140259, 95pct<0.0036, median<0.0016
q4_0::layers.23.attention.wk.weight               : mse 0.00000433, maxerr 0.02102661, 95pct<0.0040, median<0.0016
q4_0::layers.23.attention.wo.weight               : mse 0.00000377, maxerr 0.04870605, 95pct<0.0036, median<0.0016
q4_0::layers.23.attention.wq.weight               : mse 0.00000430, maxerr 0.04418945, 95pct<0.0040, median<0.0016
q4_0::layers.23.attention.wv.weight               : mse 0.00000390, maxerr 0.01161957, 95pct<0.0036, median<0.0016
q4_0::layers.23.feed_forward.w1.weight            : mse 0.00000406, maxerr 0.03436279, 95pct<0.0036, median<0.0018
q4_0::layers.23.feed_forward.w2.weight            : mse 0.00000367, maxerr 0.04855347, 95pct<0.0034, median<0.0016
q4_0::layers.23.feed_forward.w3.weight            : mse 0.00000373, maxerr 0.02526855, 95pct<0.0036, median<0.0016
q4_0::layers.24.attention.wk.weight               : mse 0.00000436, maxerr 0.02143860, 95pct<0.0040, median<0.0016
q4_0::layers.24.attention.wo.weight               : mse 0.00000386, maxerr 0.05621338, 95pct<0.0036, median<0.0016
q4_0::layers.24.attention.wq.weight               : mse 0.00000431, maxerr 0.04943848, 95pct<0.0040, median<0.0016
q4_0::layers.24.attention.wv.weight               : mse 0.00000399, maxerr 0.01126862, 95pct<0.0036, median<0.0016
q4_0::layers.24.feed_forward.w1.weight            : mse 0.00000407, maxerr 0.02159119, 95pct<0.0036, median<0.0018
q4_0::layers.24.feed_forward.w2.weight            : mse 0.00000371, maxerr 0.06005859, 95pct<0.0034, median<0.0016
q4_0::layers.24.feed_forward.w3.weight            : mse 0.00000377, maxerr 0.02104187, 95pct<0.0036, median<0.0016
q4_0::layers.25.attention.wk.weight               : mse 0.00000464, maxerr 0.02005005, 95pct<0.0040, median<0.0016
q4_0::layers.25.attention.wo.weight               : mse 0.00000393, maxerr 0.04763794, 95pct<0.0036, median<0.0016
q4_0::layers.25.attention.wq.weight               : mse 0.00000455, maxerr 0.03808594, 95pct<0.0040, median<0.0016
q4_0::layers.25.attention.wv.weight               : mse 0.00000402, maxerr 0.01160431, 95pct<0.0036, median<0.0016
q4_0::layers.25.feed_forward.w1.weight            : mse 0.00000409, maxerr 0.02044678, 95pct<0.0036, median<0.0018
q4_0::layers.25.feed_forward.w2.weight            : mse 0.00000373, maxerr 0.03298950, 95pct<0.0036, median<0.0016
q4_0::layers.25.feed_forward.w3.weight            : mse 0.00000380, maxerr 0.01884460, 95pct<0.0036, median<0.0016
q4_0::layers.26.attention.wk.weight               : mse 0.00000449, maxerr 0.02630615, 95pct<0.0040, median<0.0016
q4_0::layers.26.attention.wo.weight               : mse 0.00000416, maxerr 0.02603149, 95pct<0.0038, median<0.0018
q4_0::layers.26.attention.wq.weight               : mse 0.00000440, maxerr 0.03735352, 95pct<0.0040, median<0.0016
q4_0::layers.26.attention.wv.weight               : mse 0.00000426, maxerr 0.01278687, 95pct<0.0038, median<0.0018
q4_0::layers.26.feed_forward.w1.weight            : mse 0.00000409, maxerr 0.03500366, 95pct<0.0036, median<0.0018
q4_0::layers.26.feed_forward.w2.weight            : mse 0.00000378, maxerr 0.04370117, 95pct<0.0036, median<0.0016
q4_0::layers.26.feed_forward.w3.weight            : mse 0.00000386, maxerr 0.03059387, 95pct<0.0036, median<0.0016
q4_0::layers.27.attention.wk.weight               : mse 0.00000446, maxerr 0.02406311, 95pct<0.0040, median<0.0016
q4_0::layers.27.attention.wo.weight               : mse 0.00000437, maxerr 0.07098389, 95pct<0.0038, median<0.0018
q4_0::layers.27.attention.wq.weight               : mse 0.00000442, maxerr 0.04006958, 95pct<0.0040, median<0.0016
q4_0::layers.27.attention.wv.weight               : mse 0.00000441, maxerr 0.01357269, 95pct<0.0038, median<0.0018
q4_0::layers.27.feed_forward.w1.weight            : mse 0.00000408, maxerr 0.02963257, 95pct<0.0036, median<0.0018
q4_0::layers.27.feed_forward.w2.weight            : mse 0.00000383, maxerr 0.04663086, 95pct<0.0036, median<0.0016
q4_0::layers.27.feed_forward.w3.weight            : mse 0.00000389, maxerr 0.04153442, 95pct<0.0036, median<0.0016
q4_0::layers.28.attention.wk.weight               : mse 0.00000427, maxerr 0.02304077, 95pct<0.0040, median<0.0016
q4_0::layers.28.attention.wo.weight               : mse 0.00000446, maxerr 0.05538940, 95pct<0.0038, median<0.0018
q4_0::layers.28.attention.wq.weight               : mse 0.00000424, maxerr 0.04208374, 95pct<0.0040, median<0.0016
q4_0::layers.28.attention.wv.weight               : mse 0.00000446, maxerr 0.01184082, 95pct<0.0038, median<0.0018
q4_0::layers.28.feed_forward.w1.weight            : mse 0.00000405, maxerr 0.03170776, 95pct<0.0036, median<0.0016
q4_0::layers.28.feed_forward.w2.weight            : mse 0.00000387, maxerr 0.05294800, 95pct<0.0036, median<0.0016
q4_0::layers.28.feed_forward.w3.weight            : mse 0.00000393, maxerr 0.03025818, 95pct<0.0036, median<0.0016
q4_0::layers.29.attention.wk.weight               : mse 0.00000421, maxerr 0.01965332, 95pct<0.0040, median<0.0016
q4_0::layers.29.attention.wo.weight               : mse 0.00000473, maxerr 0.04461670, 95pct<0.0040, median<0.0018
q4_0::layers.29.attention.wq.weight               : mse 0.00000417, maxerr 0.04244995, 95pct<0.0038, median<0.0016
q4_0::layers.29.attention.wv.weight               : mse 0.00000473, maxerr 0.01258850, 95pct<0.0040, median<0.0018
q4_0::layers.29.feed_forward.w1.weight            : mse 0.00000407, maxerr 0.03314209, 95pct<0.0036, median<0.0016
q4_0::layers.29.feed_forward.w2.weight            : mse 0.00000391, maxerr 0.09802246, 95pct<0.0036, median<0.0016
q4_0::layers.29.feed_forward.w3.weight            : mse 0.00000397, maxerr 0.02760315, 95pct<0.0036, median<0.0016
q4_0::layers.3.attention.wk.weight                : mse 0.00000748, maxerr 0.02275085, 95pct<0.0054, median<0.0020
q4_0::layers.3.attention.wo.weight                : mse 0.00000202, maxerr 0.05377197, 95pct<0.0026, median<0.0012
q4_0::layers.3.attention.wq.weight                : mse 0.00000683, maxerr 0.04766846, 95pct<0.0050, median<0.0020
q4_0::layers.3.attention.wv.weight                : mse 0.00000202, maxerr 0.00859070, 95pct<0.0026, median<0.0012
q4_0::layers.3.feed_forward.w1.weight             : mse 0.00000389, maxerr 0.03158569, 95pct<0.0036, median<0.0016
q4_0::layers.3.feed_forward.w2.weight             : mse 0.00000331, maxerr 0.05627441, 95pct<0.0034, median<0.0016
q4_0::layers.3.feed_forward.w3.weight             : mse 0.00000329, maxerr 0.02278137, 95pct<0.0034, median<0.0016
q4_0::layers.30.attention.wk.weight               : mse 0.00000430, maxerr 0.02133179, 95pct<0.0040, median<0.0016
q4_0::layers.30.attention.wo.weight               : mse 0.00000472, maxerr 0.06579590, 95pct<0.0040, median<0.0018
q4_0::layers.30.attention.wq.weight               : mse 0.00000427, maxerr 0.04168701, 95pct<0.0038, median<0.0016
q4_0::layers.30.attention.wv.weight               : mse 0.00000461, maxerr 0.01303864, 95pct<0.0040, median<0.0018
q4_0::layers.30.feed_forward.w1.weight            : mse 0.00000412, maxerr 0.02958679, 95pct<0.0038, median<0.0016
q4_0::layers.30.feed_forward.w2.weight            : mse 0.00000410, maxerr 0.18200684, 95pct<0.0036, median<0.0016
q4_0::layers.30.feed_forward.w3.weight            : mse 0.00000405, maxerr 0.03591919, 95pct<0.0036, median<0.0018
q4_0::layers.31.attention.wk.weight               : mse 0.00000459, maxerr 0.02066040, 95pct<0.0040, median<0.0016
q4_0::layers.31.attention.wo.weight               : mse 0.00000385, maxerr 0.17346191, 95pct<0.0036, median<0.0016
q4_0::layers.31.attention.wq.weight               : mse 0.00000440, maxerr 0.02816772, 95pct<0.0040, median<0.0016
q4_0::layers.31.attention.wv.weight               : mse 0.00000375, maxerr 0.01520538, 95pct<0.0036, median<0.0016
q4_0::layers.31.feed_forward.w1.weight            : mse 0.00000450, maxerr 0.02647400, 95pct<0.0038, median<0.0018
q4_0::layers.31.feed_forward.w2.weight            : mse 0.00000414, maxerr 0.11260986, 95pct<0.0036, median<0.0018
q4_0::layers.31.feed_forward.w3.weight            : mse 0.00000440, maxerr 0.04486084, 95pct<0.0038, median<0.0018
q4_0::layers.4.attention.wk.weight                : mse 0.00000719, maxerr 0.02165222, 95pct<0.0052, median<0.0020
q4_0::layers.4.attention.wo.weight                : mse 0.00000202, maxerr 0.04003906, 95pct<0.0026, median<0.0012
q4_0::layers.4.attention.wq.weight                : mse 0.00000707, maxerr 0.04748535, 95pct<0.0052, median<0.0020
q4_0::layers.4.attention.wv.weight                : mse 0.00000202, maxerr 0.00906372, 95pct<0.0026, median<0.0012
q4_0::layers.4.feed_forward.w1.weight             : mse 0.00000399, maxerr 0.03872681, 95pct<0.0036, median<0.0016
q4_0::layers.4.feed_forward.w2.weight             : mse 0.00000328, maxerr 0.05072021, 95pct<0.0032, median<0.0016
q4_0::layers.4.feed_forward.w3.weight             : mse 0.00000331, maxerr 0.03533936, 95pct<0.0034, median<0.0016
q4_0::layers.5.attention.wk.weight                : mse 0.00000640, maxerr 0.03253174, 95pct<0.0050, median<0.0018
q4_0::layers.5.attention.wo.weight                : mse 0.00000207, maxerr 0.04260254, 95pct<0.0026, median<0.0012
q4_0::layers.5.attention.wq.weight                : mse 0.00000631, maxerr 0.04281616, 95pct<0.0048, median<0.0018
q4_0::layers.5.attention.wv.weight                : mse 0.00000209, maxerr 0.01441193, 95pct<0.0026, median<0.0012
q4_0::layers.5.feed_forward.w1.weight             : mse 0.00000416, maxerr 0.03350830, 95pct<0.0038, median<0.0018
q4_0::layers.5.feed_forward.w2.weight             : mse 0.00000322, maxerr 0.04428101, 95pct<0.0032, median<0.0016
q4_0::layers.5.feed_forward.w3.weight             : mse 0.00000329, maxerr 0.02728271, 95pct<0.0034, median<0.0016
q4_0::layers.6.attention.wk.weight                : mse 0.00000670, maxerr 0.02120972, 95pct<0.0050, median<0.0020
q4_0::layers.6.attention.wo.weight                : mse 0.00000211, maxerr 0.05706787, 95pct<0.0026, median<0.0012
q4_0::layers.6.attention.wq.weight                : mse 0.00000641, maxerr 0.04986572, 95pct<0.0048, median<0.0020
q4_0::layers.6.attention.wv.weight                : mse 0.00000212, maxerr 0.00904083, 95pct<0.0028, median<0.0012
q4_0::layers.6.feed_forward.w1.weight             : mse 0.00000404, maxerr 0.04025269, 95pct<0.0036, median<0.0016
q4_0::layers.6.feed_forward.w2.weight             : mse 0.00000329, maxerr 0.04974365, 95pct<0.0032, median<0.0016
q4_0::layers.6.feed_forward.w3.weight             : mse 0.00000337, maxerr 0.02461243, 95pct<0.0034, median<0.0016
q4_0::layers.7.attention.wk.weight                : mse 0.00000629, maxerr 0.02215576, 95pct<0.0050, median<0.0018
q4_0::layers.7.attention.wo.weight                : mse 0.00000220, maxerr 0.03393555, 95pct<0.0028, median<0.0012
q4_0::layers.7.attention.wq.weight                : mse 0.00000619, maxerr 0.04800415, 95pct<0.0048, median<0.0018
q4_0::layers.7.attention.wv.weight                : mse 0.00000226, maxerr 0.00874329, 95pct<0.0028, median<0.0012
q4_0::layers.7.feed_forward.w1.weight             : mse 0.00000397, maxerr 0.02743530, 95pct<0.0036, median<0.0016
q4_0::layers.7.feed_forward.w2.weight             : mse 0.00000331, maxerr 0.04418945, 95pct<0.0034, median<0.0016
q4_0::layers.7.feed_forward.w3.weight             : mse 0.00000340, maxerr 0.02548218, 95pct<0.0034, median<0.0016
q4_0::layers.8.attention.wk.weight                : mse 0.00000599, maxerr 0.02326965, 95pct<0.0048, median<0.0018
q4_0::layers.8.attention.wo.weight                : mse 0.00000218, maxerr 0.03485107, 95pct<0.0028, median<0.0012
q4_0::layers.8.attention.wq.weight                : mse 0.00000598, maxerr 0.04147339, 95pct<0.0048, median<0.0018
q4_0::layers.8.attention.wv.weight                : mse 0.00000221, maxerr 0.01078796, 95pct<0.0028, median<0.0012
q4_0::layers.8.feed_forward.w1.weight             : mse 0.00000397, maxerr 0.02899170, 95pct<0.0036, median<0.0016
q4_0::layers.8.feed_forward.w2.weight             : mse 0.00000332, maxerr 0.03820801, 95pct<0.0034, median<0.0016
q4_0::layers.8.feed_forward.w3.weight             : mse 0.00000342, maxerr 0.02285767, 95pct<0.0034, median<0.0016
q4_0::layers.9.attention.wk.weight                : mse 0.00000567, maxerr 0.02212524, 95pct<0.0048, median<0.0018
q4_0::layers.9.attention.wo.weight                : mse 0.00000215, maxerr 0.03619385, 95pct<0.0028, median<0.0012
q4_0::layers.9.attention.wq.weight                : mse 0.00000560, maxerr 0.04025269, 95pct<0.0046, median<0.0018
q4_0::layers.9.attention.wv.weight                : mse 0.00000218, maxerr 0.00840759, 95pct<0.0028, median<0.0012
q4_0::layers.9.feed_forward.w1.weight             : mse 0.00000386, maxerr 0.04083252, 95pct<0.0036, median<0.0016
q4_0::layers.9.feed_forward.w2.weight             : mse 0.00000337, maxerr 0.04580688, 95pct<0.0034, median<0.0016
q4_0::layers.9.feed_forward.w3.weight             : mse 0.00000346, maxerr 0.04849243, 95pct<0.0034, median<0.0016
q4_0::output.weight                               : mse 0.00000308, maxerr 0.02610779, 95pct<0.0032, median<0.0014
q4_0::tok_embeddings.weight                       : mse 0.00000302, maxerr 0.01635742, 95pct<0.0032, median<0.0014
q4_0                                              : mse 0.00000386, maxerr 0.18200684, 95pct<0.0036, median<0.0016
q4_1::layers.0.attention.wk.weight                : mse 0.00000684, maxerr 0.04107666, 95pct<0.0054, median<0.0016
q4_1::layers.0.attention.wo.weight                : mse 0.00000092, maxerr 0.02982175, 95pct<0.0020, median<0.0008
q4_1::layers.0.attention.wq.weight                : mse 0.00000702, maxerr 0.02834333, 95pct<0.0056, median<0.0016
q4_1::layers.0.attention.wv.weight                : mse 0.00000114, maxerr 0.00560303, 95pct<0.0022, median<0.0008
q4_1::layers.0.feed_forward.w1.weight             : mse 0.00000175, maxerr 0.03655243, 95pct<0.0024, median<0.0012
q4_1::layers.0.feed_forward.w2.weight             : mse 0.00000259, maxerr 0.04300943, 95pct<0.0030, median<0.0014
q4_1::layers.0.feed_forward.w3.weight             : mse 0.00000165, maxerr 0.01006266, 95pct<0.0024, median<0.0012
q4_1::layers.1.attention.wk.weight                : mse 0.00000728, maxerr 0.02334900, 95pct<0.0058, median<0.0016
q4_1::layers.1.attention.wo.weight                : mse 0.00000086, maxerr 0.03453889, 95pct<0.0018, median<0.0008
q4_1::layers.1.attention.wq.weight                : mse 0.00000700, maxerr 0.01987410, 95pct<0.0056, median<0.0016
q4_1::layers.1.attention.wv.weight                : mse 0.00000083, maxerr 0.00478211, 95pct<0.0018, median<0.0008
q4_1::layers.1.feed_forward.w1.weight             : mse 0.00000283, maxerr 0.02051294, 95pct<0.0030, median<0.0014
q4_1::layers.1.feed_forward.w2.weight             : mse 0.00000272, maxerr 0.03843182, 95pct<0.0030, median<0.0014
q4_1::layers.1.feed_forward.w3.weight             : mse 0.00000255, maxerr 0.01320738, 95pct<0.0030, median<0.0014
q4_1::layers.10.attention.wk.weight               : mse 0.00000472, maxerr 0.01563987, 95pct<0.0044, median<0.0016
q4_1::layers.10.attention.wo.weight               : mse 0.00000193, maxerr 0.02667642, 95pct<0.0026, median<0.0012
q4_1::layers.10.attention.wq.weight               : mse 0.00000462, maxerr 0.02052003, 95pct<0.0042, median<0.0016
q4_1::layers.10.attention.wv.weight               : mse 0.00000194, maxerr 0.00943857, 95pct<0.0026, median<0.0012
q4_1::layers.10.feed_forward.w1.weight            : mse 0.00000314, maxerr 0.01556396, 95pct<0.0032, median<0.0014
q4_1::layers.10.feed_forward.w2.weight            : mse 0.00000283, maxerr 0.02537537, 95pct<0.0030, median<0.0014
q4_1::layers.10.feed_forward.w3.weight            : mse 0.00000294, maxerr 0.01292909, 95pct<0.0032, median<0.0014
q4_1::layers.11.attention.wk.weight               : mse 0.00000505, maxerr 0.01603444, 95pct<0.0044, median<0.0016
q4_1::layers.11.attention.wo.weight               : mse 0.00000212, maxerr 0.02708334, 95pct<0.0028, median<0.0012
q4_1::layers.11.attention.wq.weight               : mse 0.00000490, maxerr 0.02761781, 95pct<0.0042, median<0.0016
q4_1::layers.11.attention.wv.weight               : mse 0.00000215, maxerr 0.00829771, 95pct<0.0028, median<0.0012
q4_1::layers.11.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01594034, 95pct<0.0032, median<0.0014
q4_1::layers.11.feed_forward.w2.weight            : mse 0.00000288, maxerr 0.03227139, 95pct<0.0032, median<0.0014
q4_1::layers.11.feed_forward.w3.weight            : mse 0.00000297, maxerr 0.01712444, 95pct<0.0032, median<0.0014
q4_1::layers.12.attention.wk.weight               : mse 0.00000460, maxerr 0.01596579, 95pct<0.0042, median<0.0016
q4_1::layers.12.attention.wo.weight               : mse 0.00000205, maxerr 0.02017212, 95pct<0.0026, median<0.0012
q4_1::layers.12.attention.wq.weight               : mse 0.00000446, maxerr 0.02285360, 95pct<0.0042, median<0.0016
q4_1::layers.12.attention.wv.weight               : mse 0.00000200, maxerr 0.00573961, 95pct<0.0026, median<0.0012
q4_1::layers.12.feed_forward.w1.weight            : mse 0.00000316, maxerr 0.02284165, 95pct<0.0032, median<0.0014
q4_1::layers.12.feed_forward.w2.weight            : mse 0.00000289, maxerr 0.03540853, 95pct<0.0032, median<0.0014
q4_1::layers.12.feed_forward.w3.weight            : mse 0.00000301, maxerr 0.01079203, 95pct<0.0032, median<0.0014
q4_1::layers.13.attention.wk.weight               : mse 0.00000439, maxerr 0.01437837, 95pct<0.0042, median<0.0016
q4_1::layers.13.attention.wo.weight               : mse 0.00000220, maxerr 0.03025717, 95pct<0.0028, median<0.0012
q4_1::layers.13.attention.wq.weight               : mse 0.00000425, maxerr 0.02128702, 95pct<0.0040, median<0.0016
q4_1::layers.13.attention.wv.weight               : mse 0.00000219, maxerr 0.00622152, 95pct<0.0028, median<0.0012
q4_1::layers.13.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01500246, 95pct<0.0032, median<0.0014
q4_1::layers.13.feed_forward.w2.weight            : mse 0.00000294, maxerr 0.02633134, 95pct<0.0032, median<0.0014
q4_1::layers.13.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01099548, 95pct<0.0032, median<0.0014
q4_1::layers.14.attention.wk.weight               : mse 0.00000438, maxerr 0.01511636, 95pct<0.0042, median<0.0016
q4_1::layers.14.attention.wo.weight               : mse 0.00000221, maxerr 0.02438152, 95pct<0.0028, median<0.0012
q4_1::layers.14.attention.wq.weight               : mse 0.00000431, maxerr 0.02371013, 95pct<0.0040, median<0.0016
q4_1::layers.14.attention.wv.weight               : mse 0.00000222, maxerr 0.00633850, 95pct<0.0028, median<0.0012
q4_1::layers.14.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01666871, 95pct<0.0032, median<0.0014
q4_1::layers.14.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.03455403, 95pct<0.0032, median<0.0014
q4_1::layers.14.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01462197, 95pct<0.0032, median<0.0016
q4_1::layers.15.attention.wk.weight               : mse 0.00000446, maxerr 0.01495159, 95pct<0.0042, median<0.0016
q4_1::layers.15.attention.wo.weight               : mse 0.00000222, maxerr 0.02541506, 95pct<0.0028, median<0.0012
q4_1::layers.15.attention.wq.weight               : mse 0.00000431, maxerr 0.02229919, 95pct<0.0040, median<0.0016
q4_1::layers.15.attention.wv.weight               : mse 0.00000223, maxerr 0.00649338, 95pct<0.0028, median<0.0012
q4_1::layers.15.feed_forward.w1.weight            : mse 0.00000313, maxerr 0.01446533, 95pct<0.0032, median<0.0014
q4_1::layers.15.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.04414570, 95pct<0.0032, median<0.0014
q4_1::layers.15.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01306508, 95pct<0.0032, median<0.0016
q4_1::layers.16.attention.wk.weight               : mse 0.00000441, maxerr 0.01464437, 95pct<0.0040, median<0.0016
q4_1::layers.16.attention.wo.weight               : mse 0.00000250, maxerr 0.04169917, 95pct<0.0030, median<0.0014
q4_1::layers.16.attention.wq.weight               : mse 0.00000419, maxerr 0.02543133, 95pct<0.0040, median<0.0016
q4_1::layers.16.attention.wv.weight               : mse 0.00000253, maxerr 0.00660706, 95pct<0.0030, median<0.0014
q4_1::layers.16.feed_forward.w1.weight            : mse 0.00000317, maxerr 0.01479188, 95pct<0.0032, median<0.0014
q4_1::layers.16.feed_forward.w2.weight            : mse 0.00000297, maxerr 0.03672282, 95pct<0.0032, median<0.0014
q4_1::layers.16.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01314189, 95pct<0.0032, median<0.0014
q4_1::layers.17.attention.wk.weight               : mse 0.00000418, maxerr 0.01311338, 95pct<0.0040, median<0.0016
q4_1::layers.17.attention.wo.weight               : mse 0.00000256, maxerr 0.02875367, 95pct<0.0030, median<0.0014
q4_1::layers.17.attention.wq.weight               : mse 0.00000401, maxerr 0.03082275, 95pct<0.0038, median<0.0016
q4_1::layers.17.attention.wv.weight               : mse 0.00000256, maxerr 0.00813599, 95pct<0.0030, median<0.0014
q4_1::layers.17.feed_forward.w1.weight            : mse 0.00000319, maxerr 0.01236165, 95pct<0.0032, median<0.0016
q4_1::layers.17.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02805888, 95pct<0.0032, median<0.0014
q4_1::layers.17.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01775716, 95pct<0.0032, median<0.0016
q4_1::layers.18.attention.wk.weight               : mse 0.00000403, maxerr 0.01377869, 95pct<0.0040, median<0.0014
q4_1::layers.18.attention.wo.weight               : mse 0.00000254, maxerr 0.03247070, 95pct<0.0030, median<0.0014
q4_1::layers.18.attention.wq.weight               : mse 0.00000392, maxerr 0.02439576, 95pct<0.0038, median<0.0014
q4_1::layers.18.attention.wv.weight               : mse 0.00000255, maxerr 0.00598729, 95pct<0.0030, median<0.0014
q4_1::layers.18.feed_forward.w1.weight            : mse 0.00000324, maxerr 0.01477051, 95pct<0.0034, median<0.0016
q4_1::layers.18.feed_forward.w2.weight            : mse 0.00000298, maxerr 0.03860271, 95pct<0.0032, median<0.0014
q4_1::layers.18.feed_forward.w3.weight            : mse 0.00000307, maxerr 0.01042479, 95pct<0.0032, median<0.0014
q4_1::layers.19.attention.wk.weight               : mse 0.00000388, maxerr 0.01365611, 95pct<0.0038, median<0.0014
q4_1::layers.19.attention.wo.weight               : mse 0.00000276, maxerr 0.03216144, 95pct<0.0030, median<0.0014
q4_1::layers.19.attention.wq.weight               : mse 0.00000378, maxerr 0.02803510, 95pct<0.0038, median<0.0014
q4_1::layers.19.attention.wv.weight               : mse 0.00000280, maxerr 0.00700684, 95pct<0.0030, median<0.0014
q4_1::layers.19.feed_forward.w1.weight            : mse 0.00000328, maxerr 0.01841432, 95pct<0.0034, median<0.0016
q4_1::layers.19.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02656788, 95pct<0.0032, median<0.0014
q4_1::layers.19.feed_forward.w3.weight            : mse 0.00000306, maxerr 0.01330259, 95pct<0.0032, median<0.0014
q4_1::layers.2.attention.wk.weight                : mse 0.00000883, maxerr 0.01976573, 95pct<0.0062, median<0.0020
q4_1::layers.2.attention.wo.weight                : mse 0.00000123, maxerr 0.03828126, 95pct<0.0020, median<0.0010
q4_1::layers.2.attention.wq.weight                : mse 0.00000823, maxerr 0.02216390, 95pct<0.0058, median<0.0020
q4_1::layers.2.attention.wv.weight                : mse 0.00000119, maxerr 0.00736135, 95pct<0.0020, median<0.0010
q4_1::layers.2.feed_forward.w1.weight             : mse 0.00000315, maxerr 0.03544718, 95pct<0.0032, median<0.0016
q4_1::layers.2.feed_forward.w2.weight             : mse 0.00000271, maxerr 0.05198061, 95pct<0.0030, median<0.0014
q4_1::layers.2.feed_forward.w3.weight             : mse 0.00000262, maxerr 0.01909560, 95pct<0.0030, median<0.0014
q4_1::layers.20.attention.wk.weight               : mse 0.00000400, maxerr 0.01639201, 95pct<0.0038, median<0.0016
q4_1::layers.20.attention.wo.weight               : mse 0.00000290, maxerr 0.02312827, 95pct<0.0032, median<0.0014
q4_1::layers.20.attention.wq.weight               : mse 0.00000388, maxerr 0.03564453, 95pct<0.0038, median<0.0016
q4_1::layers.20.attention.wv.weight               : mse 0.00000298, maxerr 0.00713094, 95pct<0.0032, median<0.0014
q4_1::layers.20.feed_forward.w1.weight            : mse 0.00000331, maxerr 0.01476848, 95pct<0.0034, median<0.0016
q4_1::layers.20.feed_forward.w2.weight            : mse 0.00000300, maxerr 0.04094645, 95pct<0.0032, median<0.0014
q4_1::layers.20.feed_forward.w3.weight            : mse 0.00000306, maxerr 0.01144791, 95pct<0.0032, median<0.0014
q4_1::layers.21.attention.wk.weight               : mse 0.00000371, maxerr 0.01407850, 95pct<0.0038, median<0.0014
q4_1::layers.21.attention.wo.weight               : mse 0.00000294, maxerr 0.04772949, 95pct<0.0032, median<0.0014
q4_1::layers.21.attention.wq.weight               : mse 0.00000363, maxerr 0.02847900, 95pct<0.0038, median<0.0014
q4_1::layers.21.attention.wv.weight               : mse 0.00000302, maxerr 0.00687256, 95pct<0.0032, median<0.0014
q4_1::layers.21.feed_forward.w1.weight            : mse 0.00000334, maxerr 0.01481831, 95pct<0.0034, median<0.0016
q4_1::layers.21.feed_forward.w2.weight            : mse 0.00000299, maxerr 0.02454491, 95pct<0.0032, median<0.0014
q4_1::layers.21.feed_forward.w3.weight            : mse 0.00000305, maxerr 0.00941722, 95pct<0.0032, median<0.0014
q4_1::layers.22.attention.wk.weight               : mse 0.00000382, maxerr 0.01457518, 95pct<0.0038, median<0.0016
q4_1::layers.22.attention.wo.weight               : mse 0.00000296, maxerr 0.05219725, 95pct<0.0032, median<0.0014
q4_1::layers.22.attention.wq.weight               : mse 0.00000375, maxerr 0.02614343, 95pct<0.0038, median<0.0016
q4_1::layers.22.attention.wv.weight               : mse 0.00000298, maxerr 0.00633164, 95pct<0.0032, median<0.0014
q4_1::layers.22.feed_forward.w1.weight            : mse 0.00000335, maxerr 0.01621208, 95pct<0.0034, median<0.0016
q4_1::layers.22.feed_forward.w2.weight            : mse 0.00000302, maxerr 0.02524516, 95pct<0.0032, median<0.0014
q4_1::layers.22.feed_forward.w3.weight            : mse 0.00000308, maxerr 0.01791126, 95pct<0.0032, median<0.0016
q4_1::layers.23.attention.wk.weight               : mse 0.00000355, maxerr 0.01381835, 95pct<0.0038, median<0.0014
q4_1::layers.23.attention.wo.weight               : mse 0.00000312, maxerr 0.05039060, 95pct<0.0032, median<0.0014
q4_1::layers.23.attention.wq.weight               : mse 0.00000352, maxerr 0.02543131, 95pct<0.0036, median<0.0014
q4_1::layers.23.attention.wv.weight               : mse 0.00000323, maxerr 0.00705466, 95pct<0.0034, median<0.0016
q4_1::layers.23.feed_forward.w1.weight            : mse 0.00000336, maxerr 0.02019602, 95pct<0.0034, median<0.0016
q4_1::layers.23.feed_forward.w2.weight            : mse 0.00000304, maxerr 0.02755737, 95pct<0.0032, median<0.0014
q4_1::layers.23.feed_forward.w3.weight            : mse 0.00000309, maxerr 0.01500538, 95pct<0.0032, median<0.0016
q4_1::layers.24.attention.wk.weight               : mse 0.00000358, maxerr 0.01357117, 95pct<0.0038, median<0.0014
q4_1::layers.24.attention.wo.weight               : mse 0.00000320, maxerr 0.03517246, 95pct<0.0032, median<0.0016
q4_1::layers.24.attention.wq.weight               : mse 0.00000353, maxerr 0.02697754, 95pct<0.0036, median<0.0014
q4_1::layers.24.attention.wv.weight               : mse 0.00000330, maxerr 0.00695597, 95pct<0.0034, median<0.0016
q4_1::layers.24.feed_forward.w1.weight            : mse 0.00000337, maxerr 0.01255596, 95pct<0.0034, median<0.0016
q4_1::layers.24.feed_forward.w2.weight            : mse 0.00000307, maxerr 0.03697109, 95pct<0.0032, median<0.0016
q4_1::layers.24.feed_forward.w3.weight            : mse 0.00000312, maxerr 0.01249239, 95pct<0.0032, median<0.0016
q4_1::layers.25.attention.wk.weight               : mse 0.00000382, maxerr 0.01319379, 95pct<0.0038, median<0.0016
q4_1::layers.25.attention.wo.weight               : mse 0.00000326, maxerr 0.03653157, 95pct<0.0034, median<0.0016
q4_1::layers.25.attention.wq.weight               : mse 0.00000373, maxerr 0.02534175, 95pct<0.0036, median<0.0016
q4_1::layers.25.attention.wv.weight               : mse 0.00000333, maxerr 0.00774231, 95pct<0.0034, median<0.0016
q4_1::layers.25.feed_forward.w1.weight            : mse 0.00000339, maxerr 0.01365763, 95pct<0.0034, median<0.0016
q4_1::layers.25.feed_forward.w2.weight            : mse 0.00000309, maxerr 0.02395630, 95pct<0.0032, median<0.0016
q4_1::layers.25.feed_forward.w3.weight            : mse 0.00000315, maxerr 0.01177013, 95pct<0.0032, median<0.0016
q4_1::layers.26.attention.wk.weight               : mse 0.00000370, maxerr 0.01424815, 95pct<0.0036, median<0.0016
q4_1::layers.26.attention.wo.weight               : mse 0.00000345, maxerr 0.02384442, 95pct<0.0034, median<0.0016
q4_1::layers.26.attention.wq.weight               : mse 0.00000361, maxerr 0.02352905, 95pct<0.0036, median<0.0016
q4_1::layers.26.attention.wv.weight               : mse 0.00000353, maxerr 0.00762227, 95pct<0.0034, median<0.0016
q4_1::layers.26.feed_forward.w1.weight            : mse 0.00000338, maxerr 0.02146912, 95pct<0.0034, median<0.0016
q4_1::layers.26.feed_forward.w2.weight            : mse 0.00000313, maxerr 0.02818197, 95pct<0.0032, median<0.0016
q4_1::layers.26.feed_forward.w3.weight            : mse 0.00000319, maxerr 0.02482224, 95pct<0.0032, median<0.0016
q4_1::layers.27.attention.wk.weight               : mse 0.00000367, maxerr 0.01493329, 95pct<0.0036, median<0.0016
q4_1::layers.27.attention.wo.weight               : mse 0.00000362, maxerr 0.05037433, 95pct<0.0034, median<0.0016
q4_1::layers.27.attention.wq.weight               : mse 0.00000361, maxerr 0.02156782, 95pct<0.0036, median<0.0016
q4_1::layers.27.attention.wv.weight               : mse 0.00000365, maxerr 0.00810165, 95pct<0.0036, median<0.0016
q4_1::layers.27.feed_forward.w1.weight            : mse 0.00000338, maxerr 0.02540493, 95pct<0.0034, median<0.0016
q4_1::layers.27.feed_forward.w2.weight            : mse 0.00000316, maxerr 0.02953517, 95pct<0.0032, median<0.0016
q4_1::layers.27.feed_forward.w3.weight            : mse 0.00000322, maxerr 0.02640279, 95pct<0.0032, median<0.0016
q4_1::layers.28.attention.wk.weight               : mse 0.00000351, maxerr 0.01595867, 95pct<0.0036, median<0.0014
q4_1::layers.28.attention.wo.weight               : mse 0.00000370, maxerr 0.02981770, 95pct<0.0036, median<0.0016
q4_1::layers.28.attention.wq.weight               : mse 0.00000347, maxerr 0.02494049, 95pct<0.0036, median<0.0014
q4_1::layers.28.attention.wv.weight               : mse 0.00000369, maxerr 0.00791423, 95pct<0.0036, median<0.0016
q4_1::layers.28.feed_forward.w1.weight            : mse 0.00000335, maxerr 0.02810669, 95pct<0.0034, median<0.0016
q4_1::layers.28.feed_forward.w2.weight            : mse 0.00000319, maxerr 0.03309225, 95pct<0.0032, median<0.0016
q4_1::layers.28.feed_forward.w3.weight            : mse 0.00000325, maxerr 0.02055053, 95pct<0.0032, median<0.0016
q4_1::layers.29.attention.wk.weight               : mse 0.00000346, maxerr 0.01428223, 95pct<0.0036, median<0.0014
q4_1::layers.29.attention.wo.weight               : mse 0.00000392, maxerr 0.03439641, 95pct<0.0036, median<0.0016
q4_1::layers.29.attention.wq.weight               : mse 0.00000340, maxerr 0.02388712, 95pct<0.0036, median<0.0014
q4_1::layers.29.attention.wv.weight               : mse 0.00000391, maxerr 0.00761922, 95pct<0.0036, median<0.0016
q4_1::layers.29.feed_forward.w1.weight            : mse 0.00000337, maxerr 0.02038574, 95pct<0.0034, median<0.0016
q4_1::layers.29.feed_forward.w2.weight            : mse 0.00000321, maxerr 0.05755107, 95pct<0.0032, median<0.0016
q4_1::layers.29.feed_forward.w3.weight            : mse 0.00000329, maxerr 0.01542050, 95pct<0.0034, median<0.0016
q4_1::layers.3.attention.wk.weight                : mse 0.00000611, maxerr 0.01627603, 95pct<0.0050, median<0.0018
q4_1::layers.3.attention.wo.weight                : mse 0.00000167, maxerr 0.03495282, 95pct<0.0024, median<0.0012
q4_1::layers.3.attention.wq.weight                : mse 0.00000552, maxerr 0.02804718, 95pct<0.0046, median<0.0018
q4_1::layers.3.attention.wv.weight                : mse 0.00000167, maxerr 0.00555267, 95pct<0.0024, median<0.0012
q4_1::layers.3.feed_forward.w1.weight             : mse 0.00000322, maxerr 0.02117920, 95pct<0.0032, median<0.0016
q4_1::layers.3.feed_forward.w2.weight             : mse 0.00000273, maxerr 0.03491618, 95pct<0.0030, median<0.0014
q4_1::layers.3.feed_forward.w3.weight             : mse 0.00000272, maxerr 0.01362103, 95pct<0.0030, median<0.0014
q4_1::layers.30.attention.wk.weight               : mse 0.00000353, maxerr 0.01795453, 95pct<0.0036, median<0.0014
q4_1::layers.30.attention.wo.weight               : mse 0.00000391, maxerr 0.04497075, 95pct<0.0036, median<0.0016
q4_1::layers.30.attention.wq.weight               : mse 0.00000347, maxerr 0.02401968, 95pct<0.0036, median<0.0014
q4_1::layers.30.attention.wv.weight               : mse 0.00000381, maxerr 0.00759277, 95pct<0.0036, median<0.0016
q4_1::layers.30.feed_forward.w1.weight            : mse 0.00000341, maxerr 0.01782125, 95pct<0.0034, median<0.0016
q4_1::layers.30.feed_forward.w2.weight            : mse 0.00000342, maxerr 0.12756348, 95pct<0.0032, median<0.0016
q4_1::layers.30.feed_forward.w3.weight            : mse 0.00000335, maxerr 0.02418011, 95pct<0.0034, median<0.0016
q4_1::layers.31.attention.wk.weight               : mse 0.00000375, maxerr 0.01362303, 95pct<0.0038, median<0.0016
q4_1::layers.31.attention.wo.weight               : mse 0.00000319, maxerr 0.10161138, 95pct<0.0032, median<0.0014
q4_1::layers.31.attention.wq.weight               : mse 0.00000357, maxerr 0.01820374, 95pct<0.0036, median<0.0016
q4_1::layers.31.attention.wv.weight               : mse 0.00000310, maxerr 0.00997696, 95pct<0.0032, median<0.0014
q4_1::layers.31.feed_forward.w1.weight            : mse 0.00000371, maxerr 0.02717184, 95pct<0.0036, median<0.0016
q4_1::layers.31.feed_forward.w2.weight            : mse 0.00000336, maxerr 0.09575176, 95pct<0.0034, median<0.0016
q4_1::layers.31.feed_forward.w3.weight            : mse 0.00000363, maxerr 0.03244019, 95pct<0.0034, median<0.0016
q4_1::layers.4.attention.wk.weight                : mse 0.00000589, maxerr 0.01599121, 95pct<0.0048, median<0.0018
q4_1::layers.4.attention.wo.weight                : mse 0.00000167, maxerr 0.02775061, 95pct<0.0024, median<0.0012
q4_1::layers.4.attention.wq.weight                : mse 0.00000573, maxerr 0.02762246, 95pct<0.0046, median<0.0018
q4_1::layers.4.attention.wv.weight                : mse 0.00000167, maxerr 0.00593816, 95pct<0.0024, median<0.0010
q4_1::layers.4.feed_forward.w1.weight             : mse 0.00000330, maxerr 0.02607116, 95pct<0.0034, median<0.0016
q4_1::layers.4.feed_forward.w2.weight             : mse 0.00000271, maxerr 0.04353638, 95pct<0.0030, median<0.0014
q4_1::layers.4.feed_forward.w3.weight             : mse 0.00000274, maxerr 0.02257079, 95pct<0.0030, median<0.0014
q4_1::layers.5.attention.wk.weight                : mse 0.00000527, maxerr 0.02016246, 95pct<0.0046, median<0.0016
q4_1::layers.5.attention.wo.weight                : mse 0.00000171, maxerr 0.04142249, 95pct<0.0024, median<0.0012
q4_1::layers.5.attention.wq.weight                : mse 0.00000516, maxerr 0.02691448, 95pct<0.0044, median<0.0016
q4_1::layers.5.attention.wv.weight                : mse 0.00000173, maxerr 0.00809692, 95pct<0.0024, median<0.0012
q4_1::layers.5.feed_forward.w1.weight             : mse 0.00000344, maxerr 0.02066040, 95pct<0.0034, median<0.0016
q4_1::layers.5.feed_forward.w2.weight             : mse 0.00000266, maxerr 0.02678931, 95pct<0.0030, median<0.0014
q4_1::layers.5.feed_forward.w3.weight             : mse 0.00000272, maxerr 0.01605021, 95pct<0.0030, median<0.0014
q4_1::layers.6.attention.wk.weight                : mse 0.00000552, maxerr 0.01503804, 95pct<0.0046, median<0.0018
q4_1::layers.6.attention.wo.weight                : mse 0.00000175, maxerr 0.03727213, 95pct<0.0024, median<0.0012
q4_1::layers.6.attention.wq.weight                : mse 0.00000526, maxerr 0.03130698, 95pct<0.0044, median<0.0018
q4_1::layers.6.attention.wv.weight                : mse 0.00000176, maxerr 0.00586955, 95pct<0.0024, median<0.0012
q4_1::layers.6.feed_forward.w1.weight             : mse 0.00000334, maxerr 0.02278137, 95pct<0.0034, median<0.0016
q4_1::layers.6.feed_forward.w2.weight             : mse 0.00000272, maxerr 0.03055978, 95pct<0.0030, median<0.0014
q4_1::layers.6.feed_forward.w3.weight             : mse 0.00000279, maxerr 0.01386768, 95pct<0.0030, median<0.0014
q4_1::layers.7.attention.wk.weight                : mse 0.00000518, maxerr 0.01637778, 95pct<0.0044, median<0.0016
q4_1::layers.7.attention.wo.weight                : mse 0.00000182, maxerr 0.02817380, 95pct<0.0026, median<0.0012
q4_1::layers.7.attention.wq.weight                : mse 0.00000509, maxerr 0.02885771, 95pct<0.0044, median<0.0016
q4_1::layers.7.attention.wv.weight                : mse 0.00000187, maxerr 0.00640869, 95pct<0.0026, median<0.0012
q4_1::layers.7.feed_forward.w1.weight             : mse 0.00000328, maxerr 0.01696777, 95pct<0.0034, median<0.0016
q4_1::layers.7.feed_forward.w2.weight             : mse 0.00000274, maxerr 0.02849120, 95pct<0.0030, median<0.0014
q4_1::layers.7.feed_forward.w3.weight             : mse 0.00000281, maxerr 0.01903725, 95pct<0.0030, median<0.0014
q4_1::layers.8.attention.wk.weight                : mse 0.00000493, maxerr 0.01597899, 95pct<0.0044, median<0.0016
q4_1::layers.8.attention.wo.weight                : mse 0.00000181, maxerr 0.02582398, 95pct<0.0026, median<0.0012
q4_1::layers.8.attention.wq.weight                : mse 0.00000492, maxerr 0.02330780, 95pct<0.0044, median<0.0016
q4_1::layers.8.attention.wv.weight                : mse 0.00000183, maxerr 0.00699462, 95pct<0.0026, median<0.0012
q4_1::layers.8.feed_forward.w1.weight             : mse 0.00000328, maxerr 0.01851404, 95pct<0.0034, median<0.0016
q4_1::layers.8.feed_forward.w2.weight             : mse 0.00000274, maxerr 0.02776897, 95pct<0.0030, median<0.0014
q4_1::layers.8.feed_forward.w3.weight             : mse 0.00000283, maxerr 0.01309204, 95pct<0.0030, median<0.0014
q4_1::layers.9.attention.wk.weight                : mse 0.00000468, maxerr 0.01326293, 95pct<0.0044, median<0.0016
q4_1::layers.9.attention.wo.weight                : mse 0.00000178, maxerr 0.03066409, 95pct<0.0024, median<0.0012
q4_1::layers.9.attention.wq.weight                : mse 0.00000461, maxerr 0.02470907, 95pct<0.0042, median<0.0016
q4_1::layers.9.attention.wv.weight                : mse 0.00000180, maxerr 0.00619888, 95pct<0.0026, median<0.0012
q4_1::layers.9.feed_forward.w1.weight             : mse 0.00000319, maxerr 0.02470452, 95pct<0.0034, median<0.0014
q4_1::layers.9.feed_forward.w2.weight             : mse 0.00000278, maxerr 0.02815247, 95pct<0.0030, median<0.0014
q4_1::layers.9.feed_forward.w3.weight             : mse 0.00000286, maxerr 0.02717841, 95pct<0.0032, median<0.0014
q4_1::output.weight                               : mse 0.00000251, maxerr 0.01462148, 95pct<0.0030, median<0.0014
q4_1::tok_embeddings.weight                       : mse 0.00000250, maxerr 0.01170197, 95pct<0.0030, median<0.0014
q4_1                                              : mse 0.00000318, maxerr 0.12756348, 95pct<0.0034, median<0.0014

main:    total time = 240118.23 ms",6,31
734,2023-04-03T04:21:54Z,,2023-04-19T02:36:25Z,3,88,2,"I found that when using the current llama.cpp that the first model loading time went down due to OS not knowing which location of the file will be read and it caused a 4kb requests to disk, by reading the file on the seperate thread it will now cause only 1mb requests. Which for example 1000000 4kb requests to disk is decreased down to 4000 1mb requests to disk which is a big difference.
Specifically solves #705
Right now it read for me 500 mb/s, after this patch it now reads 2.3 gb/s.
Try: git clone https://github.com/CoderRC/llama.cpp
to see the difference of first loading time only after restart to produce accurate results
From:
llama_print_timings:        load time = 31928.12 ms
To:
llama_print_timings:        load time = 11478.47 ms",6,14
736,2023-04-03T06:42:24Z,2023-04-03T16:00:55Z,2023-04-03T16:00:55Z,1,5,0,"When using main.exe on Windows in interactive mode, I found that only the first Ctrl-C would interrupt generation. If I later used Ctrl-C again, it always exited the application.
The reason appears to be what is described here: https://stackoverflow.com/questions/43959514/why-the-second-sigint-cant-be-captured-on-win32 - Windows will reset the SIGINT handler to default as soon as a signal was received.
My solution is to capture the signal again each time interactive mode is entered.",4,4
747,2023-04-03T19:05:14Z,2023-04-10T17:57:59Z,2023-04-10T17:57:59Z,1,1,6,Including upper-case Windows.h breaks compilation for x86_64-w64-mingw32 target compilation on a linux host (where the file-system is case-sensitive). Lower-case windows.h should work on Windows (case-insensitive filesystem) as well as linux build hosts.,3,0
748,2023-04-03T20:07:06Z,2023-04-05T14:36:12Z,2023-04-05T14:36:12Z,1,7,0,Added example to build the project using CMake since lot of people have been enquiring about this,5,3
763,2023-04-04T12:33:47Z,2023-04-05T14:38:38Z,2023-04-05T14:38:38Z,1,1,0,It was expected that -march=native -mtune=native are applied to C++ code too.,3,0
765,2023-04-04T15:02:54Z,2023-04-05T10:44:24Z,2023-04-05T10:44:24Z,1,4,1,"Hey!
This repo and the community are amazing, props to the maintainers :)
I am playing around with the added Swift bindings for Mac/iOS and I noticed that BLAS didn't appear to be enabled at runtime with the default Package.swift file even when I explicitly added the Accelerate framework on my target, looks like b/c some preprocessor directives weren't make their way to ggml.
Before:

After:",2,2
768,2023-04-04T20:02:43Z,,2023-04-13T13:13:38Z,1,4,0,"Prefetches some data before it's usage
(basing on original work: #295)
Before (usually 245 - 260 ms):

After (usually 226 - 234 ms):

Tested on Windows 10 + i7-10700k, needs further testing on different OSes and CPUs to make sure there's no unintentional issues hence draft. Also naming constants appropriately!",6,5
769,2023-04-04T20:42:57Z,,2023-04-14T15:40:15Z,3,54,5,"Rewrite of #365 (addresses #57) by @joshmackwilliams, updated to the new folder/file structure as the PR might have been abandoned.
From the original author:

Implements #57.
Stop keywords can be specified using the ""--stop"" parameter. Upon seeing one of these keywords in the generated output, the model will terminate generation immediately. Like reverse prompts, multiple stop keywords can be specified by specifying the --stop argument multiple times.
The implementation is heavily based on the reverse prompt implementation to keep things simple. Tested using 7B (quantized) in both interactive and non-interactive modes.",6,5
770,2023-04-05T07:25:03Z,,2023-04-05T16:19:12Z,1,1,1,"PR #613 did the correct thing to align tensors to match the new format, but forgot to change the magic in convert-gptq-to-ggml.py. This PR fixes this.",5,4
773,2023-04-05T11:40:59Z,2023-04-05T15:06:03Z,2023-04-05T15:06:03Z,2,65,0,This should allow this repo to be used as a Zig library.,2,1
775,2023-04-05T14:11:22Z,2023-04-05T19:07:34Z,2023-04-05T19:07:34Z,3,214,158,"Human generated notes
I believe this should resolve: #603 #677 #767
When I deprecated the non-contiguous ggml_mul_mat() branch in #439, I grossly underestimated the cost of transposing the V matrix on every token. It's a very heavy memory operation, with tons of cache misses.
To solve this, we now store V in the KV cache in transposed state, so we don't need to do it for future tokens.
ggml :

added ggml_view_3d()
ggml_view_tensor() now inherits the stride too
reimplement ggml_cpy() to account for dst stride
no longer require tensor->data to be memory aligned

llama :

compute RoPE on 32-bit tensors (should be more accurate)
store RoPE-ed K in the KV cache
store transposed V in the KV cache (significant speed-up)
avoid unnecessary Q copy



🤖 Generated by Copilot at 1868f6c
Summary
✨⚡📈

This pull request enhances the ggml library and the llama project by adding new tensor operations, optimizing existing ones, and using RoPE for self-attention. It also improves the debuggability of the llama project by adding optional timing and plotting code. The main files affected are ggml.c, ggml.h, and llama.cpp.

Sing, O Muse, of the mighty ggml, the wondrous library of tensors
That skilled programmers devised with cunning and crafty art
To aid the llama project, the swift and fluent speaker of words
That emulates the GPT-2, the wise and powerful oracle of language

Walkthrough

Optimize the self-attention mechanism in llama_eval_internal by using RoPE and view tensors (link, link)
Improve the performance of tensor duplication operations by avoiding unnecessary checks and using memcpy when possible (link, link, link, link)
Add a new function ggml_view_3d to create 3-dimensional view tensors from source tensors (link, link)
Comment out alignment checks in ggml_new_tensor_impl to speed up tensor creation (link)
Copy byte strides in ggml_view_tensor to preserve the source tensor layout (link)
Add optional code for debugging and profiling in llama_eval_internal (link)

Edit:
Merging this since we observe M1 and Windows working good.
If you notice something not right, feel free to revert. But I think it should be good",5,5
781,2023-04-05T16:17:01Z,2023-04-05T19:11:04Z,2023-04-05T19:11:04Z,1,37,4,"🤖 Generated by Copilot at 625f212
Summary
🧶🚀🧑‍💻

This pull request improves the performance of the ggml library by parallelizing the rope operation on tensors, and modifying the graph executor to handle parallel tasks. It affects the file ggml.c.

We'll hoist the sail with a rope operation
ggml_compute_forward_rope_f32 and f16
We'll work in parallel, no more hesitation
On the count of three, pull hard, me friends

Walkthrough

Parallelize the rope operation for both f32 and f16 data types by dividing the input rows among the available threads (link, link, link, link)
Remove the invalid assertions that the thread index is zero from the ggml_compute_forward_rope_f32 and ggml_compute_forward_rope_f16 functions (link, link)
Set the number of tasks for the rope operation node to the number of threads in the ggml.c file (link)",3,0
785,2023-04-05T17:20:39Z,2023-04-06T06:56:58Z,2023-04-06T06:56:58Z,1,5,3,Some users are struggling with docker instructions. Make it more obvious they need to replace the path in the commands below.,2,0
796,2023-04-05T22:59:24Z,2023-04-06T15:59:12Z,2023-04-06T15:59:12Z,1,1,1,"Otherwise observing this in the interactive mode:
/usr/lib/gcc/x86_64-pc-linux-gnu/12/include/g++-v12/bits/stl_vector.h:1230: reference std::vector::back() [_Tp = int, _Alloc = std::allocator]: Assertion '!this->empty()' failed.",2,2
797,2023-04-05T23:05:01Z,2023-04-07T16:11:59Z,2023-04-07T16:11:59Z,1,2,0,"I could get llama-cpp-python working but only when I built libllama.so with make and then replaced the copy of  libllama.so that came with  llama-cpp-python .
I wanted to add this here to help with the llama-cpp-python project.",4,3
801,2023-04-06T05:57:51Z,2023-04-09T23:10:47Z,2023-04-09T23:10:47Z,14,1227,824,"Features:


Support all three formats (ggml, ggmf, ggjt).  (However, I didn't include the hack needed to support GPT4All files without conversion. Those can still be used after converting them with convert.py from my other PR.)


Support both mmap and read (mmap is used by default, but can be disabled with --no-mmap, and is automatically disabled for pre-ggjt files or on platforms where mmap is not supported).


Support multi-file models like before, but automatically determine the number of parts rather than requiring --n_parts.


Improve validation and error checking.


Stop using the per-file type field (f16) entirely in favor of just relying on the per-tensor type/size fields.  This has no immediate benefit, but makes it easier to experiment with different formats, and should make it easier to support the new GPTQ-for-LLaMa models in the future (I have some work in progress on that front).


Support VirtualLock on Windows (using the same --mlock option as on Unix).


Indicate loading progress when using mmap + mlock.  (Which led me to the interesting observation that on my Linux machine, with a warm file cache, mlock actually takes some time, whereas mmap without mlock starts almost instantly...)

To help implement this, move mlock support from ggml to the loading code.





madvise/PrefetchVirtualMemory support (based on #740)


Switch from ifstream to the fopen family of functions to avoid unnecessary copying and, when mmap is enabled, allow reusing the same file descriptor for both metadata reads and mmap (whereas the existing implementation opens the file a second time to mmap).


Quantization now produces a single-file output even with multi-file inputs (not really a feature as much as 'it was easier this way').


Todo:


VirtualLock does not work at all on the one Windows machine I tested it on (it complains about quota).  Figure out why. Fixed.


Verify that using the fopen family of functions actually does what I think it does, performance-wise. Verified that when reading a large amount of data with fread, it passes the pointer directly to the kernel rather than doing an intermediate copy, on Linux, macOS, and Windows.  And ifstream does not do so on at least macOS (didn't test the other two).  So moving from ifstream to the fopen family was indeed an improvement, but there's no benefit to going further and using OS APIs directly.


More testing.


Implementation notes:
I tried to factor the code into more discrete pieces than before.
Regarding code style: I tried to follow the code style, but I'm naughty and used a few advanced C++ features repeatedly:


Destructors to make it easier to ensure everything gets cleaned up.


Exceptions.  I don't even usually use exceptions when writing C++, and I can remove them if desired... but here they make the loading code much more succinct while still properly handling a variety of errors, ranging from API calls failing to integer overflow and allocation failure.  (Edit: The exceptions are converted to error codes at the API boundary.)


Co-authored-by: Pavol Rusnak pavol@rusnak.io (for the bit I copied from #740)",11,17
806,2023-04-06T10:22:03Z,,2023-05-26T19:27:21Z,0,0,0,"Hi, this patch allow the user to specify the path of OpenBLAS headers.
As default the path is set to ""/usr/include""
Thank you.
Vincenzo",2,0
807,2023-04-06T11:56:27Z,2023-04-14T06:24:52Z,2023-04-14T06:24:52Z,1,12,10,"This slightly improves the perf.
Before:
llama_print_timings:        load time =  6973.64 ms
llama_print_timings:      sample time =   345.88 ms /   128 runs   (    2.70 ms per run)
llama_print_timings: prompt eval time =  6227.73 ms /    29 tokens (  214.75 ms per token)
llama_print_timings:        eval time = 34661.36 ms /   127 runs   (  272.92 ms per run)
llama_print_timings:       total time = 41992.88 ms
After:
llama_print_timings:        load time =  7668.98 ms
llama_print_timings:      sample time =   345.03 ms /   128 runs   (    2.70 ms per run)
llama_print_timings: prompt eval time =  6906.97 ms /    29 tokens (  238.17 ms per token)
llama_print_timings:        eval time = 33445.33 ms /   127 runs   (  263.35 ms per run)
llama_print_timings:       total time = 41471.40 ms",4,7
809,2023-04-06T12:50:03Z,,2024-02-19T14:26:47Z,3,109,6,"Add the code to check the build host to determine the right CPU feature.
This is convenient when build Windows version on the machine without AVX2.",7,9
812,2023-04-06T14:17:32Z,2023-04-07T16:02:12Z,2023-04-07T16:02:12Z,1,3,14,"Logits are not sorted before nucleus sampling if TopK is 0 or out of bounds. Reported and solved by @Piezoid #779 (comment).
Other changes:
Since logits are sorted, first index has maximum probability.
Also remove normalization, because std::discrete_distribution already divides by the sum.",3,3
814,2023-04-06T14:22:11Z,2023-04-07T16:05:29Z,2023-04-07T16:05:29Z,1,7,2,I made the same error: linking common.cpp for non-examples.,2,0
816,2023-04-06T15:42:23Z,,2023-04-13T11:26:33Z,1,21,129,"This PR simplified spin logic  in graph compute, benefits:

replaced three sync primitives with just one, so should make  the related codes easy to read, and no potential dead lock.
less spin loops, thus a bit speed up and energy savings.


No obvious logic change, no code deletion, changes are protected by a compile time feature flag `DISABLE_GGML_COMPUTE_SPIN_V2`.
This feature can be disabled by setting `-DDISABLE_GGML_COMPUTE_SPIN_V2` to `CFLAGS` in Makefile.",4,21
820,2023-04-06T21:47:36Z,2023-04-17T15:28:55Z,2023-04-17T15:28:56Z,10,753,41,"This change allows applying LoRA adapters on the fly without having to duplicate the model files.
Instructions:

Obtain the HF PEFT LoRA files adapter_config.json and adapter_model.bin of a LoRA adapter and put them in the same path.  For alpaca, this can be found at https://huggingface.co/tloen/alpaca-lora-7b/tree/main
Convert it using convert-lora-to-ggml.py to obtain ggml-adapter-model.bin

python convert-lora-to-ggml.py lora/alpaca-lora-7b


Use the ggml-adapter-model.bin with --lora

./main -m models/7B/ggml-model-f16.bin --lora lora/alpaca-lora-7b/ggml-adapter-model.bin --color -f ./prompts/alpaca.txt -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 7


When using a quantized model, the quality may suffer. To avoid this, specify a f16/f32 model with --lora-base to use as a base. The layers modified by LoRA adapter will be applied to the lora-base model and then quantized to the same format as the model specified with -m. Layers not modified by the LoRA adapter will remain untouched.

./main -m models/7B/ggml-model-q4_0.bin --lora lora/alpaca-lora-7b/ggml-adapter-model.bin --lora-base models/7B/ggml-model-f16.bin --color -f ./prompts/alpaca.txt -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 7

Limitations:

Using --lora disables mmap since the models have to be modified anyway.
When using --lora-base, a ggml_cpy operation is used to quantize the result, which currently is done in a single thread. Parallelizing ggml_cpy will improve loading times.",8,54
824,2023-04-07T05:05:32Z,,2023-04-10T20:03:29Z,1,41,17,"This is an attempt at #782.
Apologies if this is the wrong approach, learning the codebase!
Tested on Mac i7-1068NG7. Change in performance seems to be very small, if any:
Before:
========================================
perf_total_per_op_us[            NONE] =   0.000 ms
perf_total_per_op_us[             DUP] =   0.000 ms
perf_total_per_op_us[             ADD] =   0.346 ms
perf_total_per_op_us[             SUB] =   0.000 ms
perf_total_per_op_us[             MUL] =   0.492 ms
perf_total_per_op_us[             DIV] =   0.000 ms
perf_total_per_op_us[             SQR] =   0.000 ms
perf_total_per_op_us[            SQRT] =   0.000 ms
perf_total_per_op_us[             SUM] =   0.000 ms
perf_total_per_op_us[            MEAN] =   0.000 ms
perf_total_per_op_us[          REPEAT] =   0.000 ms
perf_total_per_op_us[             ABS] =   0.000 ms
perf_total_per_op_us[             SGN] =   0.000 ms
perf_total_per_op_us[             NEG] =   0.000 ms
perf_total_per_op_us[            STEP] =   0.000 ms
perf_total_per_op_us[            RELU] =   0.000 ms
perf_total_per_op_us[            GELU] =   0.000 ms
perf_total_per_op_us[            SILU] =   1.182 ms
perf_total_per_op_us[            NORM] =   0.000 ms
perf_total_per_op_us[        RMS_NORM] =   0.670 ms
perf_total_per_op_us[         MUL_MAT] = 249.842 ms
perf_total_per_op_us[           SCALE] =   0.097 ms
perf_total_per_op_us[             CPY] =   2.084 ms
perf_total_per_op_us[         RESHAPE] =   0.138 ms
perf_total_per_op_us[            VIEW] =   0.138 ms
perf_total_per_op_us[         PERMUTE] =   0.098 ms
perf_total_per_op_us[       TRANSPOSE] =   0.031 ms
perf_total_per_op_us[        GET_ROWS] =   0.003 ms
perf_total_per_op_us[   DIAG_MASK_INF] =   0.037 ms
perf_total_per_op_us[        SOFT_MAX] =   0.336 ms
perf_total_per_op_us[            ROPE] =   2.216 ms
perf_total_per_op_us[      CONV_1D_1S] =   0.000 ms
perf_total_per_op_us[      CONV_1D_2S] =   0.000 ms
perf_total_per_op_us[      FLASH_ATTN] =   0.000 ms
perf_total_per_op_us[        FLASH_FF] =   0.000 ms
========================================

llama_print_timings:        load time =  2801.11 ms
llama_print_timings:      sample time =   162.00 ms /   128 runs   (    1.27 ms per run)
llama_print_timings: prompt eval time =  1553.28 ms /     7 tokens (  221.90 ms per token)
llama_print_timings:        eval time = 26238.64 ms /   127 runs   (  206.60 ms per run)
llama_print_timings:       total time = 29204.85 ms

After:
perf_total_per_op_us[            NONE] =   0.000 ms
perf_total_per_op_us[             DUP] =   0.000 ms
perf_total_per_op_us[             ADD] =   0.412 ms
perf_total_per_op_us[             SUB] =   0.000 ms
perf_total_per_op_us[             MUL] =   0.477 ms
perf_total_per_op_us[             DIV] =   0.000 ms
perf_total_per_op_us[             SQR] =   0.000 ms
perf_total_per_op_us[            SQRT] =   0.000 ms
perf_total_per_op_us[             SUM] =   0.000 ms
perf_total_per_op_us[            MEAN] =   0.000 ms
perf_total_per_op_us[          REPEAT] =   0.000 ms
perf_total_per_op_us[             ABS] =   0.000 ms
perf_total_per_op_us[             SGN] =   0.000 ms
perf_total_per_op_us[             NEG] =   0.000 ms
perf_total_per_op_us[            STEP] =   0.000 ms
perf_total_per_op_us[            RELU] =   0.000 ms
perf_total_per_op_us[            GELU] =   0.000 ms
perf_total_per_op_us[            SILU] =   1.307 ms
perf_total_per_op_us[            NORM] =   0.000 ms
perf_total_per_op_us[        RMS_NORM] =   0.658 ms
perf_total_per_op_us[         MUL_MAT] = 284.981 ms
perf_total_per_op_us[           SCALE] =   0.097 ms
perf_total_per_op_us[             CPY] =   1.749 ms
perf_total_per_op_us[         RESHAPE] =   0.142 ms
perf_total_per_op_us[            VIEW] =   0.149 ms
perf_total_per_op_us[         PERMUTE] =   0.108 ms
perf_total_per_op_us[       TRANSPOSE] =   0.034 ms
perf_total_per_op_us[        GET_ROWS] =   0.898 ms
perf_total_per_op_us[   DIAG_MASK_INF] =   0.042 ms
perf_total_per_op_us[        SOFT_MAX] =   0.528 ms
perf_total_per_op_us[            ROPE] =   1.741 ms
perf_total_per_op_us[      CONV_1D_1S] =   0.000 ms
perf_total_per_op_us[      CONV_1D_2S] =   0.000 ms
perf_total_per_op_us[      FLASH_ATTN] =   0.000 ms
perf_total_per_op_us[        FLASH_FF] =   0.000 ms

llama_print_timings:        load time =  2540.09 ms
llama_print_timings:      sample time =   154.58 ms /   128 runs   (    1.21 ms per run)
llama_print_timings: prompt eval time =  1481.09 ms /     7 tokens (  211.58 ms per token)
llama_print_timings:        eval time = 24397.64 ms /   127 runs   (  192.11 ms per run)
llama_print_timings:       total time = 27095.16 ms

(The GGML_PERF outputs and the print timings outputs are separate runs, all are the 2nd invocation after building, using the same command)
Additionally, I'm having a bit of trouble testing the f16 codepath for this - I'm sure I'm doing something silly, but even when running with an f16 model, I'm not seeing src0->type ever equal GGML_TYPE_F16. Any tips for building?
Thanks!",7,10
838,2023-04-07T16:21:20Z,2023-04-13T13:24:30Z,2023-04-13T13:24:30Z,1,3,3,"Hey, I was trying to understand how blas was used in llama.cpp and maybe i found a bug?
at least i dont understand why - the int after the x in cblas_sgemm calls should be the pitch(or stride) of array x.
but x comes from src0 in all the ggml_compute_forward_mul_mat_*, so i am guessing the stride of src0 should be used, which is ne00?
sorry if i am missing something : (",3,2
839,2023-04-07T20:25:24Z,2023-04-08T11:15:17Z,2023-04-08T11:15:17Z,1,1,0,"Currently if you build with cmake and LLAMA_OPENBLAS=ON it does not link the OpenBlas library properly and the build fails. This add OpenBlas linking to cmake like how it's done in the Makefile LDFLAGS (below).
ifdef LLAMA_OPENBLAS
    CFLAGS  += -DGGML_USE_OPENBLAS -I/usr/local/include/openblas
    LDFLAGS += -lopenblas",1,1
840,2023-04-07T21:50:09Z,2023-04-08T15:49:39Z,2023-04-08T15:49:39Z,3,28,3,"When trying to input UTF-8 characters into Windows console, multibyte characters are interpreted as 0 (zeros) and text is cut off, because internal implementation relies on char *.
To fix this issue, console input must be set to wide characters (UTF-16) and then converted into UTF-8 string.",3,2
848,2023-04-08T10:49:58Z,2023-04-13T13:49:05Z,2023-04-13T13:49:05Z,1,2,4,followup to #847,3,1
863,2023-04-09T04:23:35Z,2023-04-14T15:19:18Z,2023-04-14T15:19:18Z,12,434,110,"Added support for --config (loading arguments from .txt file).
Added support for multiline mode, disabled by --disable-multiline (allows to input newlines - mode is toggled by sending EOF Ctrl+D on Linux/Mac, Ctrl+Z and Return on Windows)
Marked -ins mode as deprecated. Now it could be replaced with user defined --ins-prefix, --ins-suffix.
Added --stop-prompt alternative to --reverse-prompt - works same, but without adding instruct prefixes and suffixes (useful, when you're talking to agent and you want to force its response to something)
Added flexibility to --reverse-prompt and --stop-prompt, to stop, even when trailing space character is missing (because tokens usually starts with space character, and stop/reverse prompt won't trigger otherwise), enabled with --rm-trailing-space-workaround


There are some code duplicities, some todos and It's overall a bit messy.
But it should be working and could be merged, if you don't mind.",12,17
872,2023-04-10T09:55:21Z,2023-04-13T13:43:23Z,2023-04-13T13:43:23Z,2,44,28,"build.zig now works using Zig 0.10.1, which can be used as an option for Windows.
Update README.md accordingly.",3,0
874,2023-04-10T13:14:19Z,2023-04-14T14:43:55Z,2023-04-14T14:43:55Z,2,237,2,"Proof of concept support for mapping arbitrary unary/binary operations with GGML.
This would require support models that require operations that aren't currently implemented, even if it was in a basic/less than optimally performant way.
I was a C developer in a past life, but it's been a long time. This does appear to work (using map functions from Rust even) but it may not be safe.
While this most likely isn't in a state to be merged, is it something that could in theory get included? I'm willing to work with the powers that be. (Also, wasn't sure exactly where to submit this. It seems like the main ggml is significantly different from the llama.cpp one.)",4,10
877,2023-04-10T17:55:31Z,2023-04-10T20:41:53Z,2023-04-10T20:41:53Z,1,1,0,"I don't know if it's ok to say that Koala is supported, as in ""officially supported"", but it's certainly working great.",2,0
883,2023-04-10T20:13:42Z,2023-04-11T19:45:45Z,2023-04-11T19:45:45Z,15,84,46,Followup to 9d634ef,4,8
884,2023-04-10T20:54:51Z,2023-04-13T14:08:32Z,2023-04-13T14:08:32Z,1,10,2,Fixes #880,3,2
888,2023-04-11T00:13:51Z,2023-04-21T18:48:06Z,2023-04-21T18:48:06Z,1,15,2,"Currently, llama_mlock::raw_lock() tells you to raise rlimit -l no matter what actually caused mlock() to fail. This change verifies that is indeed the issue before printing a potentially misleading error message.",2,1
890,2023-04-11T02:48:18Z,2023-04-11T13:19:54Z,2023-04-11T13:19:54Z,4,22,4,"Mostly for msys2 and mingw64 builds, which are different from each other and different from standard Visual Studio builds.  Isn't Windows fun?


Define _GNU_SOURCE in more files (it's already used in ggml.c for Linux's sake).


Don't use PrefetchVirtualMemory if not building for Windows 8 or later (mingw64 doesn't by default).  But warn the user about this situation since it's probably not intended.


Check for NOMINMAX already being defined, which it is on mingw64.


Actually use the increment variable (bug in my pizza PR).


Suppress unused variable warnings in the fake pthread_create and pthread_join implementations for Windows.


(not Windows-related) Remove mention of asprintf from comment; asprintf is no longer used.


Fixes #871.",3,2
902,2023-04-11T23:27:28Z,2023-04-12T06:46:20Z,2023-04-12T06:46:20Z,1,1,1,This is to emphasize that these do not need to be obtained from elsewhere.,3,2
908,2023-04-12T06:34:30Z,2023-04-13T09:33:16Z,2023-04-13T09:33:16Z,3,5,2,,3,6
911,2023-04-12T08:28:00Z,2023-04-13T13:54:28Z,2023-04-13T13:54:28Z,1,1,0,,2,0
922,2023-04-12T18:50:30Z,,2024-02-19T14:38:44Z,1,10,0,"Regarding #918 (comment) , and
#918 (comment) and #881, here is another way: use VLAs instead of alloca, when possible.
It uses __STDC_NO_VLA__ to detect if VLAs are unavailable, in which case it falls back to alloca. In that case the program is still not a strictly conforming C program, and the -std=c11 argument to gcc may evoke warnings, but on the other hand, gcc is quite likely to support VLAs in practice.",3,1
924,2023-04-12T21:08:25Z,,2023-04-14T17:46:06Z,2,76,16,"I thought of this while working on something else.
Its super useful. You can change startup params on the fly using the command:
??? CMD param
where CMD is a command line argument, and params is the number following each argument.
In my testing so far changing values has changed the models output.
This should enable users to test different params in run time against a single prompt. We can now fine tune the params using a single prompt. Simply copy and paste the prompt over and over after changing the param and compare the output.",3,3
927,2023-04-12T23:37:24Z,2023-04-17T14:41:53Z,2023-04-17T14:41:53Z,2,56,36,"Addresses issue #920
Replaced static initialization of complex objects with a initialization on first use. This prevents an undefined behavior on program run, for example, crash in Release build, works in Debug build",3,8
929,2023-04-13T01:31:36Z,2023-04-13T13:59:53Z,2023-04-13T13:59:54Z,1,0,2,"This is unneeded as sentencepiece is now working with the latest version of python 3.11.
See google/sentencepiece/issues/810#issuecomment-1504980354",3,0
933,2023-04-13T03:10:33Z,2023-04-17T13:10:58Z,2023-04-17T13:10:58Z,4,238,47,"Apologies for the slightly clickbaity title: while technically true, as mentioned in this comment, the current AVX-512 implementation is slower than the regular AVX2 implementation. Compared to the current AVX2 implementation, the new AVX-512 implementation only gets a ≈35% speedup on modern hardware.
The measurements below were made on a laptop with a Tiger Lake CPU (i7-1165G7). 8 threads were used.
LLaMA 7B (./main -m ./models/7B/ggml-model-q4_0.bin -p ""Building a website can be done in 10 simple steps:""  -n 256)



Implementation
Time per token, ms (average over 3 runs)




AVX512, old
184


AVX2
149


AVX512, new
110



LLaMA 13B (./main -m ./models/13B/ggml-model-q4_0.bin -p ""Building a website can be done in 10 simple steps:"" -n 256)



Implementation
Time per token, ms (average over 3 runs)




AVX512, old
354


AVX2
282


AVX512, new
205



For clarity, I only include the time of generation per token, but the prompt processing improvements are very similar.
These speedup percentages are, of course, an oversimplification:

I report end-to-end token generation timings here. While regular BLAS-less inference is bottlenecked on ggml_vec_dot_q4_0(), it also does quite a lot of other stuff.
This is gcc 12.2.1, the results with other compilers can vary (but not too much). clang, in particular, does some ""optimizations"" than only hurt performance (fortunately, not in a major way).
AVX-512 is a fragmented ecosystem of extensions. My code makes use of VBMI and VNNI extensions, when available. If they are not present, the code runs slightly slower.

However, my microbenchmark (./build.sh && ./main on a Linux system with libbenchmark and gcc installed) suggests solid improvements over both the AVX2 and old AVX-512 implementations across a variety of CPUs. Unless I screwed up something major, the improvements in the microbenchmark should directly lead to improvements in generation. I would appreciate any performance measurements with this PR applied.
Implementation-wise, the basic idea is that built-in masking and register-wide shuffles in AVX-512 allow us to operate on two Q4_0 blocks at once. I tried to comment the code extensively.",6,13
934,2023-04-13T05:01:35Z,2023-04-30T18:41:35Z,2023-04-30T18:41:35Z,2,42,12,"Fixes: #932
Hyperthreading is bad, probably because we are compute bound (not memory bound).
See also: #34
Notes: I consulted GPT4 in the making of this PR.",9,32
951,2023-04-13T20:09:03Z,2023-04-15T14:53:23Z,2023-04-15T14:53:23Z,3,442,18,"ref #909
This is an implementation of mode (E) from the referenced issue.
Basically, we quantize the intermediate results to 8-bits, instead of 4-bits to gain accuracy without any performance degradation.
As a positive side-effect, we will also have full 8-bit quantization support, although I don't think it will be significantly better than the proposed 4-bit quantization with 8-bit intermediate results.
Currently:

 Reference
 ARM NEON
 AVX
 WASM

PRs are welcome into this PR to implement the missing SIMD routines
Perplexity results

  Q4_0 M1 Pro (with BLAS) [655]6.2838 (i.e. reference)
$  make clean && make -j perplexity && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw -t 8
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-q4_0-matmult
common.o
ggml.o
llama.o
main
quantize
perplexity
embedding
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c llama.cpp -o llama.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c examples/common.cpp -o common.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity  -framework Accelerate
main: seed = 1681463663
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks, batch_size=512
10.60 seconds per pass - ETA 1.93 hours
[1]4.3802,[2]4.9555,[3]5.8269,[4]6.4692,[5]6.5435,[6]6.5411,[7]6.7174,[8]6.8069,[9]7.1756,[10]7.4121,[11]7.6567,[12]7.6957,[13]7.6058,[14]7.6820,[15]7.9366,[16]7.5419,[17]7.4189,[18]7.3798,[19]7.0077,[20]6.9948,[21]6.8969,[22]6.7125,[23]6.6744,[24]6.5868,[25]6.5871,[26]6.4149,[27]6.2349,[28]6.1341,[29]6.0498,[30]5.8938,[31]5.8659,[32]5.8839,[33]5.8189,[34]5.8537,[35]5.8795,[36]5.9232,[37]5.9272,[38]5.9443,[39]5.9825,[40]6.0412,[41]6.0482,[42]6.0826,[43]6.0397,[44]6.0944,[45]6.0989,[46]6.0729,[47]6.0967,[48]6.0674,[49]6.0745,[50]6.0351,[51]6.0309,[52]6.0200,[53]6.0641,[54]6.0476,[55]6.0250,[56]6.0593,[57]6.0824,[58]6.1043,[59]6.1182,[60]6.1647,[61]6.1536,[62]6.2166,[63]6.2502,[64]6.2653,[65]6.3119,[66]6.3220,[67]6.3401,[68]6.3541,[69]6.3790,[70]6.4113,[71]6.4327,[72]6.4625,[73]6.5276,[74]6.5330,[75]6.5474,[76]6.5637,[77]6.5770,[78]6.5618,[79]6.5914,[80]6.5839,[81]6.5967,[82]6.6005,[83]6.5468,[84]6.5322,[85]6.5208,[86]6.4997,[87]6.4344,[88]6.4059,[89]6.3853,[90]6.3687,[91]6.3948,[92]6.3909,[93]6.3935,[94]6.3910,[95]6.4198,[96]6.4177,[97]6.4105,[98]6.4035,[99]6.3895,[100]6.3895,[101]6.4154,[102]6.4091,[103]6.4308,[104]6.4376,[105]6.4361,[106]6.4538,[107]6.4525,[108]6.4648,[109]6.4595,[110]6.4550,[111]6.4779,[112]6.4969,[113]6.4983,[114]6.4949,[115]6.5032,[116]6.4958,[117]6.5014,[118]6.5298,[119]6.5507,[120]6.5872,[121]6.6035,[122]6.6282,[123]6.6672,[124]6.6850,[125]6.6762,[126]6.7153,[127]6.7524,[128]6.7798,[129]6.7629,[130]6.7725,[131]6.7672,[132]6.7584,[133]6.7456,[134]6.7568,[135]6.7534,[136]6.7402,[137]6.7322,[138]6.7151,[139]6.7035,[140]6.7005,[141]6.6707,[142]6.6658,[143]6.6379,[144]6.6178,[145]6.6092,[146]6.5957,[147]6.6031,[148]6.6054,[149]6.5994,[150]6.5953,[151]6.5965,[152]6.5870,[153]6.5703,[154]6.5613,[155]6.5680,[156]6.5630,[157]6.5813,[158]6.5849,[159]6.5890,[160]6.5916,[161]6.6041,[162]6.5739,[163]6.5619,[164]6.5357,[165]6.5039,[166]6.4751,[167]6.4377,[168]6.4051,[169]6.3916,[170]6.3791,[171]6.3502,[172]6.3322,[173]6.3136,[174]6.2829,[175]6.2607,[176]6.2505,[177]6.2295,[178]6.2059,[179]6.1887,[180]6.1798,[181]6.1574,[182]6.1382,[183]6.1239,[184]6.1238,[185]6.1165,[186]6.1182,[187]6.1236,[188]6.1200,[189]6.1384,[190]6.1393,[191]6.1597,[192]6.1760,[193]6.1938,[194]6.2054,[195]6.2263,[196]6.2434,[197]6.2655,[198]6.2810,[199]6.2840,[200]6.2885,[201]6.2844,[202]6.3049,[203]6.3115,[204]6.3114,[205]6.3224,[206]6.3302,[207]6.3262,[208]6.3346,[209]6.3398,[210]6.3449,[211]6.3547,[212]6.3620,[213]6.3727,[214]6.3762,[215]6.3802,[216]6.3951,[217]6.4129,[218]6.4264,[219]6.4267,[220]6.4231,[221]6.4168,[222]6.4133,[223]6.4024,[224]6.3958,[225]6.3910,[226]6.4125,[227]6.4212,[228]6.4271,[229]6.4338,[230]6.4294,[231]6.4462,[232]6.4332,[233]6.4160,[234]6.4004,[235]6.3845,[236]6.3768,[237]6.3664,[238]6.3698,[239]6.3536,[240]6.3433,[241]6.3466,[242]6.3503,[243]6.3488,[244]6.3368,[245]6.3342,[246]6.3221,[247]6.3097,[248]6.3030,[249]6.3010,[250]6.3057,[251]6.2980,[252]6.2946,[253]6.2844,[254]6.2804,[255]6.2688,[256]6.2496,[257]6.2385,[258]6.2299,[259]6.2279,[260]6.2197,[261]6.2154,[262]6.2095,[263]6.2050,[264]6.1858,[265]6.1850,[266]6.1835,[267]6.1766,[268]6.1862,[269]6.1843,[270]6.1850,[271]6.1928,[272]6.1974,[273]6.1969,[274]6.1983,[275]6.2073,[276]6.2128,[277]6.2288,[278]6.2397,[279]6.2483,[280]6.2518,[281]6.2617,[282]6.2678,[283]6.2825,[284]6.2902,[285]6.2997,[286]6.3144,[287]6.3138,[288]6.3198,[289]6.3107,[290]6.2956,[291]6.2802,[292]6.2644,[293]6.2505,[294]6.2530,[295]6.2524,[296]6.2567,[297]6.2553,[298]6.2579,[299]6.2551,[300]6.2439,[301]6.2440,[302]6.2359,[303]6.2282,[304]6.2204,[305]6.2180,[306]6.2047,[307]6.2072,[308]6.2104,[309]6.1941,[310]6.1880,[311]6.1816,[312]6.1838,[313]6.1782,[314]6.1769,[315]6.1604,[316]6.1562,[317]6.1395,[318]6.1179,[319]6.1298,[320]6.1428,[321]6.1466,[322]6.1421,[323]6.1355,[324]6.1331,[325]6.1431,[326]6.1430,[327]6.1451,[328]6.1494,[329]6.1554,[330]6.1579,[331]6.1703,[332]6.1671,[333]6.1741,[334]6.1682,[335]6.1617,[336]6.1655,[337]6.1625,[338]6.1612,[339]6.1555,[340]6.1511,[341]6.1589,[342]6.1613,[343]6.1669,[344]6.1668,[345]6.1667,[346]6.1638,[347]6.1686,[348]6.1727,[349]6.1746,[350]6.1712,[351]6.1717,[352]6.1717,[353]6.1665,[354]6.1664,[355]6.1718,[356]6.1749,[357]6.1712,[358]6.1802,[359]6.1833,[360]6.1795,[361]6.1791,[362]6.1858,[363]6.1970,[364]6.2035,[365]6.2093,[366]6.2100,[367]6.2188,[368]6.2165,[369]6.2175,[370]6.2185,[371]6.2125,[372]6.2178,[373]6.2234,[374]6.2220,[375]6.2217,[376]6.2301,[377]6.2252,[378]6.2277,[379]6.2338,[380]6.2254,[381]6.2211,[382]6.2154,[383]6.2144,[384]6.2137,[385]6.2124,[386]6.2119,[387]6.2111,[388]6.2066,[389]6.2012,[390]6.1943,[391]6.1862,[392]6.1821,[393]6.1802,[394]6.1828,[395]6.1812,[396]6.1738,[397]6.1814,[398]6.1852,[399]6.1935,[400]6.1931,[401]6.1944,[402]6.1950,[403]6.1969,[404]6.2032,[405]6.1937,[406]6.1903,[407]6.1895,[408]6.1905,[409]6.2029,[410]6.2139,[411]6.2264,[412]6.2427,[413]6.2542,[414]6.2618,[415]6.2670,[416]6.2750,[417]6.2881,[418]6.2916,[419]6.2990,[420]6.3076,[421]6.3197,[422]6.3255,[423]6.3326,[424]6.3446,[425]6.3537,[426]6.3602,[427]6.3647,[428]6.3730,[429]6.3775,[430]6.3865,[431]6.4011,[432]6.4054,[433]6.4041,[434]6.3995,[435]6.4002,[436]6.4026,[437]6.4121,[438]6.4200,[439]6.4164,[440]6.4158,[441]6.4108,[442]6.4099,[443]6.4112,[444]6.4115,[445]6.4095,[446]6.4118,[447]6.4147,[448]6.4190,[449]6.4164,[450]6.4167,[451]6.4124,[452]6.4005,[453]6.3922,[454]6.3862,[455]6.3869,[456]6.3917,[457]6.3934,[458]6.3912,[459]6.3922,[460]6.4009,[461]6.3981,[462]6.3965,[463]6.4015,[464]6.4006,[465]6.3976,[466]6.3895,[467]6.3898,[468]6.3897,[469]6.3919,[470]6.3924,[471]6.3876,[472]6.3922,[473]6.3866,[474]6.3880,[475]6.3821,[476]6.3844,[477]6.3773,[478]6.3764,[479]6.3827,[480]6.3879,[481]6.3899,[482]6.3854,[483]6.3813,[484]6.3835,[485]6.3818,[486]6.3763,[487]6.3763,[488]6.3744,[489]6.3694,[490]6.3667,[491]6.3637,[492]6.3579,[493]6.3548,[494]6.3530,[495]6.3528,[496]6.3493,[497]6.3440,[498]6.3422,[499]6.3372,[500]6.3275,[501]6.3206,[502]6.3204,[503]6.3202,[504]6.3109,[505]6.3133,[506]6.3142,[507]6.3081,[508]6.3038,[509]6.3027,[510]6.3066,[511]6.3113,[512]6.3148,[513]6.3166,[514]6.3232,[515]6.3177,[516]6.3169,[517]6.3180,[518]6.3181,[519]6.3211,[520]6.3238,[521]6.3255,[522]6.3283,[523]6.3294,[524]6.3357,[525]6.3393,[526]6.3405,[527]6.3426,[528]6.3372,[529]6.3376,[530]6.3329,[531]6.3319,[532]6.3367,[533]6.3390,[534]6.3372,[535]6.3395,[536]6.3341,[537]6.3318,[538]6.3366,[539]6.3378,[540]6.3417,[541]6.3426,[542]6.3433,[543]6.3447,[544]6.3459,[545]6.3437,[546]6.3443,[547]6.3398,[548]6.3343,[549]6.3345,[550]6.3318,[551]6.3280,[552]6.3260,[553]6.3217,[554]6.3195,[555]6.3166,[556]6.3163,[557]6.3186,[558]6.3146,[559]6.3142,[560]6.3137,[561]6.3139,[562]6.3120,[563]6.3120,[564]6.3163,[565]6.3180,[566]6.3177,[567]6.3155,[568]6.3160,[569]6.3144,[570]6.3170,[571]6.3176,[572]6.3186,[573]6.3188,[574]6.3151,[575]6.3147,[576]6.3145,[577]6.3135,[578]6.3114,[579]6.3122,[580]6.3056,[581]6.3018,[582]6.3008,[583]6.3016,[584]6.3020,[585]6.2943,[586]6.2875,[587]6.2877,[588]6.2927,[589]6.2985,[590]6.3015,[591]6.3037,[592]6.3022,[593]6.2985,[594]6.2996,[595]6.2973,[596]6.3010,[597]6.2987,[598]6.2949,[599]6.2971,[600]6.2969,[601]6.2954,[602]6.2972,[603]6.3001,[604]6.3012,[605]6.3044,[606]6.3065,[607]6.3048,[608]6.3013,[609]6.3019,[610]6.3056,[611]6.3037,[612]6.3062,[613]6.3026,[614]6.2975,[615]6.2898,[616]6.2928,[617]6.2865,[618]6.2814,[619]6.2757,[620]6.2615,[621]6.2542,[622]6.2525,[623]6.2540,[624]6.2545,[625]6.2544,[626]6.2529,[627]6.2550,[628]6.2555,[629]6.2552,[630]6.2586,[631]6.2650,[632]6.2704,[633]6.2687,[634]6.2721,[635]6.2726,[636]6.2694,[637]6.2659,[638]6.2686,[639]6.2657,[640]6.2667,[641]6.2669,[642]6.2738,[643]6.2760,[644]6.2772,[645]6.2751,[646]6.2793,[647]6.2755,[648]6.2762,[649]6.2763,[650]6.2801,[651]6.2858,[652]6.2865,[653]6.2908,[654]6.2844,[655]6.2838,

llama_print_timings:        load time = 11216.03 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 4989892.61 ms / 335360 tokens (   14.88 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 5024616.43 ms

real	83m45.024s
user	126m54.284s
sys	4m10.884s



  Q4_0 M1 Pro (without BLAS) [655]6.2897 (impl 1))
make clean && LLAMA_NO_ACCELERATE=1 make -j perplexity && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw -t 8
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-q4_0-matmult
common.o
ggml.o
llama.o
perplexity
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:  
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread   -c ggml.c -o ggml.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c llama.cpp -o llama.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c examples/common.cpp -o common.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity 
main: seed = 1681424328
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks
21.53 seconds per pass - ETA 3.92 hours
[1]4.3858,[2]4.9662,[3]5.8382,[4]6.4833,[5]6.5555,[6]6.5501,[7]6.7244,[8]6.8139,[9]7.1831,[10]7.4180,[11]7.6620,[12]7.7028,[13]7.6127,[14]7.6873,[15]7.9419,[16]7.5479,[17]7.4241,[18]7.3855,[19]7.0131,[20]7.0003,[21]6.9022,[22]6.7175,[23]6.6793,[24]6.5912,[25]6.5917,[26]6.4198,[27]6.2395,[28]6.1386,[29]6.0541,[30]5.8977,[31]5.8703,[32]5.8883,[33]5.8232,[34]5.8586,[35]5.8843,[36]5.9276,[37]5.9310,[38]5.9481,[39]5.9864,[40]6.0454,[41]6.0524,[42]6.0870,[43]6.0440,[44]6.0991,[45]6.1037,[46]6.0781,[47]6.1018,[48]6.0726,[49]6.0798,[50]6.0408,[51]6.0364,[52]6.0252,[53]6.0690,[54]6.0526,[55]6.0301,[56]6.0646,[57]6.0877,[58]6.1096,[59]6.1235,[60]6.1705,[61]6.1590,[62]6.2224,[63]6.2561,[64]6.2711,[65]6.3178,[66]6.3278,[67]6.3458,[68]6.3598,[69]6.3848,[70]6.4170,[71]6.4384,[72]6.4684,[73]6.5335,[74]6.5388,[75]6.5536,[76]6.5697,[77]6.5828,[78]6.5677,[79]6.5973,[80]6.5898,[81]6.6026,[82]6.6063,[83]6.5526,[84]6.5381,[85]6.5266,[86]6.5055,[87]6.4398,[88]6.4112,[89]6.3906,[90]6.3741,[91]6.4001,[92]6.3963,[93]6.3990,[94]6.3964,[95]6.4252,[96]6.4229,[97]6.4157,[98]6.4089,[99]6.3949,[100]6.3949,[101]6.4209,[102]6.4145,[103]6.4366,[104]6.4436,[105]6.4421,[106]6.4597,[107]6.4582,[108]6.4706,[109]6.4653,[110]6.4607,[111]6.4837,[112]6.5027,[113]6.5043,[114]6.5012,[115]6.5094,[116]6.5021,[117]6.5076,[118]6.5361,[119]6.5569,[120]6.5934,[121]6.6098,[122]6.6346,[123]6.6736,[124]6.6913,[125]6.6824,[126]6.7216,[127]6.7587,[128]6.7862,[129]6.7693,[130]6.7789,[131]6.7735,[132]6.7647,[133]6.7519,[134]6.7631,[135]6.7596,[136]6.7465,[137]6.7384,[138]6.7213,[139]6.7096,[140]6.7065,[141]6.6768,[142]6.6719,[143]6.6440,[144]6.6239,[145]6.6154,[146]6.6019,[147]6.6096,[148]6.6118,[149]6.6056,[150]6.6015,[151]6.6026,[152]6.5932,[153]6.5765,[154]6.5675,[155]6.5742,[156]6.5692,[157]6.5876,[158]6.5912,[159]6.5953,[160]6.5979,[161]6.6103,[162]6.5801,[163]6.5680,[164]6.5418,[165]6.5099,[166]6.4812,[167]6.4437,[168]6.4110,[169]6.3975,[170]6.3851,[171]6.3561,[172]6.3380,[173]6.3193,[174]6.2885,[175]6.2663,[176]6.2561,[177]6.2350,[178]6.2114,[179]6.1942,[180]6.1853,[181]6.1628,[182]6.1436,[183]6.1293,[184]6.1291,[185]6.1219,[186]6.1236,[187]6.1290,[188]6.1253,[189]6.1438,[190]6.1447,[191]6.1652,[192]6.1816,[193]6.1993,[194]6.2109,[195]6.2318,[196]6.2488,[197]6.2708,[198]6.2864,[199]6.2893,[200]6.2938,[201]6.2896,[202]6.3103,[203]6.3169,[204]6.3168,[205]6.3278,[206]6.3357,[207]6.3317,[208]6.3401,[209]6.3453,[210]6.3504,[211]6.3604,[212]6.3678,[213]6.3785,[214]6.3821,[215]6.3863,[216]6.4012,[217]6.4190,[218]6.4325,[219]6.4328,[220]6.4292,[221]6.4228,[222]6.4192,[223]6.4083,[224]6.4016,[225]6.3968,[226]6.4183,[227]6.4270,[228]6.4329,[229]6.4397,[230]6.4352,[231]6.4521,[232]6.4390,[233]6.4218,[234]6.4061,[235]6.3904,[236]6.3826,[237]6.3721,[238]6.3755,[239]6.3593,[240]6.3489,[241]6.3522,[242]6.3559,[243]6.3544,[244]6.3425,[245]6.3397,[246]6.3276,[247]6.3152,[248]6.3085,[249]6.3065,[250]6.3112,[251]6.3035,[252]6.3001,[253]6.2898,[254]6.2857,[255]6.2741,[256]6.2550,[257]6.2438,[258]6.2352,[259]6.2332,[260]6.2250,[261]6.2207,[262]6.2147,[263]6.2103,[264]6.1913,[265]6.1904,[266]6.1890,[267]6.1820,[268]6.1917,[269]6.1897,[270]6.1904,[271]6.1982,[272]6.2029,[273]6.2023,[274]6.2038,[275]6.2128,[276]6.2183,[277]6.2343,[278]6.2452,[279]6.2538,[280]6.2574,[281]6.2672,[282]6.2733,[283]6.2880,[284]6.2958,[285]6.3053,[286]6.3200,[287]6.3194,[288]6.3255,[289]6.3163,[290]6.3012,[291]6.2857,[292]6.2699,[293]6.2561,[294]6.2586,[295]6.2580,[296]6.2624,[297]6.2610,[298]6.2636,[299]6.2608,[300]6.2495,[301]6.2497,[302]6.2416,[303]6.2339,[304]6.2261,[305]6.2236,[306]6.2104,[307]6.2128,[308]6.2161,[309]6.1997,[310]6.1936,[311]6.1873,[312]6.1895,[313]6.1839,[314]6.1826,[315]6.1661,[316]6.1618,[317]6.1451,[318]6.1234,[319]6.1353,[320]6.1483,[321]6.1521,[322]6.1476,[323]6.1410,[324]6.1386,[325]6.1485,[326]6.1485,[327]6.1506,[328]6.1549,[329]6.1609,[330]6.1634,[331]6.1758,[332]6.1726,[333]6.1796,[334]6.1737,[335]6.1672,[336]6.1710,[337]6.1680,[338]6.1667,[339]6.1609,[340]6.1565,[341]6.1643,[342]6.1668,[343]6.1723,[344]6.1722,[345]6.1721,[346]6.1692,[347]6.1740,[348]6.1782,[349]6.1800,[350]6.1766,[351]6.1771,[352]6.1771,[353]6.1720,[354]6.1718,[355]6.1773,[356]6.1803,[357]6.1767,[358]6.1857,[359]6.1888,[360]6.1850,[361]6.1846,[362]6.1912,[363]6.2024,[364]6.2089,[365]6.2148,[366]6.2155,[367]6.2242,[368]6.2220,[369]6.2229,[370]6.2239,[371]6.2179,[372]6.2231,[373]6.2287,[374]6.2274,[375]6.2270,[376]6.2354,[377]6.2305,[378]6.2330,[379]6.2391,[380]6.2307,[381]6.2264,[382]6.2206,[383]6.2196,[384]6.2189,[385]6.2176,[386]6.2172,[387]6.2163,[388]6.2118,[389]6.2064,[390]6.1995,[391]6.1914,[392]6.1873,[393]6.1854,[394]6.1880,[395]6.1864,[396]6.1790,[397]6.1866,[398]6.1904,[399]6.1987,[400]6.1983,[401]6.1996,[402]6.2002,[403]6.2021,[404]6.2084,[405]6.1989,[406]6.1955,[407]6.1947,[408]6.1957,[409]6.2081,[410]6.2191,[411]6.2316,[412]6.2479,[413]6.2594,[414]6.2670,[415]6.2722,[416]6.2803,[417]6.2934,[418]6.2969,[419]6.3043,[420]6.3130,[421]6.3251,[422]6.3308,[423]6.3380,[424]6.3500,[425]6.3592,[426]6.3657,[427]6.3701,[428]6.3785,[429]6.3829,[430]6.3920,[431]6.4066,[432]6.4109,[433]6.4097,[434]6.4050,[435]6.4057,[436]6.4081,[437]6.4176,[438]6.4255,[439]6.4220,[440]6.4214,[441]6.4163,[442]6.4154,[443]6.4167,[444]6.4170,[445]6.4150,[446]6.4174,[447]6.4203,[448]6.4247,[449]6.4220,[450]6.4223,[451]6.4180,[452]6.4062,[453]6.3979,[454]6.3918,[455]6.3925,[456]6.3972,[457]6.3990,[458]6.3968,[459]6.3978,[460]6.4064,[461]6.4036,[462]6.4020,[463]6.4071,[464]6.4062,[465]6.4032,[466]6.3951,[467]6.3954,[468]6.3952,[469]6.3974,[470]6.3980,[471]6.3932,[472]6.3978,[473]6.3921,[474]6.3935,[475]6.3876,[476]6.3900,[477]6.3828,[478]6.3818,[479]6.3882,[480]6.3934,[481]6.3955,[482]6.3909,[483]6.3868,[484]6.3890,[485]6.3873,[486]6.3817,[487]6.3818,[488]6.3799,[489]6.3749,[490]6.3722,[491]6.3692,[492]6.3634,[493]6.3603,[494]6.3585,[495]6.3583,[496]6.3548,[497]6.3495,[498]6.3477,[499]6.3427,[500]6.3330,[501]6.3260,[502]6.3259,[503]6.3257,[504]6.3163,[505]6.3188,[506]6.3198,[507]6.3137,[508]6.3094,[509]6.3083,[510]6.3122,[511]6.3168,[512]6.3204,[513]6.3222,[514]6.3288,[515]6.3232,[516]6.3225,[517]6.3235,[518]6.3236,[519]6.3267,[520]6.3294,[521]6.3311,[522]6.3339,[523]6.3350,[524]6.3412,[525]6.3449,[526]6.3461,[527]6.3481,[528]6.3428,[529]6.3432,[530]6.3385,[531]6.3375,[532]6.3423,[533]6.3446,[534]6.3428,[535]6.3452,[536]6.3398,[537]6.3374,[538]6.3422,[539]6.3434,[540]6.3474,[541]6.3483,[542]6.3490,[543]6.3504,[544]6.3516,[545]6.3494,[546]6.3501,[547]6.3456,[548]6.3401,[549]6.3403,[550]6.3375,[551]6.3338,[552]6.3317,[553]6.3275,[554]6.3253,[555]6.3224,[556]6.3221,[557]6.3244,[558]6.3204,[559]6.3200,[560]6.3195,[561]6.3196,[562]6.3178,[563]6.3178,[564]6.3221,[565]6.3239,[566]6.3235,[567]6.3213,[568]6.3218,[569]6.3201,[570]6.3228,[571]6.3234,[572]6.3244,[573]6.3245,[574]6.3209,[575]6.3204,[576]6.3203,[577]6.3193,[578]6.3172,[579]6.3180,[580]6.3114,[581]6.3076,[582]6.3066,[583]6.3074,[584]6.3077,[585]6.3001,[586]6.2933,[587]6.2935,[588]6.2985,[589]6.3043,[590]6.3074,[591]6.3095,[592]6.3081,[593]6.3044,[594]6.3054,[595]6.3031,[596]6.3069,[597]6.3045,[598]6.3007,[599]6.3029,[600]6.3027,[601]6.3013,[602]6.3031,[603]6.3061,[604]6.3071,[605]6.3104,[606]6.3125,[607]6.3108,[608]6.3073,[609]6.3079,[610]6.3115,[611]6.3097,[612]6.3122,[613]6.3086,[614]6.3035,[615]6.2957,[616]6.2988,[617]6.2925,[618]6.2873,[619]6.2817,[620]6.2674,[621]6.2602,[622]6.2585,[623]6.2600,[624]6.2604,[625]6.2603,[626]6.2588,[627]6.2610,[628]6.2615,[629]6.2612,[630]6.2646,[631]6.2710,[632]6.2764,[633]6.2747,[634]6.2780,[635]6.2786,[636]6.2754,[637]6.2719,[638]6.2746,[639]6.2717,[640]6.2727,[641]6.2730,[642]6.2798,[643]6.2820,[644]6.2832,[645]6.2810,[646]6.2853,[647]6.2815,[648]6.2821,[649]6.2822,[650]6.2861,[651]6.2917,[652]6.2924,[653]6.2967,[654]6.2903,[655]6.2897,



  Q4_0 M1 Pro (without BLAS) [655]6.2895 (impl 2)
$  make clean && LLAMA_NO_ACCELERATE=1 make -j perplexity && time ./perplexity -m ./models/7B/ggml-model-q4_0.bin -f ./build/wiki.test.raw -t 8
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-q4_0-matmult
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:  
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread   -c ggml.c -o ggml.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c llama.cpp -o llama.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -c examples/common.cpp -o common.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity 
main: seed = 1681502601
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks, batch_size=512
16.61 seconds per pass - ETA 3.02 hours
[1]4.3830,[2]4.9584,[3]5.8322,[4]6.4747,[5]6.5500,[6]6.5456,[7]6.7229,[8]6.8116,[9]7.1823,[10]7.4177,[11]7.6627,[12]7.7040,[13]7.6137,[14]7.6886,[15]7.9441,[16]7.5493,[17]7.4257,[18]7.3873,[19]7.0143,[20]7.0007,[21]6.9029,[22]6.7178,[23]6.6799,[24]6.5919,[25]6.5925,[26]6.4205,[27]6.2399,[28]6.1389,[29]6.0543,[30]5.8981,[31]5.8706,[32]5.8888,[33]5.8238,[34]5.8593,[35]5.8851,[36]5.9284,[37]5.9321,[38]5.9490,[39]5.9871,[40]6.0456,[41]6.0527,[42]6.0872,[43]6.0444,[44]6.0994,[45]6.1042,[46]6.0786,[47]6.1023,[48]6.0731,[49]6.0802,[50]6.0411,[51]6.0367,[52]6.0256,[53]6.0694,[54]6.0531,[55]6.0305,[56]6.0652,[57]6.0880,[58]6.1098,[59]6.1237,[60]6.1703,[61]6.1591,[62]6.2223,[63]6.2562,[64]6.2711,[65]6.3176,[66]6.3278,[67]6.3461,[68]6.3601,[69]6.3854,[70]6.4176,[71]6.4391,[72]6.4690,[73]6.5340,[74]6.5395,[75]6.5540,[76]6.5703,[77]6.5834,[78]6.5683,[79]6.5980,[80]6.5905,[81]6.6033,[82]6.6072,[83]6.5535,[84]6.5390,[85]6.5275,[86]6.5063,[87]6.4411,[88]6.4125,[89]6.3919,[90]6.3753,[91]6.4013,[92]6.3973,[93]6.3999,[94]6.3974,[95]6.4262,[96]6.4240,[97]6.4168,[98]6.4097,[99]6.3956,[100]6.3954,[101]6.4214,[102]6.4150,[103]6.4371,[104]6.4438,[105]6.4423,[106]6.4599,[107]6.4585,[108]6.4709,[109]6.4656,[110]6.4609,[111]6.4838,[112]6.5028,[113]6.5042,[114]6.5011,[115]6.5094,[116]6.5019,[117]6.5074,[118]6.5360,[119]6.5570,[120]6.5934,[121]6.6097,[122]6.6346,[123]6.6735,[124]6.6914,[125]6.6824,[126]6.7216,[127]6.7587,[128]6.7862,[129]6.7693,[130]6.7787,[131]6.7733,[132]6.7644,[133]6.7517,[134]6.7628,[135]6.7593,[136]6.7462,[137]6.7382,[138]6.7209,[139]6.7092,[140]6.7061,[141]6.6763,[142]6.6715,[143]6.6435,[144]6.6235,[145]6.6149,[146]6.6013,[147]6.6090,[148]6.6112,[149]6.6051,[150]6.6009,[151]6.6020,[152]6.5926,[153]6.5758,[154]6.5668,[155]6.5735,[156]6.5686,[157]6.5870,[158]6.5906,[159]6.5948,[160]6.5974,[161]6.6099,[162]6.5797,[163]6.5675,[164]6.5413,[165]6.5094,[166]6.4807,[167]6.4432,[168]6.4104,[169]6.3970,[170]6.3845,[171]6.3555,[172]6.3374,[173]6.3188,[174]6.2880,[175]6.2658,[176]6.2555,[177]6.2344,[178]6.2109,[179]6.1937,[180]6.1849,[181]6.1624,[182]6.1431,[183]6.1289,[184]6.1287,[185]6.1214,[186]6.1232,[187]6.1286,[188]6.1249,[189]6.1433,[190]6.1442,[191]6.1647,[192]6.1811,[193]6.1988,[194]6.2104,[195]6.2314,[196]6.2484,[197]6.2705,[198]6.2861,[199]6.2891,[200]6.2937,[201]6.2895,[202]6.3099,[203]6.3165,[204]6.3164,[205]6.3274,[206]6.3352,[207]6.3311,[208]6.3396,[209]6.3448,[210]6.3499,[211]6.3597,[212]6.3671,[213]6.3778,[214]6.3814,[215]6.3856,[216]6.4005,[217]6.4184,[218]6.4320,[219]6.4322,[220]6.4287,[221]6.4225,[222]6.4188,[223]6.4079,[224]6.4012,[225]6.3965,[226]6.4181,[227]6.4268,[228]6.4327,[229]6.4393,[230]6.4349,[231]6.4518,[232]6.4387,[233]6.4215,[234]6.4059,[235]6.3901,[236]6.3823,[237]6.3719,[238]6.3752,[239]6.3590,[240]6.3487,[241]6.3520,[242]6.3557,[243]6.3542,[244]6.3422,[245]6.3395,[246]6.3274,[247]6.3150,[248]6.3083,[249]6.3062,[250]6.3109,[251]6.3032,[252]6.2998,[253]6.2895,[254]6.2855,[255]6.2739,[256]6.2548,[257]6.2436,[258]6.2349,[259]6.2328,[260]6.2247,[261]6.2204,[262]6.2145,[263]6.2100,[264]6.1910,[265]6.1902,[266]6.1887,[267]6.1817,[268]6.1914,[269]6.1895,[270]6.1901,[271]6.1979,[272]6.2025,[273]6.2020,[274]6.2034,[275]6.2124,[276]6.2179,[277]6.2340,[278]6.2449,[279]6.2535,[280]6.2571,[281]6.2670,[282]6.2731,[283]6.2878,[284]6.2956,[285]6.3051,[286]6.3197,[287]6.3191,[288]6.3252,[289]6.3161,[290]6.3009,[291]6.2855,[292]6.2697,[293]6.2558,[294]6.2583,[295]6.2577,[296]6.2621,[297]6.2607,[298]6.2634,[299]6.2606,[300]6.2494,[301]6.2495,[302]6.2414,[303]6.2337,[304]6.2259,[305]6.2234,[306]6.2102,[307]6.2126,[308]6.2158,[309]6.1994,[310]6.1934,[311]6.1871,[312]6.1893,[313]6.1837,[314]6.1824,[315]6.1659,[316]6.1616,[317]6.1450,[318]6.1233,[319]6.1351,[320]6.1482,[321]6.1520,[322]6.1475,[323]6.1409,[324]6.1384,[325]6.1484,[326]6.1484,[327]6.1505,[328]6.1547,[329]6.1607,[330]6.1632,[331]6.1756,[332]6.1725,[333]6.1795,[334]6.1736,[335]6.1671,[336]6.1709,[337]6.1679,[338]6.1666,[339]6.1608,[340]6.1564,[341]6.1642,[342]6.1666,[343]6.1722,[344]6.1721,[345]6.1721,[346]6.1692,[347]6.1739,[348]6.1781,[349]6.1799,[350]6.1765,[351]6.1770,[352]6.1771,[353]6.1719,[354]6.1718,[355]6.1773,[356]6.1803,[357]6.1767,[358]6.1857,[359]6.1888,[360]6.1851,[361]6.1847,[362]6.1913,[363]6.2025,[364]6.2090,[365]6.2149,[366]6.2156,[367]6.2243,[368]6.2220,[369]6.2229,[370]6.2239,[371]6.2180,[372]6.2232,[373]6.2289,[374]6.2276,[375]6.2272,[376]6.2356,[377]6.2307,[378]6.2332,[379]6.2393,[380]6.2308,[381]6.2266,[382]6.2208,[383]6.2198,[384]6.2190,[385]6.2178,[386]6.2173,[387]6.2165,[388]6.2119,[389]6.2065,[390]6.1995,[391]6.1914,[392]6.1874,[393]6.1855,[394]6.1881,[395]6.1865,[396]6.1791,[397]6.1866,[398]6.1904,[399]6.1987,[400]6.1983,[401]6.1997,[402]6.2003,[403]6.2021,[404]6.2085,[405]6.1990,[406]6.1956,[407]6.1948,[408]6.1958,[409]6.2082,[410]6.2192,[411]6.2317,[412]6.2480,[413]6.2595,[414]6.2671,[415]6.2723,[416]6.2804,[417]6.2935,[418]6.2970,[419]6.3043,[420]6.3130,[421]6.3251,[422]6.3308,[423]6.3380,[424]6.3500,[425]6.3591,[426]6.3656,[427]6.3701,[428]6.3784,[429]6.3829,[430]6.3919,[431]6.4065,[432]6.4108,[433]6.4095,[434]6.4049,[435]6.4056,[436]6.4080,[437]6.4175,[438]6.4254,[439]6.4218,[440]6.4213,[441]6.4162,[442]6.4153,[443]6.4166,[444]6.4169,[445]6.4149,[446]6.4173,[447]6.4202,[448]6.4246,[449]6.4218,[450]6.4221,[451]6.4179,[452]6.4060,[453]6.3977,[454]6.3917,[455]6.3924,[456]6.3972,[457]6.3989,[458]6.3967,[459]6.3977,[460]6.4063,[461]6.4035,[462]6.4019,[463]6.4070,[464]6.4061,[465]6.4030,[466]6.3950,[467]6.3952,[468]6.3951,[469]6.3973,[470]6.3978,[471]6.3930,[472]6.3976,[473]6.3920,[474]6.3933,[475]6.3875,[476]6.3898,[477]6.3826,[478]6.3817,[479]6.3881,[480]6.3933,[481]6.3953,[482]6.3907,[483]6.3866,[484]6.3888,[485]6.3871,[486]6.3816,[487]6.3816,[488]6.3797,[489]6.3748,[490]6.3721,[491]6.3690,[492]6.3632,[493]6.3602,[494]6.3584,[495]6.3582,[496]6.3547,[497]6.3494,[498]6.3476,[499]6.3426,[500]6.3329,[501]6.3259,[502]6.3258,[503]6.3255,[504]6.3162,[505]6.3187,[506]6.3196,[507]6.3135,[508]6.3092,[509]6.3081,[510]6.3121,[511]6.3167,[512]6.3202,[513]6.3220,[514]6.3287,[515]6.3231,[516]6.3224,[517]6.3234,[518]6.3235,[519]6.3266,[520]6.3293,[521]6.3310,[522]6.3339,[523]6.3349,[524]6.3412,[525]6.3449,[526]6.3461,[527]6.3482,[528]6.3428,[529]6.3432,[530]6.3385,[531]6.3375,[532]6.3423,[533]6.3446,[534]6.3428,[535]6.3452,[536]6.3398,[537]6.3374,[538]6.3422,[539]6.3434,[540]6.3474,[541]6.3483,[542]6.3490,[543]6.3504,[544]6.3516,[545]6.3494,[546]6.3501,[547]6.3455,[548]6.3400,[549]6.3402,[550]6.3375,[551]6.3337,[552]6.3316,[553]6.3274,[554]6.3252,[555]6.3223,[556]6.3220,[557]6.3243,[558]6.3204,[559]6.3199,[560]6.3194,[561]6.3195,[562]6.3177,[563]6.3177,[564]6.3221,[565]6.3238,[566]6.3234,[567]6.3212,[568]6.3217,[569]6.3201,[570]6.3227,[571]6.3234,[572]6.3244,[573]6.3245,[574]6.3209,[575]6.3204,[576]6.3203,[577]6.3192,[578]6.3172,[579]6.3180,[580]6.3113,[581]6.3075,[582]6.3066,[583]6.3073,[584]6.3077,[585]6.3000,[586]6.2932,[587]6.2934,[588]6.2984,[589]6.3042,[590]6.3073,[591]6.3094,[592]6.3080,[593]6.3043,[594]6.3054,[595]6.3030,[596]6.3068,[597]6.3044,[598]6.3007,[599]6.3028,[600]6.3026,[601]6.3011,[602]6.3029,[603]6.3059,[604]6.3070,[605]6.3103,[606]6.3123,[607]6.3106,[608]6.3071,[609]6.3077,[610]6.3114,[611]6.3095,[612]6.3120,[613]6.3084,[614]6.3033,[615]6.2955,[616]6.2985,[617]6.2923,[618]6.2871,[619]6.2815,[620]6.2672,[621]6.2599,[622]6.2582,[623]6.2597,[624]6.2602,[625]6.2601,[626]6.2585,[627]6.2606,[628]6.2612,[629]6.2609,[630]6.2643,[631]6.2707,[632]6.2761,[633]6.2744,[634]6.2777,[635]6.2783,[636]6.2751,[637]6.2717,[638]6.2743,[639]6.2714,[640]6.2724,[641]6.2727,[642]6.2796,[643]6.2817,[644]6.2829,[645]6.2808,[646]6.2850,[647]6.2812,[648]6.2819,[649]6.2820,[650]6.2859,[651]6.2915,[652]6.2922,[653]6.2965,[654]6.2902,[655]6.2895,

llama_print_timings:        load time = 17160.99 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 12080168.91 ms / 335360 tokens (   36.02 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 12113760.51 ms

real	201m54.092s
user	1602m45.695s
sys	2m57.719s",7,16
953,2023-04-13T22:51:49Z,2023-04-22T09:10:39Z,2023-04-22T09:10:39Z,4,466,43,"Use ggml_internal_get_quantize_fn to loop through all quantization formats and run sanity checks on the implemented functions.
They are run by ctest, but also accept a few command line parameters for more output.
This is a quick test with generated data, so the measurements are not very useful to guide perplexity, but they might surface issues like #876
Also add a microbenchmark that times these functions directly without running the rest of the GGML graph.
Some similarity with #653, but I think there is value both in having tests that run the full GGML graph and tests for specific issues with the SIMD implementations.
Example output:

test-quantize-fns -v
 q4_0 absolute quantization error: ok (0.001466)
 q4_0 reference implementation error: ok (0.000000)
 q4_0 dot product error: ok (0.002492)
 q4_1 absolute quantization error: ok (0.001296)
 q4_1 reference implementation error: ok (0.000000)
 q4_1 dot product error: ok (0.012034)
0 tests failed



test-quantize-perf -3 --op vec_dot_q
q4_0
  vec_dot_q
    3200 values (0.01 MB)
      min cycles/32 vals   :      2.95
      avg cycles/32 vals   :      2.97
      float32 throughput   :     59.60 GB/s
      quantized throughput :      9.31 GB/s
    64000 values (0.24 MB)
      min cycles/32 vals   :      2.54
      avg cycles/32 vals   :      3.89
      float32 throughput   :     45.85 GB/s
      quantized throughput :      7.16 GB/s
    640000 values (2.44 MB)
      min cycles/32 vals   :      2.52
      avg cycles/32 vals   :      2.77
      float32 throughput   :     64.26 GB/s
      quantized throughput :     10.04 GB/s

q4_1
  vec_dot_q
    3200 values (0.01 MB)
      min cycles/32 vals   :      5.44
      avg cycles/32 vals   :      5.48
      float32 throughput   :     29.80 GB/s
      quantized throughput :      5.59 GB/s
    64000 values (0.24 MB)
      min cycles/32 vals   :      5.21
      avg cycles/32 vals   :      6.79
      float32 throughput   :     26.20 GB/s
      quantized throughput :      4.91 GB/s
    640000 values (2.44 MB)
      min cycles/32 vals   :      5.05
      avg cycles/32 vals   :      5.06
      float32 throughput   :     35.32 GB/s
      quantized throughput :      6.62 GB/s",4,5
960,2023-04-14T08:29:09Z,2023-04-14T12:23:22Z,2023-04-14T12:23:22Z,1,5,3,followup to #545,2,0
962,2023-04-14T08:36:52Z,2023-04-14T13:37:11Z,2023-04-14T13:37:12Z,2,3,3,"after #545 we do not need torch, tqdm and requests in the dependencies
@sw why did you add requests in #293 (comment), I can't find it is used anywhere",2,1
976,2023-04-14T18:43:28Z,2023-04-14T19:46:49Z,2023-04-14T19:46:49Z,1,1,1,fixes #974,2,0
979,2023-04-14T19:11:05Z,2023-05-02T16:13:26Z,2023-05-02T16:13:26Z,1,5,5,"This PR modifies the example to change the boolean input_noecho to input_echo to remove negation.
I was reading thru the code, and thought it would probably make it easier for further refactoring later.
Thanks to everyone who has been committing to this project!",3,7
980,2023-04-14T19:16:29Z,2023-04-23T08:21:26Z,2023-04-23T08:21:26Z,1,13,12,fixes #975,7,15
981,2023-04-14T19:18:30Z,2023-04-25T21:19:58Z,2023-04-25T21:19:58Z,1,3,3,,4,0
986,2023-04-14T20:35:20Z,2023-04-15T18:28:56Z,2023-04-15T18:28:56Z,1,4,4,"As rightly pointed out by @jxy here, my changes in #703 limiting the calculation to int8_t might overflow.
-> Change the types to int instead.",3,2
987,2023-04-14T20:37:33Z,2023-04-15T05:51:55Z,2023-04-15T05:51:55Z,1,1,1,"While working on #933, I noticed that the benchmark always prints zeroes instead of tensor sums:
------ Test 1 - Matrix Mult via F32 code ------------------------------------------------------------------------------
cgraph->n_threads=1
            m11: type = 0 ( FP32) ne = 11008 x  4096 x     1, nb = (    4, 44032, 180355072) - Sum of tensor m11 is   0.00
             m2: type = 0 ( FP32) ne = 11008 x   128 x     1, nb = (    4, 44032, 5636096) - Sum of tensor m2 is   0.00
    gf.nodes[0]: type = 0 ( FP32) ne =  4096 x   128 x     1, nb = (    4, 16384, 2097152) - Sum of tensor gf.nodes[0] is   0.00
...

This looks weird because these sums are later used to compare the result of quantized matrix multiplication against the result of regular one, so they are not supposed to be zero. This is my interpretation of why that happens:

tensor_sum_elements() only sums FP32 tensors and returns 0 for everything else
it checks if the provided tensor is FP32 by comparing its type with 6
the benchmark was developed with a version of ggml where 6 corresponded to GGML_TYPE_F32
however, the ggml_type definition was changed later so that 6 now means GGML_TYPE_I32
hence, tensor_sum_elements() now returns 0 when passed a tensor with GGML_TYPE_F32

As far as I can see, there are no compelling reasons to hardcode 6 in the type comparison. Changing 6 to GGML_TYPE_F32 both makes the code correct (I hope) again and prevents similar errors in the future. Here's what the benchmark prints now:
------ Test 1 - Matrix Mult via F32 code ------------------------------------------------------------------------------
cgraph->n_threads=1
            m11: type = 0 ( FP32) ne = 11008 x  4096 x     1, nb = (    4, 44032, 180355072) - Sum of tensor m11 is 16777216.00
             m2: type = 0 ( FP32) ne = 11008 x   128 x     1, nb = (    4, 44032, 5636096) - Sum of tensor m2 is 2818048.00
    gf.nodes[0]: type = 0 ( FP32) ne =  4096 x   128 x     1, nb = (    4, 16384, 2097152) - Sum of tensor gf.nodes[0] is 11611394048.00
...",2,0
991,2023-04-15T02:01:42Z,2023-04-15T21:53:21Z,2023-04-15T21:53:21Z,1,4,2,"Calling mmap.mmap on Windows apparently resets the file offset of the raw file object (and makes the BufferedReader return a negative file offset).  For safetensors, avoid using the file offset after calling mmap.  For GGML format, explicitly save and restore the offset.
Fixes #966.",3,0
992,2023-04-15T02:18:49Z,2023-04-15T05:51:12Z,2023-04-15T05:51:12Z,1,15,0,"Current CMakeLists.txt can't find OpenBLAS header file, cblas.h.
I fixed CMakeLists.txt.
Please confirm my PR.",2,0
1009,2023-04-16T02:48:34Z,2023-04-16T09:13:42Z,2023-04-16T09:13:42Z,2,6,1,"Currently, main fails to build on mingw (msys2) due to missing headers:
llama.cpp: In function 'llama_context* llama_init_from_file(const char*, llama_context_params)':
llama.cpp:1675:23: error: 'time' was not declared in this scope; did you mean 'ftime'?
 1675 |         params.seed = time(NULL);
      |                       ^~~~
      |                       ftime

This is solved by including <ctime>.
Also there are several warnings about printf formats:
llama.cpp: In function 'size_t checked_div(size_t, size_t)':
llama.cpp:258:39: warning: unknown conversion type character 'z' in format [-Wformat=]
  258 |         throw format(""error dividing %zu / %zu"", a, b);
      |                                       ^

As far as I know, this wraning is bogus and appears to be a bug: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95130
This is solved by using gnu_printf for format attribute when building for mingw.",2,0
1014,2023-04-16T09:57:11Z,2023-04-21T18:18:09Z,2023-04-21T18:18:09Z,1,13,5,"This new loop around llama_eval is a bit redundant with the batching done in the main loop, but without a refactor it's all still necessary to keep print statements happening at the right times.",2,1
1021,2023-04-17T07:06:36Z,2023-04-22T08:56:35Z,2023-04-22T08:56:35Z,1,4,1,Currently the timing stats do not print when you use CTRL+C to exit interactive mode. This is the simplest way to fix that.,3,1
1025,2023-04-17T11:51:35Z,,2023-05-11T11:27:39Z,5,13096,3,"I've created an example which provides an HTTP interface to LLaMA using Crow. This comes as a single header file which I've committed. Also, this library depends on Boost, so to build this example, one needs to install Boost (for MacOS its brew install boost).",5,22
1029,2023-04-17T15:23:14Z,2023-04-17T19:34:36Z,2023-04-17T19:34:36Z,1,1,0,Thank you for the great library. I have been developing Ruby bindings for llama.cpp. I would be grateful if you could add it to the bindings list on README.,2,0
1031,2023-04-17T16:16:52Z,2023-04-17T18:26:24Z,2023-04-17T18:26:24Z,1,2,1,"This came up when trying to convert the gpt4all-lora-unfiltered-quantized.bin file
I don't know if it was excluded on purpose.",3,3
1035,2023-04-18T01:32:24Z,2023-04-18T22:53:24Z,2023-04-18T22:53:24Z,1,238,64,"Reduces overall LoRA loading times significantly when using a different base model with --lora-base, from 32s to 24s in my test case.
It also seems to improve the general performance of ggml_cpy significantly, about twice as fast, but overall this is an insignificant fraction of the eval time, so it isn't really noticeable.
I tried to cover all the paths in ggml_cpy, but there are a lot of them and only a few are hit in llama.cpp, so I have not tested every single one.
Perplexity (bs=512):
MASTER: perf_total_per_op_us[             CPY] =  309.170 ms
PR:     perf_total_per_op_us[             CPY] =  132.353 ms

LoRA (quantize):
MASTER: perf_total_per_op_us[             CPY] =  45.780 ms
PR:     perf_total_per_op_us[             CPY] =   5.255 ms",3,3
1040,2023-04-18T10:42:58Z,2023-05-09T02:45:48Z,2023-05-09T02:45:48Z,3,364,85,"I wanted to make it easier for authors to use this in their writing process.
I've tested the code on both Linux and Windows. I don't see why it wouldn't also work on a Mac, but it would be nice to have confirmation of that.
I also apologize for the volume of lines changed but each change seemed tied to the next so I ended up putting it through as one commit. With any code changes, my goal was to make main.cpp simpler to read. There's still a lot that can be done, but I do believe the input loop is a lot cleaner now.
Changes:

Adds --author-mode where we read from input until the user ends their line with a \
Allows the user to end with / such that inference starting from that position (without the \n)
Highlights the above-mentioned control characters (when --color is enabled) so it's harder to accidentally end a line with \ or /
Cleans up superfluous control characters and line breaks

Before:

After:



  Detailed changes
Author mode allows one to write (or paste) multiple lines of text without ending each line in a \. LLM models do better the more context you give them, but the original input flow made this difficult. (Before pasting your writing you'd have to edit a \ into each line or paste it to a file, save that file, and then pass that path in with -f. If you're doing multiple revisions, that can become overwhelming.
Another useful feature for writing is the ability to start inference in the middle of a line. It’s not uncommon for writers to get stuck in the middle of a paragraph or piece of dialogue. I wanted to provide a way for the language model to generate text from that point, but previously, the best one could do was to hit enter and have the code append a \n to the buffer before starting inference.
I considered making author mode the default and creating a --chat-mode option with the previous behavior (and implying chat mode with --instruct). However, I decided against changing the default interaction people were familiar with. As a compromise, I allow the / operator to work in both modes.
The other quality of life improvement I really wanted for writing with the AI was to get rid of all the superfluous newlines and control characters that jumble up your text while working. To do this and have it display properly, I had to change the code to read one char at a time. By reading one character at a time, we don't have to print the newline when the user hits enter. This allows the use of / and \ to flow more smoothly and lets us clean up any control characters from the input before we start text inference. This also makes the Windows and Linux versions behave the same in regards to input buffering, for better and for worse. (For example, if you hit a key while it's inferring, it won't jumble up the text, but you also won't see your text while the model is loading.)
Another improvement I made for writing with the AI was to eliminate superfluous newlines and control characters that can clutter up text while working. To achieve this and display text properly, I changed the code to read one character at a time. To start with, when the user hits enter, we don’t have to print a newline. It allows for smoother use of / and and lets us remove any control characters from the input before starting text inference. This change also makes the Windows and Linux versions behave consistently in terms of input buffering, with both advantages and disadvantages. For instance, hitting a key while the model is inferring won’t mix it into the text on Linux anymore, but you also won’t see any of the input you type while the model is loading.
The other code changes are to console_state. It now holds the previous termios settings so we can safely reset the terminal on exit for POSIX systems. This also means including termios.h. The other change was to add the author_mode boolean. I thought about calling it input_mode and making an enum but unless we want to have different input modes for chat, instruct, etc, this is just simpler (and it's easily changed in the future).

I also considered finding a way of hiding that first space that's generated when priming the llama models since the original llama tokenizer also starts each prompt with a single space. Are we sure this is necessary? If so, would it be acceptable to just hide that first space?

Future possible changes
Further quality of life enhancements:

Full support for left, right, ins, del, home, and end keys

I don't foresee any issues implementing this other than there just being a larger code base, which may be unwanted
At some point using a library like ncurses is better, but to keep the requirements to a minimum, adding this ourselves doesn't feel unreasonable


Use color by default for interactive terminals (that aren't being piped)

Implemented with isatty on POSIX systems and _isatty on Windows
Programs like grep already do this
I would suggest adding --color=always,never,auto just like grep has but perhaps switch from color=never to color=auto on any interactive mode (for compatibility --color without an option would just indicate always)


Change the cursor to indicate when the user is waiting on inference

We could probably only do this when use_color is true since changing the cursor will leave artifacts in stdout if piped


Choose a different color for control characters \ and /

I think the current colors are clear enough but if someone feels otherwise, it would be easy to use a different color


Read input on another thread

This would give us the ability to process each line as it is added as opposed to waiting until all input has been entered. This will also allow us to take input a little earlier as the model is being primed with "" ""
Would make the program ""feel"" a little faster



Further code improvements:

Replacing last_n_tokens with a ring-buffer
Abstract away signal handling code to common to remove the last of the #ifdef blocks in main.cpp code
Evaluate conversions from 'time_t' to 'int32_t' and 'time_t' to 'int32_t' and static cast them


Of the future possible changes, I think using the other control keys is most appealing to me. Followed by putting the input on a separate thread to allow context chugging on previous lines as the user writes subsequent lines. I'm not very active on github so I have no way of proving it, but I'm quite experience with threading and synchronization. Both of these (particularly the multithreading) may add more complexity to the code than is desired for an ""example"" program.
Edit: This is also a first step towards moving away from Ctrl-C to interrupt inference if that's desired. Once we are handling keys one a time like this, we can more easily make Esc (or any other key) interrupt inference.",5,37
1041,2023-04-18T14:41:40Z,2023-04-18T19:00:14Z,2023-04-18T19:00:14Z,5,326,1,"I was surprised by the belief that the dot product x * y, where x holds quantized model weights and y contains floating point values, it is faster to quantize y, and to perform the dot product using the quantized y values (accepting the associated loss in precision), then to just directly compute x * y. So, I had to try it myself. This PR adds a simple program that measures the time it takes to perform the dot product between vectors holding 1 << 18 values. I picked a relatively large vector size to not have to get involved with the science of accurately measuring elapsed time for short lasting operations.
Basically, we fill two vectors x and y with random values and quantize x into q. We then measure the time for

Computing d = q * y directly
Computing y' = quantize(y); d = q * y'

For 2. we use the vectorized (SIMD-ified) functions from ggml (or, if requested by a command line argument, the corresponding scalar functions from ggml).
On my Mac, 1. is faster than 2 (~55 us vs ~75 us). On the x86_64 CPU that I have available (Ryzen 7950X), 1. is somewhat slower compared to the AVX2 implementation (~50 us vs ~35 us).
On both CPUs the direct product 1. as implemented in the dot() function in this POC is much faster than the scalar version of 2 from ggml. (~15X faster on the Ryzen 7950X and ~6X faster on the Mac).
I think that with some ARM_NEON or AVX2 magic one should be able to further speed up 1.
To use it, make -j and then e.g. ./vdot 100 to measure 100 dot products with the SIMD-ified ggml functions, or ./vdot 100 1 to measure the scalar ggml functions instead.
Added a comparison for Q4_1 quantization. Here, the direct product 1. is faster than 2. for ARM_NEON and AVX2. On my Mac I get ~69 us for 1 and ~121 us for 2. On the Ryzen 7950X I measured ~60 us for 1. and ~62 us for 2. In any case, implemented as in this POC, the dot product of Q4_1 quantized values is only marginally slower (~20%) than Q4_0.",4,3
1042,2023-04-18T16:14:50Z,2023-04-21T18:27:07Z,2023-04-21T18:27:07Z,1,1,1,"cmake3 --build . --config Release
[  5%] Built target ggml
[ 16%] Built target llama
[ 22%] Linking CXX executable ../bin/test-tokenizer-0
../libllama.a(ggml.c.o)：在函数‘ggml_graph_compute’中：
ggml.c:(.text+0xf2db)：对‘pthread_create’未定义的引用
ggml.c:(.text+0xf9d4)：对‘pthread_join’未定义的引用
collect2: error: ld returned 1 exit status
gmake[2]: *** [bin/test-tokenizer-0] 错误 1
gmake[1]: *** [tests/CMakeFiles/test-tokenizer-0.dir/all] 错误 2
gmake: *** [all] 错误 2",3,2
1043,2023-04-18T16:23:32Z,,2023-04-19T16:01:31Z,1,45,16,"Plugged @ikawrakow's idea from #1041
On master, I get ~51 ms / token:
 $  make -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" -c 2048 -n 512 -t 8 --ignore-eos -s 3 -n 64 -t 8
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/main/main.cpp ggml.o llama.o common.o -o main  -framework Accelerate
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize  -framework Accelerate
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/quantize-stats/quantize-stats.cpp ggml.o llama.o -o quantize-stats  -framework Accelerate
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity  -framework Accelerate
c++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding  -framework Accelerate

====  Run ./main -h for help.  ====

main: seed = 3
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)
llama_init_from_file: kv self size  = 1024.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000
generate: n_ctx = 2048, n_batch = 8, n_predict = 64, n_keep = 0


 I believe the meaning of life is to serve others.
I am a mother, wife and daughter who believes in community service and helping others. My career started as a legal assistant for a criminal defense attorney but soon realized that I was more interested in assisting my clients with their personal matters than with their court cases. I switched to working as
llama_print_timings:        load time =   398.00 ms
llama_print_timings:      sample time =    47.12 ms /    64 runs   (    0.74 ms per run)
llama_print_timings: prompt eval time =   380.06 ms /     8 tokens (   47.51 ms per token)
llama_print_timings:        eval time =  3270.89 ms /    63 runs   (   51.92 ms per run)
llama_print_timings:       total time =  3717.02 ms
On this branch I get ~226 ms / token for same run:
$  make -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" -c 2048 -n 512 -t 8 --ignore-eos -s 3 -n 64 -t 8
I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

make: Nothing to be done for `default'.
main: seed = 3
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 5809.32 MB (+ 1026.00 MB per state)
llama_init_from_file: kv self size  = 1024.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000
generate: n_ctx = 2048, n_batch = 8, n_predict = 64, n_keep = 0


 I believe the meaning of life is to learn, love and leave a legacy.
I believe that if you give it away, it will always come back. If you treat others with kindness and respect, they will reciprocate in time.
If you put out good energy, it will return to you.
What I would like my children
llama_print_timings:        load time =  1595.40 ms
llama_print_timings:      sample time =    47.13 ms /    64 runs   (    0.74 ms per run)
llama_print_timings: prompt eval time =  1586.76 ms /     8 tokens (  198.35 ms per token)
llama_print_timings:        eval time = 14298.88 ms /    63 runs   (  226.97 ms per run)
llama_print_timings:       total time = 15942.39 ms
If I have to guess, at 8 threads the computation becomes memory bound and therefore, even though the Q4_0 x F32 is faster, the Q4_0 x Q8_0 ends up being more performant due to the less memory data being used",3,2
1044,2023-04-18T16:41:16Z,2023-04-19T09:22:46Z,2023-04-19T09:22:46Z,5,221,13,"Adds support for NVIDIA cuBLAS for batched operations. In my system this is significantly faster than OpenBLAS.
Build with LLAMA_CUBLAS:
make clean && LLAMA_CUBLAS=1 make

Perplexity seconds per pass (i9 9900k, RTX 3080 10GB)




7B q4_0
7B f16
7B f32




cuBLAS
8.92
5.24
7.70


OpenBLAS
22.64
24.85
18.18


No BLAS
26.39
30.35
54.33",9,19
1046,2023-04-18T18:19:12Z,2023-04-18T20:54:57Z,2023-04-18T20:54:57Z,5,287,11,"ref #959
This is a reimplementation of #1026 by introducing new quantization type Q4_2
This PR implements only ARM NEON. The plan is to merge this soon and add rest of the SIMD implementations.
For now, no need for SIMD quantize / dequantize - will be added later when needed",3,6
1061,2023-04-19T14:18:03Z,2023-04-19T16:06:38Z,2023-04-19T16:06:38Z,3,9,535,"Cleanup following #951 and #1046:

remove unused ggml_vec_dot_q4_0
warn for unused C functions (I didn't touch C++)
use ggml_is_quantized for the work buffer calculation",3,2
1062,2023-04-19T15:15:55Z,2023-04-19T18:20:14Z,2023-04-19T18:20:14Z,1,87,3,"For quantize-stats we get
q4_2: rmse 0.00159301, maxerr 0.17480469, 95pct<0.0030, median<0.0012
For 7B perplexity with BLAS enabled we get 6.2038 after 655 chunks.
Quantization is slow (~90 seconds on my Mac for 7B) as not multi-threaded as in PR #896.",5,0
1065,2023-04-19T16:08:38Z,2023-04-20T01:14:15Z,2023-04-20T01:14:15Z,5,221,41,"For me this makes cuBLAS about twice as fast with quantized models.
Perplexity seconds per pass



Model
PR
Master




q4_0
5.05
8.62


q4_1
5.37
8.59


q4_2
4.99
10.76



Prompt eval time with 7B q4_0 (bs=512)
cuBLAS (PR):     prompt eval time =  7840.48 ms /   631 tokens (   12.43 ms per token)
cuBLAS (Master): prompt eval time = 15457.33 ms /   631 tokens (   24.50 ms per token)
OpenBLAS:        prompt eval time = 34856.06 ms /   631 tokens (   55.24 ms per token)
No BLAS:         prompt eval time = 43549.67 ms /   631 tokens (   69.02 ms per token)

13B q4_0
cuBLAS (PR):     prompt eval time = 13826.48 ms /   631 tokens (   21.91 ms per token)
cuBLAS (Master): prompt eval time = 27987.82 ms /   631 tokens (   44.35 ms per token)
OpenBLAS:        prompt eval time = 61476.58 ms /   631 tokens (   97.43 ms per token)
No BLAS:         prompt eval time = 81645.43 ms /   631 tokens (  129.39 ms per token)",8,15
1068,2023-04-19T18:48:47Z,2023-04-20T06:45:41Z,2023-04-20T06:45:41Z,1,73,26,Adding a missing piece to the puzzle...,3,1
1073,2023-04-19T21:58:03Z,,2023-10-10T21:34:02Z,6,480,15,"Adds a q4_0c type that corresponds to the q4_0 layout but with a different memory layout.
Draft status, currently only accelerated for AVX-512, will add a PoC of Neon acceleration but wanted to put this out there since there is some experimentation with quantization formats going on now.
The layout consists of all the quantized values first in blocks of 128 nibbles, followed by all the scales.
The nibbles within a block are laid out consecutively in the lower nibbles, and then consecutively in the higher nibbles.
For dot products we use a q8_0c format, with all the qs bytes followed by all the scales.
The big win is for architectures with larger registers like AVX-512, that can now get two continuous blocks of qs by doing roughly
   xqs01  = xqs0123      & 0x0f0f...
   xqs23  = xqs0123 >> 4 & 0x0f0f...

The dot product implementation here borrows from @dfyz's implementation in #933, but becomes simpler because we don't need to do tricks with the byte layout.
Besides the simplified implementation there is also a small improvement in performance:
llama_print_timings: prompt eval time =   665.66 ms /     6 tokens (  110.94 ms per token)
llama_print_timings:       total time = 15398.10 ms
vs
llama_print_timings: prompt eval time =   449.19 ms /     6 tokens (   74.86 ms per token)
llama_print_timings:       total time = 13557.80 ms
The SIMD implementation with 128-bit registers like Neon should look very similar to the current implementations, with similar speeds. Possibly some benefit from doing only aligned loads.
The scalar implementations are slightly more complex but I do not see any degraded performance.
Perplexity should be exactly the same as q4_0.

Example timings (7B)
system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 |
Current master q4_0:
llama_print_timings:        load time =   797.54 ms
llama_print_timings:      sample time =    69.77 ms /   100 runs   (    0.70 ms per run)
llama_print_timings: prompt eval time =   665.66 ms /     6 tokens (  110.94 ms per token)
llama_print_timings:        eval time = 14528.96 ms /    99 runs   (  146.76 ms per run)
llama_print_timings:       total time = 15398.10 ms

q4_0 with #933
llama_print_timings:        load time =   751.74 ms
llama_print_timings:      sample time =    89.38 ms /   100 runs   (    0.89 ms per run)
llama_print_timings: prompt eval time =   620.75 ms /     6 tokens (  103.46 ms per token)
llama_print_timings:        eval time = 13475.35 ms /    99 runs   (  136.11 ms per run)
llama_print_timings:       total time = 14318.72 ms

continuous layout q4_0c:
llama_print_timings:        load time =   596.51 ms
llama_print_timings:      sample time =    73.39 ms /   100 runs   (    0.73 ms per run)
llama_print_timings: prompt eval time =   449.19 ms /     6 tokens (   74.86 ms per token)
llama_print_timings:        eval time = 12885.82 ms /    99 runs   (  130.16 ms per run)
llama_print_timings:       total time = 13557.80 ms


Todos:

 AVX-512 acceleration
 PoC ARM NEON acceleration
 Investigate performance on M1
 test out fully aligned loads by aligning the quantized file to 64 bytes

Future improvements:

PoC for q4_2-like quantization
PoC 3-bit quantization
Support row lengths not divisible by 128. I don't see any big obstacles, will require some extra code for the leftover block, and padding of rows to keep alignment.",4,15
1074,2023-04-20T02:23:59Z,2023-04-20T15:15:19Z,2023-04-20T15:15:19Z,1,1,2,"Accelerate is an Apple framework which can only be used on macOS, and the CMake build ignores the LLAMA_ACCELERATE variable when run on non-Apple platforms. This implies setting LLAMA_ACCELERATE is a no-op on Ubuntu and can be removed.
This will reduce visual noise in CI check results (in addition to reducing the number of checks we have to run for every PR). Right now every sanitized build is duplicated twice for no good reason (e.g., we have CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, ON) and CI / ubuntu-latest-cmake-sanitizer (ADDRESS, Debug, OFF)).",2,0
1075,2023-04-20T05:45:29Z,2023-04-20T17:42:27Z,2023-04-20T17:42:27Z,6,182,60,"This PR adds multi-threading for quantization.
The gain is very minor for small models (e.g., LLaMA 7B) and simple quantization (Q4_0 and Q4_1), but very significant for large models and the now more elaborate Q4_2 quantization.
quantize-stats now finishes in just 14.5 seconds (7B) or 44 seconds (13B) on my computer for all 3 quantization types. The single-threaded version took 144 seconds (7B) or 242 seconds (13B).",5,2
1080,2023-04-20T11:09:53Z,2023-04-20T13:28:43Z,2023-04-20T13:28:43Z,1,1,1,,3,1
1082,2023-04-20T16:00:00Z,2023-04-20T17:35:53Z,2023-04-20T17:35:53Z,5,293,37,"Initial Q4_3 implementation runs at ~82 ms / token on M1.
Need to see if we can optimize that somehow.
For example Q4_1 runs at ~55 ms / token, so there is probably lots of room for improvement
#define QK4_3 16
typedef struct {
    ggml_fp16_t d;         // delta
    ggml_fp16_t m;         // min
    uint8_t qs[QK4_3 / 2]; // nibbles / quants
} block_q4_3;

Merging this, although the speed is not satisfying. We have to try to get it as fast as Q4_1.
We might have to change the block_q4_3 if needed to achieve this",2,0
1083,2023-04-20T17:01:00Z,2023-04-21T15:18:26Z,2023-04-21T15:18:26Z,3,251,46,"The idea behind being that Q8_0 quantized
values get used many times in the matrix multiplications where they are involved. In the current implementations, when we are evaluating the dot products, we need to compute the sum of the quants in the Q8_0 vector, so the same operation is repeated many times. This makes the Q4_1 * Q8_0 dot product significantly slower than Q4_0 * Q8_0 (~80%).
In the PR the sum of Q8_0 quants is computed during quantization and stored it in the
now modified block_q8_0 struct. It is then reused in the subsequent dot products.
In a synthetic benchmark (just compute a bunch of dot products, see q8dot.cpp), this change speeds up the Q4_1 * Q8_0 dot product by 80%, making the performance identical to Q4_0 * Q8_0.
In practical application, I see a ~15% gain in speed for token prediction on M2, and ~5% gain on Ryzen 7950X. The speed gain in the prompt evaluation is much bigger (around 50%).
I have only done the change for the scalar version, ARM_NEON, and AVX2, so we still need an AVX implementation.",5,3
1087,2023-04-20T18:43:33Z,2023-08-25T09:09:42Z,2023-08-25T09:09:43Z,12,335,59,"Currently I can say that for regular users the CLBlast version is much easier to run. If you want the most performance, though, HIP is for you.

Remember to tweak the new settings LLAMA_CUDA_DMMV_X and LLAMA_CUDA_MMV_Y, LLAMA_CUDA_KQUANTS_ITER
I get the best result with 128, 8 and 1, for example.

Note for unsupported GPU users:
You need to use an environment variable to force ROCm to run.
You can ckeck this resource: ROCm supported-gpu-list
export HSA_OVERRIDE_GFX_VERSION=10.3.0
This will make it work in the currently running shell, after that ./main and other llama.cpp commands will run.
rocBLAS is only released for a limited number of GPUs: gfx900 gfx906 gfx908 gfx90a gfx1030 (depends on ROCm version, etc).
If you look in /opt/rocm/lib/rocblas/library/ you should see a lot of files, but only for some GPUs, for others you need to find something that is close enough, like gfx1030 instead of gfx1033, and then that becomes 10.3.0 for the environment variable.

If you have multiple AMD devices:
If you have a GPU and APU then it may try to use wrong devices. There is an environment variable you can set to control the selected device:
export HIP_VISIBLE_DEVICES=0

ROCm port
I just define all the cudaXxx functions to hipXxx etc. This may seem stupidly simple but it's exactly the same kind of trick AMD uses to make HIP code compile with nvcc, you can see it in /opt/rocm/include/hip/nvidia_detail/nvidia_hip_runtime_api.h (for some reason I can't find the source for this anywhere online but it has a free license, so if you want, I can post it).
HIP can also compile the Cuda kernel programs without any major modifications, just some header stuff.
Compiling
To  this, you need the ROCm developer kit and hipBLAS which may be a separate package.
With CMake I have to invoke:
CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ cmake -DLLAMA_HIPBLAS=ON
It is probably unavoidable to use the LLVM Clang compiler You can use the ROCm included one or the system one, but mixing it with GCC objects is just asking for trouble.
Makefile should work, too, pass in LLAMA_HIPBLAS=1. You can use the env variable ROCM_PATH if ROCm is not installed at /opt/rocm:
make -j4 LLAMA_HIPBLAS=1
Makefile will override the compilers to ROCm LLVM, so it should be a simple command to compile. But you should be able to  override the compilers on the make command line.
Docker
Probably the best option right now is using Docker with AMD's images:
FROM rocm/dev-ubuntu-22.04:5.5-complete AS build

WORKDIR /app

COPY . ./

RUN make LLAMA_HIPBLAS=1

ENV PATH=""/app:$PATH""
CMD [ ""main"" ]
Save it somewhere as rocm.Dockerfile then in llama.cpp's source do:
docker build -f /path/to/rocm.Dockerfile . -t llama.cpp:rocm
Then run it like this:
docker run --rm -it --init \
    --device /dev/dri --device /dev/kfd \
    -v/my/models:/models llama.cpp:rocm \
    main -m /models/llama-7b-q4_2.bin -p ""$(cat prompts/dan.txt)""
You can also add the override like this: -e HSA_OVERRIDE_GFX_VERSION=10.3.0 and -e HIP_VISIBLE_DEVICES=0 as needed. There may be also some other security flags needed on some distros, and whatever permissions your user needs to have for the devices (usually group video).
Using nerdctl, I had to add the DRI devices separately (--device /dev/dri/card0 --device /dev/dri/renderD128 rather than the /dev/dri directory like in Docker), it also works, but beware that on some buildkit setups it will load the whole image  via tarballs and since it's several gigabytes it will take some time to build.
All the commands are there besides main, you can also run /bin/bash for a dev shell, mount the llama.cpp source somewhere and use it for development. It is a bit of a thick image, for end users, maybe too big, I want to trim it down but the AMD stuff is bloated.


What's up with the compilers?
Regarding hipcc, it is not really a compiler, I had a lot of problems with it, it couldn't compile and link .cpp and .o files together (like hipcc main.cpp llama.o ggml.o ...). If you open it in a text editor you see it's a Perl script and all it does is provide some default flags for the Clang compiler. It might work in CMake, since CMake always compiles to objects first.
It shouldn't be a requirement to use AMD's version of Clang, it is possible to use any normal Clang or LLVM (maybe even Zig?) to compile the device code. In the CMake build I added a warning if the compiler is not Clang but it won't stop you from experimenting (well, it will probably fail to compile the .cu file).
If you use VS Code then the C/C++ plugin doesn't support HIP correctly, it sees in compileCommands.json (part of CMake's output) that the .cu file is using a language argument -x hip and it doesn't know what that is, so the whole file is locked to the C language even if it's actually C++ and you'll see some red squiggles. This flag comes from the hip::device package in CMake.
In CMake it is harder to use different compilers in the same project (may need to use a subdirectory) than in Make, so currently the .cu file is handled as a C++ file and compiled with the rest of the C++ files, this is what AMD's vision is with HIP -- they should just be normal C++ files.
I also tried adding another language, HIP enable_language(HIP), to CMake but I had some trouble getting the CMake to configure in all environments consistently, maybe it it needs some package that was missing in the container. In this case, it would work more similar to Cuda: I can define the .cu file's language to be HIP, whatever compiler configured for HIP compiles it and a compiler that can link it correctly will link it to an executable. When it was working on Arch, it configured it automatically like: CMAKE_CXX_COMPILER=/usr/bin/g++ and CMAKE_HIP_COMPILER=/usr/bin/clang++ and it was working correctly, using the HIP compliler to link in the end. This would be the ideal solution, it would give the user the most control over the config -- if I got it to work, that is 😜. If someone more experienced with this knows how to do it, please go ahead.
For the Makefile I thought it would be easier to override the compilers, because it is supposed to be more beginner friendly and you can get a result in one command (that is if everything is installed properly). But it has some variables also.",39,368
1088,2023-04-20T19:53:07Z,2023-04-20T21:56:45Z,2023-04-20T21:56:45Z,1,20,0,"With the recent flurry of new formats, I didn't want to keep the *.pth files around. So here are the SHA-256 checksum of the ggml model files up to 30B.
Unless I've missed something in the recent commits, model quantization should still be deterministic.",2,1
1091,2023-04-20T21:17:05Z,2023-04-22T08:27:05Z,2023-04-22T08:27:05Z,1,1,1,"As GGML only uses BLAS when n_batch is 32 or larger it is not used by default even if llama.cpp is compiled with a BLAS lib. This is because the default n_batch is 8. My path sets n_batch to the maximum 512 when BLAS is enabled at compile-time and keeps at at 8 if there is no BLAS.
Personally I see the best performance with OpenBLAS and a size of 512, so this is why I chose this value. Experimentation may be needed to come up with a good default (as long as it's larger than 32).
This came out of a discussion at #1065 (comment).",5,17
1094,2023-04-20T22:19:06Z,2023-04-21T19:59:17Z,2023-04-21T19:59:17Z,4,168,107,"Previously, the cuda memory was allocated and freed as needed for each mat mul operation, which is very inefficient.
By using a memory pool, this is about 30-50% faster in my machine.
PR:
7B q4_0 3.59 seconds per pass - ETA 0.65 hours
7B f16  4.59 seconds per pass - ETA 0.83 hours
7B f32  6.68 seconds per pass - ETA 1.22 hours

 7B q4_0 llama_print_timings: prompt eval time =  5493.53 ms /   631 tokens (    8.71 ms per token)
13B q4_0 llama_print_timings: prompt eval time =  9003.90 ms /   631 tokens (   14.27 ms per token)
30B q4_0 llama_print_timings: prompt eval time = 19682.41 ms /   631 tokens (   31.19 ms per token)

Master:
7B q4_0 5.09 seconds per pass - ETA 0.93 hours

 7B q4_0 prompt eval time =  7840.48 ms /   631 tokens (   12.43 ms per token)
13B q4_0 prompt eval time = 13826.48 ms /   631 tokens (   21.91 ms per token)",6,25
1096,2023-04-21T01:54:42Z,2023-04-21T12:57:57Z,2023-04-21T12:57:57Z,1,7,1,"Easier to read, especially when the ETA is below an hour.",3,0
1099,2023-04-21T09:25:15Z,2023-04-22T07:37:05Z,2023-04-22T07:37:05Z,1,92,121,"Apart from adding the AVX2 optimization for Q4_3, this refactors some commonly used intrinsic sequences into inline functions.",4,4
1100,2023-04-21T13:22:57Z,2023-04-22T08:18:21Z,2023-04-22T08:18:21Z,2,5,1,Fix the build error when enable BUILD_SHARED_LIBS. Export all symbols as a solution for now.,2,2
1105,2023-04-21T15:27:25Z,2023-04-22T06:21:33Z,2023-04-22T06:21:33Z,2,133,1,"I have implemented functions for getting and setting the rest of the model state.
It includes: random number generator state, logits, embedding and kv_cache.
It was necessary to store the logits so that we can eval tokens, save state, restart program, load state and then sample.
With just restoring kv_cache the sampling did not have access to the required logits and indeed segfaulted on the initially empty logits vector.
The logits vector initial capacity was reserved with a wrong value. This resulted in changing capacity after the first evaluation in which the logits vector is actually resized. I fixed this bug because it propagated to the state size, resulting in unnecessarily changes.
The random number generator state is also included to ensure consistent sampling results.
Since the internal state of the rng is more than just the seed, it is serialized using the standard C++ api for this purpose by streaming into a stringbuffer. For simplicity I did not add further logic to parse and compress the serialized rng state.
For completeness I also stored the embedding vector.
Because the whole state is not in one contiguous memory buffer I decided on an output pointer parameter to get the state data.
The user is responsible to allocate the memory where the state is written to. To support this the required number of bytes can be requested.",2,0
1106,2023-04-21T15:29:58Z,,2023-05-03T20:25:20Z,4,333,82,"The PR adds a new build option (LLAMA_NO_RMSE), which is off by default. When off, all current quantization types (Q4_0, Q4_1, Q4_2, Q4_3) are performed with RMSE minimization (on master RMSE minimization is enabled for Q4_2 only and cannot easily be disabled).
This makes generation of quantized models quite a bit longer, but still in the same ballpark as it used to take before it was multi-threaded in PR #1075.
With this option enabled, Q4_3 gives a perplexity of 6.0344 for the 7B model, so 0.0273 lower than simple Q4_3 quantization as reported by @ggerganov in #406. If I also enable his trick of not quantizing output tensors, perplexity becomes 6.0085.
Perplexity result for Q4_3 without quantization of output tensors for the 13B model is 5.3117.
Details for these perplexity runs can be found in here (issue #406)
As far as I can tell, we are now on par with best known GPTQ result for 7B, and better for 13B by about 0.05.",7,13
1107,2023-04-21T16:52:34Z,2023-04-22T06:54:33Z,2023-04-22T06:54:33Z,1,10,1,"When using Alpaca, at first I wasn't sure why llama.cpp was giving much poorer results than I received with alpaca.cpp.  After digging in a bit, I discovered that the default repeat_penalty of 1.0 in llama.cpp results in very poor behavior, and even just increasing this a little bit to 1.1 results in MUCH cleaner default behavior.
Examples:
repeat_penalty = 1.1
sampling: temp = 0.200000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000
generate: n_ctx = 2048, n_batch = 256, n_predict = -1, n_keep = 21


== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to LLaMa.
 - If you want to submit another line, end your input in '\'.

 Below is an instruction that describes a task. Write a response that appropriately completes the request.
> What are some good ice breaker questions to use with middle schoolers?
- What's your favorite subject in school and why?
- If you could be any animal, what would it be and why?
- If you had a magic wand, what three things would you change about the world?
- If you were given $100 to spend on anything you wanted, what would you buy?
- What's your favorite book or movie?
- If you could have any superpower, what would it be and why?
- If you had a time machine, where in the past or future would you go?
>

repeat_penalty = 1.0
sampling: temp = 0.200000, top_k = 10000, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.000000
generate: n_ctx = 2048, n_batch = 256, n_predict = -1, n_keep = 21


== Running in interactive mode. ==
 - Press Ctrl+C to interject at any time.
 - Press Return to return control to LLaMa.
 - If you want to submit another line, end your input in '\'.

 Below is an instruction that describes a task. Write a response that appropriately completes the request.
> What are some good ice breaker questions to use with middle schoolers?
- What is your favorite subject in school?
- What is your favorite book?
- What is your favorite movie?
- What is your favorite food?
- What is your favorite hobby?
- What is your favorite sport?
- What is your favorite song?
- What is your favorite TV show?
- What is your favorite game?
- What is your favorite toy?
- What is your favorite season?
- What is your favorite animal?
- What is your favorite color?
- What is your favorite sport team?
- What is your favorite vacation spot?
- What is your favorite dessert?
- What is your favorite board game?
- What is your favorite video game?
- What is your favorite TV show character?
- What is your favorite movie character?
- What is your favorite book character?
- What is your favorite cartoon character?
- What is your favorite superhero?
- What is your favorite supervillain?
- What is your favorite superhero movie?
- What is your favorite superhero TV show?
- What is your favorite superhero video game?
- What is your favorite superhero comic book?
- What is your favorite superhero movie character?
- What is your favorite superhero TV show character?
- What is your favorite superhero video game character?
- What is your favorite superhero comic book character?
- What is your favorite superhero movie villain?
- What is your favorite superhero TV show villain?
- What is your favorite superhero video game villain?
- What is your favorite superhero comic book villain?
- What is your favorite superhero movie hero?
- What is your favorite superhero TV show hero?
- What is your favorite superhero video game hero?
- What is your favorite superhero comic book hero?
- What is your favorite superhero movie sidekick?
- What is your favorite superhero TV show sidekick?
- What is your favorite superhero video game sidekick?
- What is your favorite superhero comic book sidekick?
- What is your favorite superhero movie power?
- What is your favorite superhero TV show power?
- What is your favorite superhero video game power?
- What is your favorite superhero comic book power?
- What is your favorite superhero movie villain power?
- What is your favorite superhero TV show villain power?
- What is your favorite superhero video game villain power?
- What is your favorite superhero comic book villain power?
- What is your favorite superhero movie superpower?
- What is your favorite superhero TV show superpower?
- What is your favorite superhero video game superpower?
- What is your favorite superhero comic book superpower?
- What is your favorite superhero movie supervillain power?
- What is your favorite superhero TV show supervillain power?
- What is your favorite superhero video game supervillain power?
- What is your favorite super^C

Finally had to kill it at this point, because it wasn't stopping.
I've tinkered around with other settings like temperature or more aggressive repeat penalties, but I think that minimal changes are better at this point.  A value of 1.1 is nice and gentle and will provide a better default ""out of the box"" / first-impression experience for users who are wanting to get up and running with this quickly.",2,0
1109,2023-04-21T17:56:41Z,2023-04-22T07:55:36Z,2023-04-22T07:55:36Z,1,70,80,"This one looks promising - it does not change the Q4_3 format from master and only modifies slightly Q8_0 by adding low and high sums. The results should be identical, but now the Q4_3 dot product evaluates much faster:
#define QK8_0 32
typedef struct {
    float   d;          // delta
    float   s0;         // d * sum(qs[i]) low
    float   s1;         // d * sum(qs[i]) high
    int8_t  qs[QK8_0];  // quants
} block_q8_0;
llama_print_timings:      sample time =    47.11 ms /    64 runs   (    0.74 ms per run)
llama_print_timings: prompt eval time =   482.44 ms /     8 tokens (   60.30 ms per token)
llama_print_timings:        eval time =  3419.36 ms /    63 runs   (   54.28 ms per run)
llama_print_timings:       total time =  3959.05 ms

I think this is the way to go. But, let's see the ppl results from the Q4_3a #1108 approach first",2,1
1119,2023-04-22T09:41:37Z,2023-04-23T07:57:06Z,2023-04-23T07:57:06Z,1,12,0,Use only three instructions to implement packNibbles when AVX512 is available. (The _mm256_cvtepi16_epi8 requires AVX512 support),3,2
1123,2023-04-22T11:31:53Z,2023-05-03T01:01:58Z,2023-05-03T01:01:58Z,1,7,10,"This uses Window's SetConsoleCtrlHandler to handle ctrl+c.
We currently use signal but Windows' official documentation for signal warns us:


Don't issue low-level or STDIO.H I/O routines (for example, printf or fread).
Don't call heap routines or any routine that uses the heap routines (for example, malloc, _strdup, or _putenv).
Don't use any function that generates a system call (for example, _getcwd or time).
Don't use longjmp unless the interrupt is caused by a floating-point exception (that is, sig is SIGFPE).
Don't use any overlay routines.


This wasn't a big deal before but since we now print timings on ctrl+c exit (1021), we should probably handle the signal properly.
The documentation for SetConsoleCtrlHandler has no such warning and their example code even shows them using printf.
Another advantage to doing it this way is we don't have to reinitialize SIGINT every loop in main like we currently do.
The only downside I see to this is the inclusion of <windows.h>, but fortunately we can cut down on what the header pulls in by defining WIN32_LEAN_AND_MEAN. If this still isn't satisfactory, I can change this to do a little cluster of defines at the top as is done in common.cpp.",3,2
1126,2023-04-22T12:36:01Z,2023-04-29T05:34:41Z,2023-04-29T05:34:41Z,8,808,156,"ignore EOS should apply -inf to EOS logit, new line penalization option, logit bias support (#1024)
New samplers:

locally typical sampling
tail free sampling
frequency and presence penalty
Mirostat & Mirostat v2



🤖 Generated by Copilot at f571806
Summary
🦙📝?🦙🧠?🦙🔧?

This pull request enhances the llama text generation library with new sampling techniques and features, such as logit bias, typicality filtering, frequency and presence penalties, mirostat, and newline penalty. It also updates the examples and the API to use the new sampling functions and structs, and to handle arrays of llama_token_data. It modifies the command line options and the usage message in ./examples/common.cpp to reflect the new parameters and defaults.

We're coding with the llama, the llama of the sea
We're sampling with the logits, the logits are the key
We're adding new features, new features to the gpt_params
We're heaving on the yardarm, on the yardarm, one, two, three

Walkthrough

Implement new sampling techniques and features for llama, such as tail free sampling, frequency and presence penalties, Mirostat sampling, logit bias, and newline penalty (link, link, link, link, link, link, link)
Update the command line options and parameters in the common files to reflect the new sampling techniques and features, and add descriptions and references for them in the usage message (link, link, link, link)
Update the message printed to the standard error stream in the main example to include the values of the new parameters (link)
Modify the existing parameters and sampling logic in the main and save-load-state examples to use the new sampling techniques and features, and the new llama API functions (link, link, link)
Modify the llama_token_data struct to store the logit instead of the plog, and add a new struct and a new function for handling arrays of llama_token_data (link)",7,16
1128,2023-04-22T14:25:30Z,2023-04-24T15:29:58Z,2023-04-24T15:29:58Z,1,2,2,Continued from #1127.  The -Wno-pedantic is necessary to avoid a warning: style of line directive is a GCC extension for every single line in the compilation.,6,7
1131,2023-04-22T18:45:05Z,2023-04-24T15:45:33Z,2023-04-24T15:45:33Z,5,18,10,"I have tried to take a first stab at writing a brief README for main.exe ( #518).
While trying to untangle the --instruct and --interactive behaviour, I noticed a small inconsistency in the naming of internal parameters: There is params.interactive_start, while the command line parameter is called --interactive_first, and there is also an --interactive_start that, confusingly, does the same as --interactive.
So, I added a small refactoring change to rename the structure member for consistency.
Feedback is very welcome. If this looks reasonable, I can also try the same at least for perplexity and embeddings.",5,6
1139,2023-04-23T10:06:03Z,2023-04-23T15:37:02Z,2023-04-23T15:37:03Z,1,180,2,I've been slowly working on this. It contains a couple of examples and a longer explanation of the options a user might use.,4,7
1143,2023-04-23T15:57:53Z,2023-04-24T04:40:02Z,2023-04-24T04:40:02Z,2,179,154,"Normalize the code style
Move the definitions at the correct place in llama.cpp
Retire llama_get_kv_cache(), llama_get_kv_cache_size() and llama_set_kv_cache()

Not sure how to test this - maybe we need to add an example, or extend main with store/load state functionality",3,1
1145,2023-04-23T19:34:28Z,2023-04-23T21:03:45Z,2023-04-23T21:03:45Z,1,1,1,https://arxiv.org/abs/2106.09685,2,0
1150,2023-04-24T03:38:01Z,2023-04-24T16:23:31Z,2023-04-24T16:23:31Z,3,133,0,"This is the save_load_state example script from #730 (comment)
It will load model params read from command line.
Then evaluate and print the initial prompt and save the state.
n_predict tokens are generate and printed.
Model is freed and load in new context.
Load the state and generate and print n_predict.
The n_predict generated tokens should be the same.",2,3
1162,2023-04-24T19:41:30Z,2023-04-24T21:02:02Z,2023-04-24T21:02:02Z,1,6,1,"I was trying the test-grad0 test from ggml and this way I found a bug in ggml_compute_forward_sum_f32.
Only the sum of the last processed row was written to dst:

  
    
      llama.cpp/ggml.c
    
    
        Lines 3201 to 3210
      in
      8a0f867
    
  
  
    

        
          
           inline static void ggml_vec_sum_f32(const int n, float * s, const float * x) { 
        

        
          
           #ifndef GGML_USE_ACCELERATE 
        

        
          
               ggml_float sum = 0.0; 
        

        
          
               for (int i = 0; i < n; ++i) { 
        

        
          
                   sum += (ggml_float)x[i]; 
        

        
          
               } 
        

        
          
               *s = sum; 
        

        
          
           #else 
        

        
          
               vDSP_sve(x, 1, s, n); 
        

        
          
           #endif 
        
    
  



  
    
      llama.cpp/ggml.c
    
    
        Lines 6782 to 6790
      in
      8a0f867
    
  
  
    

        
          
           for (int64_t i03 = 0; i03 < ne03; i03++) { 
        

        
          
               for (int64_t i02 = 0; i02 < ne02; i02++) { 
        

        
          
                   for (int64_t i01 = 0; i01 < ne01; i01++) { 
        

        
          
                       ggml_vec_sum_f32(ne00, 
        

        
          
                               (float *) (dst->data), 
        

        
          
                               (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03)); 
        

        
          
                   } 
        

        
          
               } 
        

        
          
           } 
        
    
  


With this PR the sum over all rows is accumulated and then written to dst.
What is not so nice about it, is that the row sum is converted from ggml_float to float, losing precision during accumulation.

Was computing single sum over all rows like I did meant by this TODO?

  
    
      llama.cpp/ggml.h
    
    
        Lines 486 to 489
      in
      8a0f867
    
  
  
    

        
          
           // TODO: compute sum along rows 
        

        
          
           GGML_API struct ggml_tensor * ggml_sum( 
        

        
          
                   struct ggml_context * ctx, 
        

        
          
                   struct ggml_tensor  * a);",2,1
1164,2023-04-24T20:17:44Z,2023-04-28T14:57:17Z,2023-04-28T14:57:17Z,8,411,16,"Add CLBlast support as an alternative to CuBLAS to speed up context processing.
The advantage of CLBlast over CuBLAS is that it is vendor-agnostic, it runs on basically any GPU (even some phones). It is also a much smaller library than proprietary CuBLAS, while managing to be nearly as fast.
Resolves #1059",9,27
1168,2023-04-25T03:00:02Z,2023-04-28T16:13:33Z,2023-04-28T16:13:33Z,6,312,0,"Closes #1163
This is pretty much just a straight port of aigoopy/llm-jeopardy/
Leaving as a draft since it's still missing a lot of features, and I will continue to work on it to make it more usable.",3,6
1169,2023-04-25T03:09:44Z,2023-04-28T15:59:37Z,2023-04-28T15:59:38Z,6,156,2,"Hi! I decided to take a stab at leveraging the new get / set state APIs to cache initial prompt evaluation in main. On my M2 at least, this feature lets me start up chat-13B.sh with 65B in seconds (after having run before).
Overview

Adds llama_load_session_file and llama_save_session_file APIs to serialize the model state + a user-provided sequence of input tokens (more on that later)
Adds a --session arg to examples/main that designates a file to load/save the session (creating on first run). Currently this is just used to speed up initial prompt evaluation, but could eventually e.g., restore conversations

Approach
Establishes a binary session file format that prepends some additional metadata to the state returned by llama_copy_state_data.
'ggst' | <u32> 0 | <llama_hparams> | <u32> inp_token_count | <token_count * llama_token> inp_tokens | <llama_state>

The embedded hparams is a sanity check that we don't load the state for a different model. The inp_tokens stream represents the sequence of input tokens whose evaluation led to llama_state.
When a past session is present during model evaluation, the session tokens are used (in examples/main) to determine the matching prefix length between the saved session and the current prompt (and technically input). These are skipped over using n_past. Regular evaluation then continues from the next token onward.
For convenience, a single --session arg in examples/main designates the file to save the session to (creating if needed) and load from on successive calls.
Testing
For interactive sessions, I tested this with examples/chat-13B.sh against quantized 30B and 65B:
examples/chat-13B.sh -m ~/llama-models/30B/ggml-model-q4_0.bin --session chat-session-30B.bin

I also tested the regular, non-session usage.
To measure performance I ran chat-13B.sh, modified to be non-interactive and generate only 10 tokens.
Results
Some rough timing results from my M2 running 30B on the prompt from chat-13B.sh.
Before this feature, ~37s startup:
llama_print_timings:        load time = 34743.04 ms
llama_print_timings:      sample time =    29.21 ms /    10 runs   (    2.92 ms per run)
llama_print_timings: prompt eval time = 34721.95 ms /   508 tokens (   68.35 ms per token)
llama_print_timings:        eval time =  1994.03 ms /     9 runs   (  221.56 ms per run)
llama_print_timings:       total time = 36766.42 ms

After this feature, first run, ~40s startup:
llama_print_timings:        load time = 35040.24 ms
llama_print_timings:      sample time =    29.48 ms /    10 runs   (    2.95 ms per run)
llama_print_timings: prompt eval time = 35024.65 ms /   508 tokens (   68.95 ms per token)
llama_print_timings:        eval time =  2001.31 ms /     9 runs   (  222.37 ms per run)
llama_print_timings:       total time = 39635.71 ms

After this feature, successive runs, ~5s:
llama_print_timings:        load time =  2874.73 ms
llama_print_timings:      sample time =    28.82 ms /    10 runs   (    2.88 ms per run)
llama_print_timings: prompt eval time =  2148.04 ms /    14 tokens (  153.43 ms per token)
llama_print_timings:        eval time =  1753.77 ms /     9 runs   (  194.86 ms per run)
llama_print_timings:       total time =  4657.45 ms

Caveats

I don't have a deep understanding of n_past, just that it can be leveraged for this prefix behavior
Session files are ~GBs large and don't leverage mmap, incurring a slight delay to save/load
The session usage in examples/main is oriented to optimizing initial prompt evaluation time. It uses a heuristic to determine if the session should be (re-)saved, such that loading (near) identitcal prompts doesn't incur the seconds to write the session file",7,8
1170,2023-04-25T07:21:47Z,2023-04-25T21:33:08Z,2023-04-25T21:33:09Z,1,7,2,".0 in the config file for the lora_alpha param
and I got this error
fout.write(struct.pack(""ii"", int(params[""r""]), params[""lora_alpha""]))
struct.error: required argument is not an integer

I just cast",3,7
1173,2023-04-25T14:47:37Z,2023-05-03T01:46:20Z,2023-05-03T01:46:20Z,1,28,1,"In Linux we can use a bashism to inject newlines into the prompt:
./main -m models/7B/ggml-model.bin -n -1 --color -r ""User:"" --in-prefix "" "" --prompt $'User: Hi\nAI: Hello. I am an AI chatbot. Would you like to talk?\nUser: Sure!\nAI: What would you like to talk about?\nUser:'
But in Windows there's simply no way. This patch processes the escape sequence given in prompts.
In Linux and on Macs, it allows us a cleaner way of doing it:
./main -m models/7B/ggml-model.bin -n -1 --color -r ""User:"" --in-prefix "" "" --prompt 'User: Hi\nAI: Hello. I am an AI chatbot. Would you like to talk?\nUser: Sure!\nAI: What would you like to talk about?\nUser:'
And makes the same thing possible in Windows:
main.exe -m models\7B\ggml-model.bin -n -1 --color -r ""User:"" --in-prefix "" "" --prompt ""User: Hi\nAI: Hello. I am an AI chatbot. Would you like to talk?\nUser: Sure!\nAI: What would you like to talk about?\nUser:""",4,9
1179,2023-04-25T18:45:36Z,2023-04-25T20:40:51Z,2023-04-25T20:40:51Z,8,312,147,"8-bit integer quantization support
Perplexity: 5.9563

main: seed = 1682448271
llama.cpp: loading model from ../models/7B/ggml-model-q8_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 7 (mostly Q8_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 7403851.11 KB
llama_model_load_internal: mem required  = 9022.32 MB (+ 1026.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 12 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks, batch_size=512
2.16 seconds per pass - ETA 23 minutes
[1]4.2384,[2]4.7355,[3]5.5889,[4]6.1733,[5]6.3007,[6]6.2664,[7]6.4584,[8]6.5534,[9]6.8821,[10]7.1241,[11]7.3343,[12]7.3544,[13]7.2708,[14]7.3211,[15]7.5611,[16]7.1921,[17]7.0824,[18]7.0290,[19]6.6805,[20]6.6714,[21]6.5799,[22]6.4063,[23]6.3776,[24]6.2865,[25]6.2832,[26]6.1237,[27]5.9534,[28]5.8555,[29]5.7683,[30]5.6163,[31]5.5870,[32]5.6074,[33]5.5519,[34]5.5823,[35]5.6052,[36]5.6417,[37]5.6421,[38]5.6530,[39]5.6856,[40]5.7357,[41]5.7442,[42]5.7817,[43]5.7443,[44]5.8007,[45]5.8034,[46]5.7780,[47]5.7980,[48]5.7735,[49]5.7746,[50]5.7359,[51]5.7319,[52]5.7225,[53]5.7672,[54]5.7516,[55]5.7301,[56]5.7587,[57]5.7784,[58]5.7973,[59]5.8144,[60]5.8550,[61]5.8479,[62]5.9050,[63]5.9359,[64]5.9492,[65]5.9908,[66]5.9990,[67]6.0162,[68]6.0307,[69]6.0542,[70]6.0843,[71]6.1051,[72]6.1363,[73]6.1946,[74]6.1988,[75]6.2121,[76]6.2241,[77]6.2353,[78]6.2208,[79]6.2482,[80]6.2417,[81]6.2527,[82]6.2567,[83]6.2067,[84]6.1891,[85]6.1764,[86]6.1553,[87]6.0913,[88]6.0665,[89]6.0472,[90]6.0334,[91]6.0560,[92]6.0502,[93]6.0510,[94]6.0486,[95]6.0757,[96]6.0756,[97]6.0701,[98]6.0644,[99]6.0516,[100]6.0506,[101]6.0743,[102]6.0695,[103]6.0895,[104]6.0968,[105]6.0968,[106]6.1132,[107]6.1125,[108]6.1257,[109]6.1208,[110]6.1173,[111]6.1394,[112]6.1595,[113]6.1616,[114]6.1578,[115]6.1637,[116]6.1548,[117]6.1596,[118]6.1877,[119]6.2092,[120]6.2432,[121]6.2577,[122]6.2818,[123]6.3180,[124]6.3350,[125]6.3260,[126]6.3639,[127]6.3994,[128]6.4290,[129]6.4143,[130]6.4225,[131]6.4188,[132]6.4116,[133]6.3988,[134]6.4088,[135]6.4048,[136]6.3944,[137]6.3872,[138]6.3698,[139]6.3597,[140]6.3562,[141]6.3275,[142]6.3241,[143]6.2945,[144]6.2745,[145]6.2656,[146]6.2541,[147]6.2576,[148]6.2580,[149]6.2528,[150]6.2486,[151]6.2506,[152]6.2410,[153]6.2254,[154]6.2170,[155]6.2237,[156]6.2190,[157]6.2355,[158]6.2396,[159]6.2440,[160]6.2466,[161]6.2584,[162]6.2308,[163]6.2197,[164]6.1968,[165]6.1670,[166]6.1406,[167]6.1045,[168]6.0748,[169]6.0614,[170]6.0508,[171]6.0248,[172]6.0083,[173]5.9923,[174]5.9631,[175]5.9419,[176]5.9307,[177]5.9112,[178]5.8890,[179]5.8725,[180]5.8632,[181]5.8422,[182]5.8248,[183]5.8115,[184]5.8107,[185]5.8036,[186]5.8047,[187]5.8108,[188]5.8071,[189]5.8239,[190]5.8247,[191]5.8453,[192]5.8610,[193]5.8772,[194]5.8879,[195]5.9087,[196]5.9240,[197]5.9445,[198]5.9593,[199]5.9623,[200]5.9671,[201]5.9619,[202]5.9801,[203]5.9872,[204]5.9855,[205]5.9956,[206]6.0024,[207]5.9986,[208]6.0069,[209]6.0108,[210]6.0159,[211]6.0265,[212]6.0334,[213]6.0436,[214]6.0458,[215]6.0483,[216]6.0622,[217]6.0800,[218]6.0927,[219]6.0925,[220]6.0890,[221]6.0840,[222]6.0818,[223]6.0725,[224]6.0655,[225]6.0617,[226]6.0818,[227]6.0896,[228]6.0948,[229]6.1008,[230]6.0976,[231]6.1139,[232]6.1026,[233]6.0866,[234]6.0722,[235]6.0523,[236]6.0459,[237]6.0366,[238]6.0393,[239]6.0249,[240]6.0151,[241]6.0169,[242]6.0206,[243]6.0189,[244]6.0079,[245]6.0050,[246]5.9942,[247]5.9829,[248]5.9759,[249]5.9735,[250]5.9781,[251]5.9713,[252]5.9681,[253]5.9587,[254]5.9536,[255]5.9429,[256]5.9255,[257]5.9139,[258]5.9060,[259]5.9038,[260]5.8959,[261]5.8918,[262]5.8864,[263]5.8813,[264]5.8592,[265]5.8587,[266]5.8569,[267]5.8505,[268]5.8591,[269]5.8572,[270]5.8583,[271]5.8658,[272]5.8691,[273]5.8694,[274]5.8719,[275]5.8799,[276]5.8858,[277]5.9012,[278]5.9110,[279]5.9202,[280]5.9230,[281]5.9326,[282]5.9383,[283]5.9527,[284]5.9605,[285]5.9688,[286]5.9823,[287]5.9818,[288]5.9875,[289]5.9795,[290]5.9642,[291]5.9497,[292]5.9353,[293]5.9224,[294]5.9246,[295]5.9238,[296]5.9284,[297]5.9271,[298]5.9300,[299]5.9276,[300]5.9171,[301]5.9172,[302]5.9096,[303]5.9013,[304]5.8931,[305]5.8897,[306]5.8775,[307]5.8797,[308]5.8827,[309]5.8674,[310]5.8621,[311]5.8559,[312]5.8581,[313]5.8526,[314]5.8510,[315]5.8357,[316]5.8306,[317]5.8148,[318]5.7952,[319]5.8067,[320]5.8187,[321]5.8231,[322]5.8192,[323]5.8126,[324]5.8099,[325]5.8199,[326]5.8201,[327]5.8222,[328]5.8260,[329]5.8318,[330]5.8344,[331]5.8465,[332]5.8437,[333]5.8504,[334]5.8451,[335]5.8393,[336]5.8430,[337]5.8409,[338]5.8402,[339]5.8352,[340]5.8311,[341]5.8389,[342]5.8417,[343]5.8464,[344]5.8465,[345]5.8470,[346]5.8446,[347]5.8487,[348]5.8520,[349]5.8543,[350]5.8511,[351]5.8520,[352]5.8520,[353]5.8463,[354]5.8464,[355]5.8514,[356]5.8544,[357]5.8510,[358]5.8598,[359]5.8624,[360]5.8591,[361]5.8587,[362]5.8656,[363]5.8765,[364]5.8824,[365]5.8875,[366]5.8888,[367]5.8972,[368]5.8949,[369]5.8958,[370]5.8972,[371]5.8921,[372]5.8968,[373]5.9013,[374]5.8998,[375]5.9000,[376]5.9065,[377]5.9022,[378]5.9049,[379]5.9106,[380]5.9029,[381]5.8996,[382]5.8946,[383]5.8940,[384]5.8935,[385]5.8925,[386]5.8920,[387]5.8919,[388]5.8883,[389]5.8833,[390]5.8766,[391]5.8692,[392]5.8654,[393]5.8638,[394]5.8663,[395]5.8651,[396]5.8581,[397]5.8649,[398]5.8686,[399]5.8762,[400]5.8764,[401]5.8777,[402]5.8787,[403]5.8806,[404]5.8870,[405]5.8776,[406]5.8744,[407]5.8740,[408]5.8757,[409]5.8869,[410]5.8976,[411]5.9087,[412]5.9241,[413]5.9349,[414]5.9423,[415]5.9477,[416]5.9553,[417]5.9671,[418]5.9705,[419]5.9771,[420]5.9857,[421]5.9970,[422]6.0010,[423]6.0080,[424]6.0184,[425]6.0268,[426]6.0331,[427]6.0375,[428]6.0456,[429]6.0505,[430]6.0586,[431]6.0723,[432]6.0760,[433]6.0753,[434]6.0713,[435]6.0722,[436]6.0747,[437]6.0841,[438]6.0914,[439]6.0883,[440]6.0875,[441]6.0826,[442]6.0811,[443]6.0824,[444]6.0829,[445]6.0811,[446]6.0834,[447]6.0863,[448]6.0904,[449]6.0881,[450]6.0889,[451]6.0850,[452]6.0716,[453]6.0632,[454]6.0576,[455]6.0586,[456]6.0632,[457]6.0651,[458]6.0630,[459]6.0636,[460]6.0720,[461]6.0693,[462]6.0679,[463]6.0717,[464]6.0706,[465]6.0680,[466]6.0604,[467]6.0606,[468]6.0603,[469]6.0623,[470]6.0627,[471]6.0580,[472]6.0623,[473]6.0572,[474]6.0583,[475]6.0523,[476]6.0539,[477]6.0468,[478]6.0457,[479]6.0512,[480]6.0556,[481]6.0573,[482]6.0530,[483]6.0489,[484]6.0509,[485]6.0488,[486]6.0431,[487]6.0428,[488]6.0405,[489]6.0359,[490]6.0336,[491]6.0307,[492]6.0252,[493]6.0226,[494]6.0209,[495]6.0204,[496]6.0166,[497]6.0111,[498]6.0094,[499]6.0053,[500]5.9962,[501]5.9897,[502]5.9899,[503]5.9893,[504]5.9808,[505]5.9829,[506]5.9837,[507]5.9780,[508]5.9741,[509]5.9735,[510]5.9769,[511]5.9815,[512]5.9849,[513]5.9869,[514]5.9930,[515]5.9877,[516]5.9867,[517]5.9878,[518]5.9875,[519]5.9904,[520]5.9929,[521]5.9941,[522]5.9967,[523]5.9974,[524]6.0030,[525]6.0061,[526]6.0070,[527]6.0088,[528]6.0038,[529]6.0043,[530]5.9995,[531]5.9984,[532]6.0029,[533]6.0052,[534]6.0036,[535]6.0057,[536]6.0004,[537]5.9984,[538]6.0033,[539]6.0044,[540]6.0080,[541]6.0083,[542]6.0094,[543]6.0110,[544]6.0121,[545]6.0102,[546]6.0110,[547]6.0070,[548]6.0024,[549]6.0025,[550]5.9996,[551]5.9963,[552]5.9941,[553]5.9906,[554]5.9886,[555]5.9856,[556]5.9852,[557]5.9875,[558]5.9837,[559]5.9834,[560]5.9833,[561]5.9835,[562]5.9814,[563]5.9810,[564]5.9853,[565]5.9873,[566]5.9871,[567]5.9850,[568]5.9856,[569]5.9843,[570]5.9871,[571]5.9876,[572]5.9886,[573]5.9886,[574]5.9851,[575]5.9844,[576]5.9843,[577]5.9829,[578]5.9811,[579]5.9816,[580]5.9753,[581]5.9717,[582]5.9707,[583]5.9715,[584]5.9718,[585]5.9644,[586]5.9577,[587]5.9583,[588]5.9631,[589]5.9682,[590]5.9712,[591]5.9733,[592]5.9721,[593]5.9690,[594]5.9700,[595]5.9677,[596]5.9709,[597]5.9689,[598]5.9660,[599]5.9681,[600]5.9676,[601]5.9661,[602]5.9670,[603]5.9697,[604]5.9705,[605]5.9739,[606]5.9759,[607]5.9742,[608]5.9710,[609]5.9718,[610]5.9753,[611]5.9736,[612]5.9762,[613]5.9727,[614]5.9678,[615]5.9608,[616]5.9635,[617]5.9577,[618]5.9530,[619]5.9478,[620]5.9345,[621]5.9280,[622]5.9264,[623]5.9280,[624]5.9285,[625]5.9287,[626]5.9276,[627]5.9298,[628]5.9299,[629]5.9295,[630]5.9327,[631]5.9382,[632]5.9438,[633]5.9424,[634]5.9458,[635]5.9464,[636]5.9431,[637]5.9396,[638]5.9420,[639]5.9390,[640]5.9399,[641]5.9401,[642]5.9466,[643]5.9487,[644]5.9499,[645]5.9481,[646]5.9520,[647]5.9480,[648]5.9489,[649]5.9492,[650]5.9529,[651]5.9581,[652]5.9592,[653]5.9631,[654]5.9569,[655]5.9563,
llama_print_timings:        load time =  5233.52 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1351323.55 ms / 335360 tokens (    4.03 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1385072.75 ms",2,6
1181,2023-04-25T19:10:29Z,2023-04-25T21:41:56Z,2023-04-25T21:41:56Z,1,8,8,"#729 changed the quantization output for Q4_0 and Q4_2. We should update SHA256SUMS so as not to confuse people.
I don't have the 65B model lying around, would be great if someone could provide that and check my changes.",3,2
1183,2023-04-25T22:24:12Z,2023-04-26T20:03:04Z,2023-04-26T20:03:04Z,1,78,7,This pull request adds clear instructions about how to build llama.cpp on every platform with and without BLAS support.,4,15
1184,2023-04-25T23:24:13Z,2023-04-26T20:08:43Z,2023-04-26T20:08:43Z,2,10,0,The llama_set_state_data function restores the rng state to what it was at the time llama_copy_state_data was called. But users may want to restore the state and proceed with a different seed.,4,4
1187,2023-04-26T07:39:03Z,2023-04-26T20:14:13Z,2023-04-26T20:14:13Z,8,711,30,"Follow up on the idea by @ikawrakow in #729 (comment)
Q5_0
#define QK5_0 32
typedef struct {
    ggml_fp16_t d;          // delta
    uint8_t qh[4];          // 5-th bit of quants (uint32_t)
    uint8_t qs[QK5_0 / 2];  // nibbles / quants
} block_q5_0;
On M1 Pro, it evaluates at about 53 ms / token for 7B model
This format is bigger than Q4_0 and Q4_2.
Perplexity for 7B: 6.0139

main: seed = 1682523351
llama.cpp: loading model from ../models/7B/ggml-model-q5_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 8 (mostly Q5_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4525003.11 KB
llama_model_load_internal: mem required  = 6210.95 MB (+ 1026.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 12 / 64 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks, batch_size=512
1.90 seconds per pass - ETA 20 minutes
[1]4.2484,[2]4.7547,[3]5.6316,[4]6.2345,[5]6.3575,[6]6.3361,[7]6.5288,[8]6.6259,[9]6.9688,[10]7.2116,[11]7.4185,[12]7.4477,[13]7.3615,[14]7.4028,[15]7.6442,[16]7.2662,[17]7.1544,[18]7.1013,[19]6.7447,[20]6.7341,[21]6.6423,[22]6.4727,[23]6.4417,[24]6.3490,[25]6.3524,[26]6.1948,[27]6.0225,[28]5.9267,[29]5.8384,[30]5.6840,[31]5.6553,[32]5.6751,[33]5.6167,[34]5.6509,[35]5.6727,[36]5.7108,[37]5.7162,[38]5.7249,[39]5.7569,[40]5.8063,[41]5.8181,[42]5.8563,[43]5.8172,[44]5.8757,[45]5.8774,[46]5.8511,[47]5.8708,[48]5.8464,[49]5.8466,[50]5.8085,[51]5.8044,[52]5.7966,[53]5.8401,[54]5.8246,[55]5.8020,[56]5.8319,[57]5.8511,[58]5.8723,[59]5.8888,[60]5.9307,[61]5.9239,[62]5.9817,[63]6.0119,[64]6.0250,[65]6.0682,[66]6.0762,[67]6.0930,[68]6.1060,[69]6.1291,[70]6.1593,[71]6.1827,[72]6.2131,[73]6.2709,[74]6.2743,[75]6.2872,[76]6.2990,[77]6.3104,[78]6.2966,[79]6.3240,[80]6.3168,[81]6.3290,[82]6.3332,[83]6.2818,[84]6.2637,[85]6.2520,[86]6.2305,[87]6.1659,[88]6.1399,[89]6.1208,[90]6.1060,[91]6.1289,[92]6.1233,[93]6.1227,[94]6.1208,[95]6.1480,[96]6.1481,[97]6.1437,[98]6.1379,[99]6.1246,[100]6.1235,[101]6.1472,[102]6.1415,[103]6.1609,[104]6.1672,[105]6.1674,[106]6.1840,[107]6.1833,[108]6.1953,[109]6.1898,[110]6.1861,[111]6.2078,[112]6.2281,[113]6.2300,[114]6.2262,[115]6.2329,[116]6.2228,[117]6.2279,[118]6.2566,[119]6.2776,[120]6.3119,[121]6.3264,[122]6.3510,[123]6.3874,[124]6.4045,[125]6.3951,[126]6.4335,[127]6.4703,[128]6.5000,[129]6.4848,[130]6.4939,[131]6.4899,[132]6.4836,[133]6.4700,[134]6.4798,[135]6.4761,[136]6.4651,[137]6.4581,[138]6.4401,[139]6.4302,[140]6.4270,[141]6.3973,[142]6.3939,[143]6.3640,[144]6.3438,[145]6.3341,[146]6.3221,[147]6.3254,[148]6.3252,[149]6.3196,[150]6.3152,[151]6.3173,[152]6.3077,[153]6.2910,[154]6.2824,[155]6.2890,[156]6.2838,[157]6.3001,[158]6.3039,[159]6.3088,[160]6.3113,[161]6.3228,[162]6.2941,[163]6.2831,[164]6.2598,[165]6.2292,[166]6.2024,[167]6.1659,[168]6.1355,[169]6.1222,[170]6.1113,[171]6.0850,[172]6.0680,[173]6.0515,[174]6.0219,[175]6.0007,[176]5.9895,[177]5.9700,[178]5.9476,[179]5.9303,[180]5.9207,[181]5.8998,[182]5.8821,[183]5.8682,[184]5.8678,[185]5.8605,[186]5.8607,[187]5.8668,[188]5.8631,[189]5.8800,[190]5.8808,[191]5.9013,[192]5.9171,[193]5.9332,[194]5.9440,[195]5.9652,[196]5.9808,[197]6.0014,[198]6.0161,[199]6.0190,[200]6.0240,[201]6.0190,[202]6.0373,[203]6.0446,[204]6.0430,[205]6.0534,[206]6.0602,[207]6.0560,[208]6.0648,[209]6.0689,[210]6.0739,[211]6.0842,[212]6.0916,[213]6.1022,[214]6.1043,[215]6.1072,[216]6.1210,[217]6.1388,[218]6.1515,[219]6.1514,[220]6.1479,[221]6.1431,[222]6.1408,[223]6.1310,[224]6.1242,[225]6.1201,[226]6.1407,[227]6.1492,[228]6.1545,[229]6.1608,[230]6.1582,[231]6.1744,[232]6.1626,[233]6.1464,[234]6.1317,[235]6.1126,[236]6.1058,[237]6.0962,[238]6.0987,[239]6.0844,[240]6.0742,[241]6.0768,[242]6.0802,[243]6.0784,[244]6.0674,[245]6.0641,[246]6.0532,[247]6.0416,[248]6.0345,[249]6.0322,[250]6.0368,[251]6.0298,[252]6.0264,[253]6.0170,[254]6.0116,[255]6.0000,[256]5.9825,[257]5.9702,[258]5.9622,[259]5.9603,[260]5.9523,[261]5.9478,[262]5.9425,[263]5.9367,[264]5.9148,[265]5.9142,[266]5.9126,[267]5.9060,[268]5.9154,[269]5.9131,[270]5.9141,[271]5.9219,[272]5.9253,[273]5.9252,[274]5.9277,[275]5.9362,[276]5.9423,[277]5.9579,[278]5.9679,[279]5.9771,[280]5.9799,[281]5.9897,[282]5.9957,[283]6.0103,[284]6.0183,[285]6.0268,[286]6.0399,[287]6.0392,[288]6.0455,[289]6.0373,[290]6.0221,[291]6.0074,[292]5.9927,[293]5.9794,[294]5.9817,[295]5.9804,[296]5.9848,[297]5.9835,[298]5.9864,[299]5.9839,[300]5.9735,[301]5.9736,[302]5.9659,[303]5.9570,[304]5.9485,[305]5.9450,[306]5.9325,[307]5.9347,[308]5.9381,[309]5.9224,[310]5.9169,[311]5.9105,[312]5.9130,[313]5.9078,[314]5.9061,[315]5.8907,[316]5.8854,[317]5.8697,[318]5.8496,[319]5.8614,[320]5.8732,[321]5.8779,[322]5.8741,[323]5.8675,[324]5.8646,[325]5.8744,[326]5.8745,[327]5.8768,[328]5.8806,[329]5.8864,[330]5.8888,[331]5.9009,[332]5.8980,[333]5.9046,[334]5.8992,[335]5.8932,[336]5.8969,[337]5.8943,[338]5.8933,[339]5.8882,[340]5.8840,[341]5.8921,[342]5.8947,[343]5.8994,[344]5.8996,[345]5.9001,[346]5.8977,[347]5.9012,[348]5.9044,[349]5.9067,[350]5.9034,[351]5.9042,[352]5.9046,[353]5.8989,[354]5.8991,[355]5.9041,[356]5.9069,[357]5.9036,[358]5.9126,[359]5.9150,[360]5.9116,[361]5.9112,[362]5.9180,[363]5.9290,[364]5.9354,[365]5.9405,[366]5.9415,[367]5.9496,[368]5.9472,[369]5.9480,[370]5.9495,[371]5.9441,[372]5.9489,[373]5.9536,[374]5.9518,[375]5.9520,[376]5.9588,[377]5.9543,[378]5.9570,[379]5.9628,[380]5.9551,[381]5.9519,[382]5.9471,[383]5.9465,[384]5.9459,[385]5.9449,[386]5.9444,[387]5.9443,[388]5.9407,[389]5.9354,[390]5.9286,[391]5.9210,[392]5.9171,[393]5.9157,[394]5.9183,[395]5.9171,[396]5.9099,[397]5.9167,[398]5.9206,[399]5.9285,[400]5.9288,[401]5.9302,[402]5.9312,[403]5.9331,[404]5.9394,[405]5.9302,[406]5.9272,[407]5.9267,[408]5.9283,[409]5.9398,[410]5.9506,[411]5.9615,[412]5.9771,[413]5.9875,[414]5.9950,[415]6.0003,[416]6.0078,[417]6.0197,[418]6.0234,[419]6.0302,[420]6.0391,[421]6.0505,[422]6.0545,[423]6.0617,[424]6.0719,[425]6.0805,[426]6.0869,[427]6.0912,[428]6.0997,[429]6.1048,[430]6.1131,[431]6.1270,[432]6.1308,[433]6.1302,[434]6.1262,[435]6.1271,[436]6.1297,[437]6.1392,[438]6.1467,[439]6.1436,[440]6.1426,[441]6.1377,[442]6.1362,[443]6.1377,[444]6.1378,[445]6.1361,[446]6.1386,[447]6.1417,[448]6.1458,[449]6.1432,[450]6.1442,[451]6.1402,[452]6.1267,[453]6.1184,[454]6.1129,[455]6.1138,[456]6.1184,[457]6.1204,[458]6.1181,[459]6.1187,[460]6.1272,[461]6.1247,[462]6.1232,[463]6.1274,[464]6.1262,[465]6.1234,[466]6.1157,[467]6.1158,[468]6.1155,[469]6.1175,[470]6.1179,[471]6.1131,[472]6.1174,[473]6.1121,[474]6.1135,[475]6.1075,[476]6.1092,[477]6.1020,[478]6.1010,[479]6.1070,[480]6.1113,[481]6.1133,[482]6.1088,[483]6.1046,[484]6.1065,[485]6.1049,[486]6.0994,[487]6.0992,[488]6.0971,[489]6.0926,[490]6.0904,[491]6.0875,[492]6.0820,[493]6.0792,[494]6.0777,[495]6.0774,[496]6.0738,[497]6.0683,[498]6.0665,[499]6.0624,[500]6.0532,[501]6.0467,[502]6.0470,[503]6.0463,[504]6.0378,[505]6.0400,[506]6.0406,[507]6.0350,[508]6.0310,[509]6.0304,[510]6.0339,[511]6.0384,[512]6.0419,[513]6.0439,[514]6.0502,[515]6.0448,[516]6.0439,[517]6.0446,[518]6.0445,[519]6.0473,[520]6.0499,[521]6.0512,[522]6.0538,[523]6.0544,[524]6.0600,[525]6.0632,[526]6.0643,[527]6.0661,[528]6.0610,[529]6.0615,[530]6.0564,[531]6.0551,[532]6.0596,[533]6.0619,[534]6.0606,[535]6.0629,[536]6.0575,[537]6.0554,[538]6.0603,[539]6.0614,[540]6.0651,[541]6.0654,[542]6.0666,[543]6.0681,[544]6.0691,[545]6.0673,[546]6.0682,[547]6.0641,[548]6.0594,[549]6.0594,[550]6.0565,[551]6.0532,[552]6.0513,[553]6.0478,[554]6.0457,[555]6.0427,[556]6.0423,[557]6.0445,[558]6.0410,[559]6.0407,[560]6.0405,[561]6.0408,[562]6.0384,[563]6.0380,[564]6.0423,[565]6.0443,[566]6.0443,[567]6.0423,[568]6.0427,[569]6.0415,[570]6.0443,[571]6.0446,[572]6.0457,[573]6.0458,[574]6.0425,[575]6.0419,[576]6.0417,[577]6.0402,[578]6.0383,[579]6.0389,[580]6.0326,[581]6.0291,[582]6.0280,[583]6.0288,[584]6.0291,[585]6.0216,[586]6.0149,[587]6.0154,[588]6.0202,[589]6.0255,[590]6.0285,[591]6.0305,[592]6.0295,[593]6.0265,[594]6.0274,[595]6.0252,[596]6.0284,[597]6.0265,[598]6.0236,[599]6.0257,[600]6.0253,[601]6.0240,[602]6.0252,[603]6.0281,[604]6.0290,[605]6.0323,[606]6.0345,[607]6.0328,[608]6.0295,[609]6.0304,[610]6.0339,[611]6.0321,[612]6.0346,[613]6.0311,[614]6.0262,[615]6.0191,[616]6.0218,[617]6.0160,[618]6.0112,[619]6.0058,[620]5.9924,[621]5.9857,[622]5.9840,[623]5.9856,[624]5.9860,[625]5.9861,[626]5.9850,[627]5.9872,[628]5.9874,[629]5.9869,[630]5.9900,[631]5.9954,[632]6.0009,[633]5.9995,[634]6.0030,[635]6.0036,[636]6.0002,[637]5.9969,[638]5.9994,[639]5.9963,[640]5.9972,[641]5.9975,[642]6.0040,[643]6.0063,[644]6.0075,[645]6.0056,[646]6.0096,[647]6.0059,[648]6.0067,[649]6.0068,[650]6.0106,[651]6.0159,[652]6.0168,[653]6.0206,[654]6.0144,[655]6.0139,
llama_print_timings:        load time =  4416.87 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1163995.97 ms / 335360 tokens (    3.47 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1198461.81 ms


Q5_1
#define QK5_1 32
typedef struct {
    ggml_fp16_t d;          // delta
    ggml_fp16_t m;          // min
    uint32_t qh;            // 5-th bit of quants
    uint8_t qs[QK5_1 / 2];  // nibbles / quants
} block_q5_1;
This format is the same size as Q4_1 and Q4_3.
On M1 Pro, it evaluates at about 55 ms / token for 7B model
The AVX implementation might make use of the following trick: https://stackoverflow.com/a/24242696
Perplexity for 7B: 5.9934

main: seed = 1682491079
llama.cpp: loading model from ../models/7B/ggml-model-q5_0.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 8 (mostly Q5_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4936267.11 KB
llama_model_load_internal: mem required  = 6612.57 MB (+ 1026.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 12 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity : calculating perplexity over 655 chunks, batch_size=512
4.47 seconds per pass - ETA 48 minutes
[1]4.2726,[2]4.7565,[3]5.6331,[4]6.2042,[5]6.3451,[6]6.3059,[7]6.4909,[8]6.5871,[9]6.9243,[10]7.1597,[11]7.3774,[12]7.4015,[13]7.3209,[14]7.3676,[15]7.6199,[16]7.2420,[17]7.1286,[18]7.0729,[19]6.7181,[20]6.7082,[21]6.6191,[22]6.4438,[23]6.4184,[24]6.3280,[25]6.3274,[26]6.1686,[27]5.9965,[28]5.8979,[29]5.8120,[30]5.6595,[31]5.6332,[32]5.6517,[33]5.5956,[34]5.6265,[35]5.6486,[36]5.6873,[37]5.6899,[38]5.7015,[39]5.7330,[40]5.7819,[41]5.7887,[42]5.8273,[43]5.7886,[44]5.8450,[45]5.8481,[46]5.8224,[47]5.8428,[48]5.8164,[49]5.8186,[50]5.7792,[51]5.7755,[52]5.7657,[53]5.8109,[54]5.7964,[55]5.7747,[56]5.8023,[57]5.8232,[58]5.8428,[59]5.8607,[60]5.9020,[61]5.8953,[62]5.9527,[63]5.9840,[64]5.9978,[65]6.0403,[66]6.0480,[67]6.0658,[68]6.0795,[69]6.1037,[70]6.1335,[71]6.1559,[72]6.1870,[73]6.2448,[74]6.2483,[75]6.2627,[76]6.2745,[77]6.2864,[78]6.2724,[79]6.3003,[80]6.2942,[81]6.3078,[82]6.3123,[83]6.2612,[84]6.2434,[85]6.2310,[86]6.2091,[87]6.1446,[88]6.1200,[89]6.1001,[90]6.0861,[91]6.1102,[92]6.1045,[93]6.1043,[94]6.1014,[95]6.1292,[96]6.1288,[97]6.1234,[98]6.1171,[99]6.1039,[100]6.1026,[101]6.1260,[102]6.1220,[103]6.1422,[104]6.1490,[105]6.1488,[106]6.1662,[107]6.1657,[108]6.1787,[109]6.1732,[110]6.1700,[111]6.1917,[112]6.2121,[113]6.2146,[114]6.2101,[115]6.2159,[116]6.2056,[117]6.2103,[118]6.2398,[119]6.2614,[120]6.2956,[121]6.3101,[122]6.3337,[123]6.3701,[124]6.3873,[125]6.3786,[126]6.4164,[127]6.4521,[128]6.4821,[129]6.4672,[130]6.4757,[131]6.4718,[132]6.4630,[133]6.4508,[134]6.4598,[135]6.4561,[136]6.4451,[137]6.4376,[138]6.4205,[139]6.4098,[140]6.4064,[141]6.3775,[142]6.3740,[143]6.3440,[144]6.3233,[145]6.3139,[146]6.3020,[147]6.3048,[148]6.3045,[149]6.2989,[150]6.2941,[151]6.2961,[152]6.2859,[153]6.2701,[154]6.2611,[155]6.2679,[156]6.2632,[157]6.2792,[158]6.2835,[159]6.2884,[160]6.2909,[161]6.3036,[162]6.2761,[163]6.2647,[164]6.2420,[165]6.2117,[166]6.1852,[167]6.1488,[168]6.1189,[169]6.1056,[170]6.0951,[171]6.0693,[172]6.0527,[173]6.0368,[174]6.0077,[175]5.9864,[176]5.9749,[177]5.9553,[178]5.9332,[179]5.9165,[180]5.9070,[181]5.8855,[182]5.8680,[183]5.8547,[184]5.8541,[185]5.8471,[186]5.8478,[187]5.8534,[188]5.8494,[189]5.8663,[190]5.8672,[191]5.8874,[192]5.9032,[193]5.9191,[194]5.9298,[195]5.9514,[196]5.9668,[197]5.9877,[198]6.0027,[199]6.0056,[200]6.0104,[201]6.0051,[202]6.0232,[203]6.0304,[204]6.0287,[205]6.0390,[206]6.0462,[207]6.0426,[208]6.0506,[209]6.0543,[210]6.0596,[211]6.0700,[212]6.0769,[213]6.0873,[214]6.0898,[215]6.0925,[216]6.1063,[217]6.1243,[218]6.1372,[219]6.1368,[220]6.1330,[221]6.1274,[222]6.1253,[223]6.1157,[224]6.1089,[225]6.1052,[226]6.1252,[227]6.1332,[228]6.1387,[229]6.1447,[230]6.1416,[231]6.1583,[232]6.1464,[233]6.1301,[234]6.1153,[235]6.0955,[236]6.0891,[237]6.0797,[238]6.0823,[239]6.0676,[240]6.0576,[241]6.0593,[242]6.0630,[243]6.0612,[244]6.0501,[245]6.0469,[246]6.0357,[247]6.0245,[248]6.0174,[249]6.0149,[250]6.0194,[251]6.0127,[252]6.0091,[253]5.9995,[254]5.9941,[255]5.9830,[256]5.9653,[257]5.9534,[258]5.9457,[259]5.9432,[260]5.9354,[261]5.9313,[262]5.9261,[263]5.9209,[264]5.8991,[265]5.8985,[266]5.8963,[267]5.8899,[268]5.8988,[269]5.8969,[270]5.8974,[271]5.9052,[272]5.9085,[273]5.9088,[274]5.9112,[275]5.9192,[276]5.9254,[277]5.9410,[278]5.9508,[279]5.9598,[280]5.9624,[281]5.9722,[282]5.9780,[283]5.9927,[284]6.0004,[285]6.0087,[286]6.0218,[287]6.0211,[288]6.0267,[289]6.0185,[290]6.0030,[291]5.9883,[292]5.9739,[293]5.9609,[294]5.9629,[295]5.9619,[296]5.9666,[297]5.9652,[298]5.9680,[299]5.9656,[300]5.9551,[301]5.9552,[302]5.9477,[303]5.9390,[304]5.9306,[305]5.9271,[306]5.9146,[307]5.9170,[308]5.9200,[309]5.9045,[310]5.8993,[311]5.8931,[312]5.8954,[313]5.8900,[314]5.8883,[315]5.8731,[316]5.8680,[317]5.8523,[318]5.8324,[319]5.8440,[320]5.8560,[321]5.8602,[322]5.8562,[323]5.8497,[324]5.8470,[325]5.8572,[326]5.8572,[327]5.8595,[328]5.8633,[329]5.8690,[330]5.8718,[331]5.8836,[332]5.8808,[333]5.8874,[334]5.8822,[335]5.8763,[336]5.8801,[337]5.8777,[338]5.8769,[339]5.8718,[340]5.8677,[341]5.8756,[342]5.8786,[343]5.8832,[344]5.8834,[345]5.8837,[346]5.8812,[347]5.8851,[348]5.8883,[349]5.8905,[350]5.8873,[351]5.8881,[352]5.8884,[353]5.8827,[354]5.8831,[355]5.8882,[356]5.8912,[357]5.8877,[358]5.8967,[359]5.8994,[360]5.8959,[361]5.8954,[362]5.9023,[363]5.9135,[364]5.9194,[365]5.9243,[366]5.9256,[367]5.9341,[368]5.9317,[369]5.9326,[370]5.9342,[371]5.9290,[372]5.9336,[373]5.9381,[374]5.9366,[375]5.9368,[376]5.9433,[377]5.9389,[378]5.9416,[379]5.9473,[380]5.9395,[381]5.9361,[382]5.9314,[383]5.9308,[384]5.9304,[385]5.9293,[386]5.9290,[387]5.9288,[388]5.9252,[389]5.9201,[390]5.9134,[391]5.9059,[392]5.9018,[393]5.9004,[394]5.9029,[395]5.9016,[396]5.8946,[397]5.9016,[398]5.9053,[399]5.9129,[400]5.9131,[401]5.9146,[402]5.9158,[403]5.9176,[404]5.9238,[405]5.9143,[406]5.9112,[407]5.9105,[408]5.9121,[409]5.9233,[410]5.9344,[411]5.9455,[412]5.9610,[413]5.9716,[414]5.9790,[415]5.9843,[416]5.9918,[417]6.0035,[418]6.0069,[419]6.0136,[420]6.0222,[421]6.0337,[422]6.0376,[423]6.0445,[424]6.0550,[425]6.0634,[426]6.0697,[427]6.0739,[428]6.0821,[429]6.0871,[430]6.0952,[431]6.1090,[432]6.1126,[433]6.1119,[434]6.1079,[435]6.1090,[436]6.1115,[437]6.1211,[438]6.1284,[439]6.1254,[440]6.1246,[441]6.1199,[442]6.1185,[443]6.1197,[444]6.1202,[445]6.1184,[446]6.1208,[447]6.1238,[448]6.1280,[449]6.1256,[450]6.1265,[451]6.1228,[452]6.1093,[453]6.1006,[454]6.0949,[455]6.0958,[456]6.1004,[457]6.1024,[458]6.1000,[459]6.1005,[460]6.1089,[461]6.1062,[462]6.1049,[463]6.1089,[464]6.1079,[465]6.1052,[466]6.0977,[467]6.0981,[468]6.0979,[469]6.0999,[470]6.1005,[471]6.0958,[472]6.1001,[473]6.0948,[474]6.0960,[475]6.0902,[476]6.0920,[477]6.0848,[478]6.0837,[479]6.0895,[480]6.0941,[481]6.0959,[482]6.0915,[483]6.0873,[484]6.0891,[485]6.0871,[486]6.0815,[487]6.0812,[488]6.0790,[489]6.0743,[490]6.0720,[491]6.0692,[492]6.0636,[493]6.0608,[494]6.0590,[495]6.0584,[496]6.0547,[497]6.0491,[498]6.0474,[499]6.0433,[500]6.0340,[501]6.0274,[502]6.0276,[503]6.0270,[504]6.0184,[505]6.0206,[506]6.0214,[507]6.0157,[508]6.0117,[509]6.0112,[510]6.0145,[511]6.0192,[512]6.0226,[513]6.0245,[514]6.0305,[515]6.0252,[516]6.0243,[517]6.0253,[518]6.0248,[519]6.0278,[520]6.0301,[521]6.0312,[522]6.0338,[523]6.0343,[524]6.0400,[525]6.0431,[526]6.0440,[527]6.0455,[528]6.0406,[529]6.0411,[530]6.0362,[531]6.0350,[532]6.0395,[533]6.0417,[534]6.0399,[535]6.0421,[536]6.0369,[537]6.0349,[538]6.0398,[539]6.0409,[540]6.0446,[541]6.0449,[542]6.0459,[543]6.0475,[544]6.0486,[545]6.0468,[546]6.0478,[547]6.0437,[548]6.0391,[549]6.0390,[550]6.0361,[551]6.0327,[552]6.0306,[553]6.0271,[554]6.0251,[555]6.0221,[556]6.0218,[557]6.0242,[558]6.0206,[559]6.0204,[560]6.0202,[561]6.0205,[562]6.0183,[563]6.0180,[564]6.0224,[565]6.0244,[566]6.0242,[567]6.0220,[568]6.0226,[569]6.0212,[570]6.0240,[571]6.0245,[572]6.0253,[573]6.0253,[574]6.0218,[575]6.0213,[576]6.0213,[577]6.0196,[578]6.0177,[579]6.0181,[580]6.0117,[581]6.0080,[582]6.0070,[583]6.0079,[584]6.0081,[585]6.0007,[586]5.9940,[587]5.9947,[588]5.9994,[589]6.0049,[590]6.0079,[591]6.0100,[592]6.0089,[593]6.0056,[594]6.0066,[595]6.0042,[596]6.0076,[597]6.0054,[598]6.0028,[599]6.0048,[600]6.0044,[601]6.0030,[602]6.0040,[603]6.0069,[604]6.0078,[605]6.0111,[606]6.0132,[607]6.0116,[608]6.0082,[609]6.0091,[610]6.0127,[611]6.0111,[612]6.0137,[613]6.0101,[614]6.0053,[615]5.9983,[616]6.0008,[617]5.9949,[618]5.9903,[619]5.9850,[620]5.9717,[621]5.9650,[622]5.9634,[623]5.9650,[624]5.9655,[625]5.9658,[626]5.9647,[627]5.9670,[628]5.9672,[629]5.9668,[630]5.9699,[631]5.9754,[632]5.9810,[633]5.9795,[634]5.9829,[635]5.9834,[636]5.9800,[637]5.9767,[638]5.9791,[639]5.9760,[640]5.9770,[641]5.9771,[642]5.9836,[643]5.9857,[644]5.9869,[645]5.9851,[646]5.9890,[647]5.9850,[648]5.9860,[649]5.9862,[650]5.9901,[651]5.9952,[652]5.9963,[653]6.0002,[654]5.9940,[655]5.9934,
llama_print_timings:        load time =  6541.18 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2917328.96 ms / 335360 tokens (    8.70 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2951478.31 ms



TODO:

 cuBLAS perplexity
 dot scalar
 dot ARM
 dot AVX",3,2
1196,2023-04-26T19:12:27Z,2023-05-03T17:58:11Z,2023-05-03T17:58:11Z,4,60,30,this is a simple improvement to the chat-13B example which reads prompts from a text file instead of inlining them in the script. i've used this with vicuna 13B with excellent results.,4,4
1203,2023-04-27T07:59:42Z,2023-05-03T15:31:29Z,2023-05-03T15:31:29Z,2,98,12,"feat: add python script to verify the file hashes, the goal is to be platform independent
the output is a table with the filename, valid checksum (if the hash matches), and file missing (if the file is not found).",3,2
1207,2023-04-27T20:10:32Z,2023-04-29T00:04:18Z,2023-04-29T00:04:18Z,6,110,54,"Copying memory to the GPU from pageable memory is slow because it forces CUDA to copy the buffer to non-pageable memory before it can DMA it to the GPU. This also means that cudaMemcpyAsync is actually synchronous.
By storing the ggml context in non-pageable, pinned memory, this additional copy is avoided, and cudaMemcpyAsync is done asynchronously. This also makes it possible to dequantize while copying data for the other matrix.
To observe most of the benefits, this has to be used with --no-mmap, otherwise the weights will be stored in paged, memory mapped memory. With mmap enabled, there is still some benefit from the non-weight matrices. In the future, this will be solved by caching the weights in the GPU memory, avoiding the copy entirely.
To avoid adding a CUDA-only function to the ggml interface, llama.cpp has been modified to include ggml-cuda.h when cuBLAS is enabled.
For me, this represents a ~30% speedup in perplexity times with cuBLAS.
PR:

Master:",4,5
1211,2023-04-28T01:34:32Z,2023-04-28T11:59:48Z,2023-04-28T11:59:48Z,1,18,0,,2,0
1212,2023-04-28T06:38:50Z,2023-04-28T13:40:33Z,2023-04-28T13:40:33Z,1,2,2,"I am using Manjaro and I installed CUDA via the package manager. The install location in my case is /opt/cuda which is also accessible via the environmental variable CUDA_PATH. However, the Makefile only uses the hard-coded location /usr/local/cuda to look up CUDA header files and libraries. Compiling with CUDA therefore does not run out-of-the-box on my machine.
This PR adds the paths that my CUDA install happens to be in to the Makefile. Alternatively it's possible to create a Makefile via cmake which automatically sets the correct include and lib directories for CUDA.",2,0
1215,2023-04-28T14:44:48Z,2023-04-28T23:31:57Z,2023-04-28T23:31:57Z,3,44,11,"This will allow cuBLAS to multiply tensors that are not contiguous in the row or column (I don't think llama has that situation) level by using cudaMemcpy2dAsync.
Testing perplexity right now on a 2080 TI.",2,4
1218,2023-04-28T17:49:05Z,2023-04-28T23:10:43Z,2023-04-28T23:10:43Z,11,21,359,"I hope this isn't too controversial...
Q4_3 turns out to be equal or worse than the Q5 types in all criteria we have: perplexity, file size, token generation speed.
In the interest of reducing code base complexity, remove the Q4_3 type.
It has only been introduced last week I think, so I don't think many people use it. Of course I'm ready to be proven wrong on this...
Notes:

I haven't tested CUDA or OpenCL
GGML_TYPE_COUNT is now somewhat incorrect. I didn't want to change the enum values that are used in model files, but we might move GGML_TYPE_I8 to the now unused value 5.",4,6
1225,2023-04-29T08:59:29Z,2023-04-30T18:34:52Z,2023-04-30T18:34:52Z,2,205,78,"I had or still have an issue with q5_0 that I can't figure out. On Nvidia trying to transfer the quantized weights to the device leads to a CL_OUT_OF_RESOURCES error. On AMD and on POCL it leads to a segfault. It seems to have a problem with 22 byte structs, while 20 or 24 bytes are alright. I am not sure why this is the case.
As a workaround I copy the weights into a new struct and do the FP16 to FP32 conversion on CPU. This seems to have little overhead and works, but it should not be needed. If anyone knows what's up here please let me know.
I also moved the .cl file into the opencl.c as requested.",4,4
1226,2023-04-29T09:45:07Z,2023-04-29T15:43:28Z,2023-04-29T15:43:29Z,3,23,9,Haven't tested this yet. The goal is to allocate just the needed amount of memory when not using cuBLAS,2,1
1229,2023-04-29T11:47:20Z,2023-04-29T15:43:42Z,2023-04-29T15:43:42Z,1,1,1,"We need this check, correct?",2,0
1232,2023-04-29T13:05:47Z,2023-05-01T16:23:47Z,2023-05-01T16:23:47Z,18,185,21,"Description:
This pull request adds a build-info.h header file, which will contain git-related build information such as branch and commit count. This will help users and maintainers to identify the build being used when reporting issues or bugs. The build-info.h file is generated using CMake and Makefile whenever .git/index changes, and is printed to stderr in main.cpp.
Changes:

Modified .gitignore to include build-info.h.
Added build information generation to CMakeLists.txt (and updated main's CMakeLists.txt to add a dependency).
Added build information generation to Makefile.
Modified examples/main/main.cpp to print build information at runtime.

This will allow us to track the build version in a more granular way and may be helpful in identifying and addressing issues or bugs until an official versioning system is introduced.
Edit: In the end this also ended being added to all examples.",7,18
1233,2023-04-29T16:27:34Z,2023-05-01T11:32:23Z,2023-05-01T11:32:23Z,3,51,8,"Fixes #1230
Additionally, adds an environment variable GGML_CUDA_NO_PINNED that can be set to disable all pinned memory usage, which fixes #1231",3,2
1237,2023-04-29T18:13:58Z,2023-07-05T16:13:06Z,2023-07-05T16:13:06Z,9,174,550,"ref: ggerganov/ggml#286
This is hopefully in line with the goal of refactoring in the May roadmap.
In my opinion, quantize_fns is too specific: what matters is not whether a type is quantized, but whether it requires a row-wise conversion, either to vec_dot_type for the vector dot-product, or to float for some other operations.
By making this more generic, some specific code paths for FP16 can be eliminated, most importantly ggml_compute_forward_mul_mat_f16_f32.
This is not finished; CUDA and Accelerate may not work, and also the work buffer allocation may be wrong.",5,16
1251,2023-04-30T09:13:57Z,2023-04-30T18:48:38Z,2023-04-30T18:48:38Z,2,19,4,"flags copied from Makefile
fixes #1210",3,2
1253,2023-04-30T10:23:58Z,2023-04-30T12:32:37Z,2023-04-30T12:32:37Z,5,20,25,"rename C++ source file to .cpp for consistency
add examples/benchmark/CMakeLists.txt
fix out-of-memory issue when enabling BLAS
fix warnings
fix typos",3,0
1259,2023-04-30T22:06:50Z,2023-05-01T16:11:08Z,2023-05-01T16:11:08Z,4,479,258,"Moves all the cuBLAS specific code from ggml.c to ggml-cuda.cu. This also makes ggml-cuda.h much simpler, since fewer definitions have to exposed now.
Additionally, improves mat mul performance by using multiple stream where possible (when multiplying 3 or 4-dimensional tensors), and by choosing between doing f16 x f32 mat muls either as f16 x f16 or as f32 x f32, depending on what requires less data to be transferred to the GPU.
Overall, improves perplexity times with cuBLAS by ~15%.

🤖 Generated by Copilot at 4e54943
Summary
🚀🧹🛠️

This pull request improves the performance, compatibility, and readability of the GGML library and the llama model loader. It refactors the CUDA and BLAS code, simplifies the error checking and memory management, and exposes some useful functions and macros. The main files affected are ggml-cuda.h, ggml.c, ggml.h, llama-util.h, and llama.cpp.

ggml refactored
CUDA and BLAS streamlined
Winter of llama

Walkthrough

Refactored the code for using cuBLAS for matrix multiplication in GGML, by moving the CUDA-related functions and macros to ggml-cuda.h and calling them from ggml.c with conditional compilation (link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link)
Exposed the functions for converting between half-precision and single-precision floating-point numbers as part of the GGML API, by adding their declarations to ggml.h and removing them from ggml.c (link, link)
Moved the macro for asserting conditions from ggml.c to ggml.h, to make it available for other source files that use the GGML library (link, link)
Improved the code style and quality in ggml.c, by removing unused variables, empty lines, and redundant conditional compilation (link, link, link, link, link, link)

From #1233:

Enhanced the llama_buffer and llama_ctx_buffer structs in llama-util.h, by adding default constructors and disabling copy and move constructors and assignment operators, to prevent memory leaks or errors (link, link, link)
Optimized the initialization of temporary buffers in the llama_model_loader struct in llama.cpp, by using the constructor of the std::vector instead of the resize method (link)",2,1
1260,2023-05-01T00:35:39Z,,2023-05-01T10:16:55Z,2,4,3,"I do not have much time trying to try to tie all the magic logic inside examples/main and refactor it, so there is another ""hack"", that needs // REVIEW before merging.",2,2
1261,2023-05-01T03:19:33Z,2023-05-01T07:24:20Z,2023-05-01T07:24:20Z,2,12,12,"It helps clarify thread-safety and ownership in the API, particularly for llama_token_to_str().",2,0
1263,2023-05-01T08:08:54Z,2023-05-01T11:55:00Z,2023-05-01T11:55:00Z,3,96,69,"Moved the llama_load_session_file and llama_save_session_file definitions in the correct place in llama.cpp
Fixed an off-by-one bug during session load
Minor style refactoring


🤖 Generated by Copilot at c0335b5
Summary
🐛🗂️♻️

This pull request enhances the llama library and its example program. It implements a new session file format for llama and updates the llama.h and llama.cpp files accordingly. It also improves the code quality and functionality of the main.cpp example.

main.cpp polished
Llama sessions have new format
Autumn leaves old code

Walkthrough

Add new functions llama_load_session_file and llama_save_session_file to handle session data with a new file format and more error checking (link, link)
Remove old functions llama_load_session_file and llama_save_session_file that used a different file format and had less error handling (link)
Define new macros LLAMA_SESSION_MAGIC and LLAMA_SESSION_VERSION for the session file format and change existing macros LLAMA_FILE_MAGIC and LLAMA_FILE_MAGIC_UNVERSIONED to use character literals (link)
Fix a bug in examples/main/main.cpp where the number of generated tokens was not counted correctly and could cause an infinite loop or a wrong output (link)
Change the error handling and the message in examples/main/main.cpp when loading a session file, and use a boolean check instead of the function return value (link)
Add single quotes around the path_session variable in the message in examples/main/main.cpp for consistency and clarity (link)
Add a space between the type cast and the variable name in examples/main/main.cpp for coding style and readability (link)
Remove the REVIEW comment in examples/main/main.cpp as the decision to stop saving the session when the context is exhausted seems final (link)",2,0
1264,2023-05-01T10:24:48Z,2023-05-01T11:56:07Z,2023-05-01T11:56:07Z,1,1,1,"ggml_used_mem currently blindly dereferences ctx->objects_end, however that pointer is initially NULL and is only set when objects are created.
This tiny change just has it return 0 in that case (presumably if no objects were ever created, no memory is used).",3,0
1266,2023-05-01T11:46:43Z,2023-05-01T12:58:51Z,2023-05-01T12:58:52Z,1,8,2,,3,1
1268,2023-05-01T15:22:00Z,2023-05-02T14:03:00Z,2023-05-02T14:03:00Z,3,67,19,"Adds a name field to ggml_tensor
Adds accessor functions ggml_get_name and ggml_set_name

While we normally don't use accessors, in this case it seemed necessary as the maximum length may change


Modifies ggml_graph_dump_dot to use the names in the graph
Modifies llama.cpp to add names to most tensors

Example graph (heavily inspired by #915 (comment)):",2,0
1271,2023-05-02T00:10:02Z,2023-05-05T20:56:09Z,2023-05-05T20:56:09Z,1,77,0,"This adds a cuBLAS build to the windows ci.
edit: did a test release, please test https://github.com/ggerganov/llama.cpp/releases/tag/ci_cublas-31ff9e2
Open questions:

the cuda dll's are huge, should we ship them? they also don't change (often)

it generates a separate .zip with just the cuda dlls


do we need the blasLt dll? ironically named ""lite"", it's the largest dll (>400mb)

yes


the toolkit install takes ages. I tried using only a select install, but that never worked.
since it takes ages, maybe not require it for merge.
which cuda version to use. I set it to 12.1, but that requires the very latest driver.

I decided on both 11.7.1 and 12.1.0


should we enable shared for all builds, so we distribute the .dll
other stuff i forgot, since the turn around time is >20min, it's a real hell to debug.

eula allowing redist https://docs.nvidia.com/cuda/eula/index.html#attachment-a",2,15
1272,2023-05-02T04:05:57Z,2023-05-03T02:26:14Z,2023-05-03T02:26:14Z,2,80,23,"Per comments in #1169 and #1247, reduces the size of serialized model state and session files by only storing the used portions of the KV cache. This corresponds to the tokens evaluated so far in the session, so the size scales with # of tokens, up to max state size at full context.
Changes

llama_copy_state_data and llama_set_state_data now use the number of evaluated tokens (as in llama_get_kv_cache_token_count) to (de)compact the KV cache data. As a result, while llama_get_state_size is unchanged, its semantics differ:  it is now the maximum buffer size required by the get / set state API
session file version has been bumped and existing session files will be invalid

Testing

Tested with a small prompt and chat-13B on 30B and 65B. Ran cold and warm and ensured same output.

./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 -p 'The meaning of life is 4' --session meaning-life-is.30.v1.bin -n 10
./examples/chat-13B.sh -m ~/llama-models/30B/ggml-model-q4_0.bin --session chat-session-30.v1.bin --seed 1


Tested examples/save-load-state

Results
Original sizes:
% du -hs *.bin
3.1G	chat-session-30.bin
5.0G	chat-session-65.bin

New sizes:
782M	chat-session-30.v1.bin
1.2G	chat-session-65.v1.bin
 12M	meaning-life-is.30.v1.bin
 20M	meaning-life-is.65.v1.bin",3,1
1275,2023-05-02T08:33:14Z,2023-05-02T16:23:44Z,2023-05-02T16:23:44Z,7,9,9,"From reading the code it looks like an oversight but the number zero '0' is a valid value to seed a random number generator. A value I use in my tests. There is no need to test if ""equal to"" anymore.",2,0
1276,2023-05-02T09:50:14Z,2023-05-08T11:54:26Z,2023-05-08T11:54:26Z,1,7,3,"When loading a safetensors file, ignore the metadata header.
If no pt or pth files are available, attempt to load safetensors files.
Edit: This has been changed to try to load safetensors files first, and only load the pt/pth/bin file if those aren't available.",2,0
1277,2023-05-02T11:50:11Z,2023-05-07T11:20:09Z,2023-05-07T11:20:09Z,1,65,8,"For CLBlast it downloads the OpenCL SDK release and CLBlast release. There is some minor fixup required to the .cmake files.
For OpenBLAS, download the release and use that, one file needs to be renamed because I think there is a mistake in LLAMA_OPENBLAS, but I found a workaround.
All these libraries are downloaded and extracted into RUNNER_TEMP. I don't know if it's the best thing, I'm not an expert on Actions, I think maybe we could use a cache here.
They both also add the .dll file that is necessary to run into the artifact. I have a working Mingw cross compile from Debian figured out for CLBlast that will give a fully static .exe but it was easier to adapt the Windows job and it doesn't require compiling the OpenCL SDK and CLBlast.
The CL version can't be tested :( but the OpenBLAS one does pass tests.",5,33
1289,2023-05-02T19:41:38Z,2023-05-03T00:43:43Z,2023-05-03T00:43:43Z,3,18,13,"If llama.cpp is added as a project submodule, .git is a text file containing gitdir. Adding extra backslash .git/ to if(EXISTS) makes it sure that .git/ is a folder containing index.",4,11
1296,2023-05-03T11:17:39Z,2023-05-04T10:03:00Z,2023-05-04T10:03:00Z,1,113,28,"With all the new exciting generation flags added recently (#1126), I wanted to learn more about them and took this opportunity to add them to main's READM.md. I tried to pick example values that might be useful for trying out the features without going overboard, but I'm not sure of the ranges that work best myself. If anyone thinks any of them are too far off, or not far enough off to notice a difference, let me know.
I also put in a section about the new --session feature. And since our prompts can take newlines now, I edited in examples for Windows including one that that works with newlines. (I used powershell for syntax highlighting but they work with both powershell and the traditional command prompt.)",3,0
1297,2023-05-03T11:49:48Z,2023-05-04T10:02:31Z,2023-05-04T10:02:31Z,1,7,5,"Fixes #1224
In the original code, when an empty line is encountered, it stops the user from entering, although in reality, an empty line can be a side effect of the previous input and should simply be ignored.",5,5
1298,2023-05-03T13:44:46Z,2023-05-03T15:26:47Z,2023-05-03T15:26:47Z,3,10,11,Fixes #1270,3,1
1301,2023-05-03T17:08:44Z,2023-05-04T22:58:56Z,2023-05-04T22:58:56Z,1,81,19,"Allows command lines such as ./quantize models/7B/ggml-model-f16.bin q5_1
If the output filename is omitted, it defaults to ggml-model-<ftype>.bin. Old command lines still work, it resolves ambiguities by checking if the argument is a valid ftype.",2,0
1303,2023-05-03T17:26:44Z,2023-05-08T14:41:55Z,2023-05-08T14:41:55Z,7,116,50,"This is likely necessary to make the generation more accurate.
We first noticed this with running the new OpenLLaMA models. The generation completely fails if the first token is not BOS.
#1291 (comment)
Setting the first token in each chunk of the perplexity computation to be BOS drives down the ppl values slightly (~0.05 for 7B), which indicates that  this is the right thing to do. Still, will be happy if somebody with better understanding chimes in and clarifies if we do need to enforce the first token to be BOS.
Another interesting observation is that the vanilla LLaMA models seem ""resilient"" to not having a BOS.
This seems to not be the case for OpenLLaMA. What is the difference that is causing this?
After merging this (or before), will recompute all perplexity values for 7B and 13B LLaMA.
Another effect from this change is that generation after context swap should be better, since before this change, we were ""losing"" the BOS token when n_keep == 0 (i.e. default value).

Perplexity after the change



Model
Measure
F16
Q4_0
Q4_1
Q4_2
Q5_0
Q5_1
Q8_0




7B
perplexity
5.9066
6.1620
6.0910
6.1466
5.9862
5.9481
5.9069


7B
file size
13.0G
4.0G
4.8G
4.0G
4.4G
4.8G
7.1G


7B
ms/tok @ 4th
128
56
61
84
91
95
75


7B
ms/tok @ 8th
128
47
55
48
53
59
75


7B
bits/weight
16.0
5.0
6.0
5.0
5.5
6.0
9.0


13B
perplexity
5.2543
5.3863
5.3607
5.3513
5.2856
5.2706
5.2548


13B
file size
25.0G
7.6G
9.1G
7.6G
8.4G
9.1G
14G


13B
ms/tok @ 4th
239
104
113
160
176
185
141


13B
ms/tok @ 8th
240
85
99
97
108
117
147


13B
bits/weight
16.0
5.0
6.0
5.0
5.5
6.0
9.0



For reference - before the change



Model
Measure
F16
Q4_0
Q4_1
Q4_2
Q5_0
Q5_1
Q8_0




7B
perplexity
5.9565
6.2103
6.1286
6.1698
6.0139
5.9934
5.9571


7B
file size
13.0G
4.0G
4.8G
4.0G
4.4G
4.8G
7.1G


7B
ms/tok @ 4th
128
56
61
84
91
95
75


7B
ms/tok @ 8th
128
47
55
48
53
59
75


7B
bits/weight
16.0
5.0
6.0
5.0
5.5
6.0
9.0


13B
perplexity
5.2455
5.3748
5.3471
5.3433
5.2768
5.2582
5.2458


13B
file size
25.0G
7.6G
9.1G
7.6G
8.4G
9.1G
14G


13B
ms/tok @ 4th
239
104
113
160
176
185
141


13B
ms/tok @ 8th
240
85
99
97
108
117
147


13B
bits/weight
16.0
5.0
6.0
5.0
5.5
6.0
9.0",5,11
1304,2023-05-03T20:10:51Z,2023-05-11T15:10:19Z,2023-05-11T15:10:19Z,2,2,1,Related to comments in #1270,5,5
1308,2023-05-03T22:09:42Z,,2023-05-04T15:47:52Z,1,55,13,"What

The current glob for model files is very restricted, I relaxed it a little so it could find the dolly model.
support for ByteStorage was added. As far as I can see it is uint8.
new: PretrainedVocab is added

Why
I want to use the dolly model with llama.cpp. They use some ByteStorage stuff of torch
Remaining issues
The vocab file is in a completely different format to sentencepiece (it uses a pretrained tokenizer):
Somehow it must get converted or another Vocab must be added to convert.py 

dolly uses a gpt_neox format which is different from what llama.cpp understands. Needs conversion",5,10
1309,2023-05-04T01:12:02Z,2023-05-04T16:54:37Z,2023-05-04T16:54:37Z,1,17,1,"For models like PygmalionAI/metharme-7b and PygmalionAI/pygmalion-7b
By default, converts BF16 to FP32 for better precision.



🤖 Generated by Copilot at c47b349
Add bfloat16 support for PyTorch models in convert.py. Define a new data type constant, a conversion function, and update the tensor handling logic.


Add support for bfloat16 tensors in PyTorch models (link, link, link, link)
Update the custom handlers for unpickling PyTorch tensors (link, link, link)",3,1
1310,2023-05-04T05:24:47Z,,2023-05-06T03:29:22Z,4,210,117,"Updates get/set state and save/load session to support incremental changes to kv state. This lets us checkpoint the state after initial prompt eval and at the end of the session, saving only what changed. On reload, we only load what is needed, flexibly supporting both the ""prompt cache"" and ""prompt continuation"" use cases. Eventually, I envision this being used to e.g., sync state more frequently, or store branching evaluation states as deltas.
Changes

updates llama_copy_state_data with an additional parameter specifying the token offset. Passing 0 gives the current behavior. Otherwise, only the ntok - offset most recent tokens are serialized.
provides an incremental session saving API: llama_init_session_file and llama_append_session_file, the latter being called N times on successive batches of tokens and incremental state. The original llama_save_session_file is simply init + append once.
session file layout changes and version is bumped, more on that below
llama_load_session_file now stops reading after filling the provided token capacity (so loading the prompt from a larger session doesn't incur full cost)
main takes advantage of these changes by saving state after prompt eval, on context swap, and on exit

Approach
The new session file format is the header followed by one or more segments of tokens + incremental state:
<... magic, version, hparams > | ( <u32> token_count | <token_count> tokens | <size_t> state_size | state )*

Each segment represents the incremental evaluation of successive sequences of tokens and the resulting change in state. The incremental state stores the final RNG and logits for that batch, the starting token offset, and the portion of KV state from that offset onward.
When loading saved state in llama_load_session_file, the caller provides the desired number of tokens (e.g., prompt length) to restore. As those tokens are read from successive segments, the state each segment is applied. Specifically, this means restoring the rng and logits and applying the KV state delta. Finally, if the requested tokens don't fit cleanly into a state segment, one less token is returned to force an evaluation and ensure correct logits at that position.
I also took this opportunity to clean up the initial session code in main.
Testing

Prompt startup time with session using chat-13B on 30B: 35s cold vs 2s warm

% ./examples/chat-13B.sh -m ~/llama-models/30B/ggml-model-q4_0.bin --session sessions/chat-1tok.30.bin -n 1
llama_print_timings:       total time = 34634.49 ms
% ./examples/chat-13B.sh -m ~/llama-models/30B/ggml-model-q4_0.bin --session sessions/chat-1tok.30.bin -n 1
llama_print_timings:       total time =  2308.30 ms


Repeat prompt invocations on a session yield the same results:

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 5 -p 'The meaning of life is 4'
...
 The meaning of life is 42: Douglas Adams
...
llama_print_timings:       total time =  2188.90 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 5 -p 'The meaning of life is 4'
...
 The meaning of life is 42: Douglas Adams
...
llama_print_timings:       total time =   1320.01 ms


Growing prompt on successive invocations with session - run time is constant:

 % ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 15 -p 'The meaning of life is 4'
...
 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the
...
llama_print_timings:       total time =  3978.35 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 15 -p 'The meaning of life is 42: Douglas Adams
The Hitchhiker'\''s Guide to the'
...
 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the Galaxy, as it appears in the computer game adaptation of the series.
...
llama_print_timings:       total time =  3331.50 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 15 -p 'The meaning of life is 42: Douglas Adams
The Hitchhiker'\''s Guide to the Galaxy, as it appears in the computer game adaptation of the series.'
...
 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the Galaxy, as it appears in the computer game adaptation of the series.
It’s been a few years since I last read Douglas Adams
...
llama_print_timings:       total time =  3379.11 ms


Rerunning a prefix of saved prompt gives sensible results:

 % ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 15 -p 'The meaning of life is 4'
...
 The meaning of life is 42: Douglas Adams...

 % ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --seed 1 --session sessions/meaning-life-30.bin -n 15 -p 'The meaning of life ' 
...
 The meaning of life  and whether it has any purpose at all 
...


Session size is still reasonable (but will now scale with total number of tokens)

 % du -hs sessions/*
768M	sessions/chat-1tok.30.bin
 31M	sessions/meaning-life-30.bin


examples/save-load-state and non-session usage work",3,4
1311,2023-05-04T06:25:54Z,2023-05-04T12:08:26Z,2023-05-04T12:08:26Z,2,32,23,"To keep things predictable, this patch changes it so we only process escape sequences when the option -e is enabled. It also includes a list of escape sequences in --help.",4,0
1314,2023-05-04T10:58:59Z,2023-05-09T12:29:20Z,2023-05-09T12:29:20Z,1,4,0,"Tested with a 13B model.
You might also want to think about other architectures.",8,18
1316,2023-05-04T12:13:37Z,2023-05-04T16:56:27Z,2023-05-04T16:56:27Z,1,9,8,"I am not getting errors thrown as std::string.
Before:
William Safire will walk us through the nuances of badterminate called after throwing an instance of 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >'

After:
William Safire will walk us through the nuances of badterminate called after throwing an instance of 'std::runtime_error'
  what():  failed to open session/7B/hello.bin: No such file or directory",3,0
1318,2023-05-04T13:14:54Z,2023-05-04T15:41:12Z,2023-05-04T15:41:12Z,4,25,0,"Hi!
I would like to submit a pull request to your project for adding the ""--in-suffix"" option. This option will allow users to specify the input file suffix when running the LLM model on CPU.
As a team working on OpenBuddy, a multilingual open model with the ability to understand user's questions and generate creative contents, we find llama.cpp's project incredibly useful for running LLM models on personal hardware. We appreciate your hard work and dedication in creating this project.
Attached is a screenshot showing successful testing on our own model. We follow a prompt format of ""User: [question]\nAssistant:"", which requires us to add ""Assistant:"" after the user's input in interactive mode for the model to understand the role switch and output the answer correctly.
Thank you for considering this pull request. We look forward to contributing to the llama.cpp project.",2,3
1324,2023-05-04T19:00:47Z,2023-05-05T14:43:37Z,2023-05-05T14:43:37Z,1,4,2,"new integer quantization types
AVX* support
*BLAS support",3,0
1325,2023-05-04T20:53:00Z,,2023-05-25T06:13:47Z,2,137,19,"@ggerganov
Context: ggerganov/ggml#123
Unfortunately (like always) this turned out to be more complicated than I initially expected.
At first I was planning to make it possible to acknowledge and clear an error, but there are some cases where recovery seems impossible like my map functions for example: Because they have to create another tensor and there's no way to free a tensor clearing the error would still leave that tensor allocated.
Probably need to scrap that plan and say ""Once an error occurs, you need to free that ggml_context and start over"". I think that's fine. (Right now those specific functions will abort if tensor creation fails after the first step, but assuming the context is just considered dead then returning NULL will be okay.)
Also, the way I implemented this is if you ignore an error and just keep trying to use the context, then it will abort.
Right now, this just adds the basic scaffolding. I also had to add NULL propagation to basically all functions that make a tensor. This could be simplified with a define instead of having to have a bunch of if (blah == NULL) return NULL; statements but for this sketch I'm keeping it simple.
This converts the asserts in ggml_add_impl and ggml_new_tensor_impl to be recoverable. There's also code cleanup, etc that would need to be done before this is actually ready to become a real pull.
Right now GGML_RECOVERABLE_ERRORS is just #defineed for testing. I have verified that it compiles (on Linux at least) and can run a model. I haven't tested triggering the asserts yet.
Before I continue with the rest, I want to make sure that I'm generally on the right track. What do you think?",3,3
1327,2023-05-04T21:50:45Z,2023-05-05T00:17:07Z,2023-05-05T00:17:07Z,1,2,2,"At line 698, @staticmethod before lazy_rebuild_tensor_V2 throws error at unpickle.load() as not callable",2,3
1329,2023-05-04T23:10:49Z,2023-05-05T12:18:21Z,2023-05-05T12:18:21Z,1,6,1,On macOS we have to load OpenCL as a framework rather than just a library.,5,8
1332,2023-05-05T16:37:35Z,2023-05-05T21:57:14Z,2023-05-05T21:57:14Z,2,5,2,This pull request adds a check in the Makefile in order to set automatically the -lcblas flag on Arch Linux. This change is based on koboldcpp's Makefile so the credit for this should go to @LostRuins.,4,3
1336,2023-05-06T01:25:04Z,2023-05-07T03:03:24Z,2023-05-07T03:03:24Z,1,3,0,"Minor edit in ggml.c which originally would prevent OpenCL from loading completely if GGML_USE_ACCELERATE was defined. Considerable speedup in prompt eval time.
Example timings with a long prompt
Accelerate only
llama_print_timings:        load time =  7336.19 ms
llama_print_timings:      sample time =    93.78 ms /   132 runs   (    0.71 ms per run)
llama_print_timings: prompt eval time = 25906.03 ms /   262 tokens (   98.88 ms per token)
llama_print_timings:        eval time = 15965.80 ms /   132 runs   (  120.95 ms per run)
llama_print_timings:       total time = 602607.10 ms

Accelerate + CLBlast
llama_print_timings:        load time =   414.94 ms
llama_print_timings:      sample time =    85.91 ms /   120 runs   (    0.72 ms per run)
llama_print_timings: prompt eval time =  9738.70 ms /   262 tokens (   37.17 ms per token)
llama_print_timings:        eval time = 13286.57 ms /   120 runs   (  110.72 ms per run)
llama_print_timings:       total time = 41768.89 ms",2,0
1338,2023-05-06T03:28:18Z,2023-05-10T15:37:15Z,2023-05-10T15:37:15Z,4,30,18,"EDITED after updates
This is a much scaled-back change in place of #1310. Renames --session to --prompt-cache and adds a new option, --prompt-cache-all, that causes user input and generations to be saved to the session/cache as well. This new option allows for fast continuation of generations (with additional input).
Testing

--prompt-cache just saves the initial prompt:

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin -n 5 -p 'The meaning of life is 4'     
...

 The meaning of life is 42
Posted on
llama_print_timings:        load time =  1785.67 ms
llama_print_timings:      sample time =     3.42 ms /     5 runs   (    0.68 ms per run)
llama_print_timings: prompt eval time =  1770.00 ms /     8 tokens (  221.25 ms per token)
llama_print_timings:        eval time =   702.82 ms /     4 runs   (  175.70 ms per run)
llama_print_timings:       total time =  2574.34 ms
% du -hs cache/meaning-life.30.bin                                                                                       
 12M	cache/meaning-life.30.bin
% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin -n 5 -p 'The meaning of life is 4'
...
 The meaning of life is 42
Posted on
llama_print_timings:        load time =   741.91 ms
llama_print_timings:      sample time =     3.38 ms /     5 runs   (    0.68 ms per run)
llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)
llama_print_timings:        eval time =  1037.25 ms /     4 runs   (  259.31 ms per run)
llama_print_timings:       total time =  1270.93 ms


--prompt-cache-all saves prompt + generations, allowing ~constant generation time for continuing generation on successive calls:

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin --prompt-cache-all --seed 1 -n 15 -p 'The meaning of life is 4'
...
 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the
main: saving final output to session file 'sessions/meaning-life.30.bin'

llama_print_timings:        load time =  1323.96 ms
llama_print_timings:      sample time =    10.38 ms /    15 runs   (    0.69 ms per run)
llama_print_timings: prompt eval time =  1303.54 ms /     8 tokens (  162.94 ms per token)
llama_print_timings:        eval time =  2447.48 ms /    14 runs   (  174.82 ms per run)
llama_print_timings:       total time =  3959.82 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin --prompt-cache-all --seed 1 -n 15 -p 'The meaning of life is 42: Douglas Adams 
quote> The Hitchhiker'\''s Guide to the'
...

 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the Galaxy, as it appears in the computer game adaptation of the series.
...
llama_print_timings:        load time =   692.69 ms
llama_print_timings:      sample time =    10.34 ms /    15 runs   (    0.69 ms per run)
llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)
llama_print_timings:        eval time =  2969.64 ms /    15 runs   (  197.98 ms per run)
llama_print_timings:       total time =  3349.01 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin --prompt-cache-all --seed 1 -n 15 -p 'The meaning of life is 42: Douglas Adams
The Hitchhiker'\''s Guide to the Galaxy, as it appears in the computer game adaptation of the series.'
...

 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the Galaxy, as it appears in the computer game adaptation of the series.
It’s been a few years since I last read Douglas Adams’
...
llama_print_timings:        load time =   686.97 ms
llama_print_timings:      sample time =    10.31 ms /    15 runs   (    0.69 ms per run)
llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)
llama_print_timings:        eval time =  2979.24 ms /    15 runs   (  198.62 ms per run)
llama_print_timings:       total time =  3382.80 ms

% ./main -m ~/llama-models/30B/ggml-model-q4_0.bin --prompt-cache cache/meaning-life.30.bin --prompt-cache-all --seed 1 -n 15 -p 'The meaning of life is 42: Douglas Adams
The Hitchhiker'\''s Guide to the Galaxy, as it appears in the computer game adaptation of the series.
quote> It’s been a few years since I last read Douglas Adams’'
...
 The meaning of life is 42: Douglas Adams
The Hitchhiker's Guide to the Galaxy, as it appears in the computer game adaptation of the series.
It’s been a few years since I last read Douglas Adams’ “Hitchhikers” novels. They are really funny
...
llama_print_timings:        load time =   693.08 ms
llama_print_timings:      sample time =    10.30 ms /    15 runs   (    0.69 ms per run)
llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)
llama_print_timings:        eval time =  2971.47 ms /    15 runs   (  198.10 ms per run)
llama_print_timings:       total time =  3405.97 ms


also tested non-session usage and chat-13B.sh with prompt cache",5,24
1341,2023-05-06T10:26:54Z,,2023-05-13T14:58:08Z,1,100,21,On master CUDA kernels for dequantization are launched as blocks with only a single thread. This is an inefficient way of utilizing GPUs since they can execute many threads in parallel. This PR changes the way CUDA kernels are launched to launch 256 times fewer blocks but with each block containing 256 threads. On my hardware (GTX 1070) this speeds up prompt processing by ~14% for 33b and a prompt with 399 tokens (navy seals copypasta). I only implemented and tested q4_0 and q5_1 because there are pending breaking quantization changes in #1305 but the change is very simple and should be easy to adapt; just tell me how you want to proceed. Notably I was not able to 100% ensure that generations are unchanged by this PR because I am getting a bug that affects the reproducibility of CUDA-accelerated prompt processing as described in #1340 .,4,23
1343,2023-05-06T13:27:21Z,2023-05-06T21:01:47Z,2023-05-06T21:01:47Z,5,14,13,"Default arguments are a C++ feature, so when building llama.cpp in a plain C project, the build fails. Removing the default value fixes the issue.",2,0
1346,2023-05-06T19:01:51Z,2023-05-08T00:42:01Z,2023-05-08T00:42:01Z,3,6,1,"Fixes #1340 .
When I worked on CUDA acceleration I noticed that the text I generated was not the same as master despite setting the same seed. It seems that due to the use of multiple CUDA streams, the reproducibility of cuBLAS matrix multiplication is not guaranteed . This does not seem to affect perplexity scores but it can unnecessarily cost developers time when they search for non-existent bugs.
This PR documents the non-reproducible nature of cuBLAS. It adds warnings to the README and ggml-cuda.cu. Additionally, when using cuBLAS and setting a seed a warning is printed which states that reproducibility is not guaranteed.",2,1
1359,2023-05-07T19:26:55Z,2023-05-08T16:33:31Z,2023-05-08T16:33:31Z,1,47,0,Nothing too fancy here - a Table of Content to make navigation easier and a quick overview for running the new Pygmalion and Metharme 7B models by PygmalionAI.,2,0
1360,2023-05-07T21:36:41Z,2023-05-13T12:56:41Z,2023-05-13T12:56:41Z,9,6315,419,"Training a llama directly with ggml would be really nice.
For this I implemented the backward passes required for the llama model, tested them with test_grad0 from ggml repo and trained a small llama from scratch to output a sinus wave.
Also see the more detailed discussion in ggerganov/ggml#8 (comment)
List of all new operations that I had to add:

GGML_OP_ADD1 : Could be replaced with add(X,repeat(Y,X)) but it is faster when repeat can be avoided.
GGML_OP_ACC : Necessary for view backward pass. This adds src1 to view(src0, nb, offset) and returns tensor of shape src0.
GGML_OP_SET : (this is new) Necessary for propagating gradients through kv cache. Instead of copying to kv cache this function sets the values in the kv cache viewed with offsets and strides and returns a tensor representing the modified kv cache. This can also be inplace returning a view to the modified kv cache.
GGML_OP_LOG : (this is new) Necessary for cross entropy loss
GGML_OP_SUM_ROWS : Necessary for repeat backward pass: Reduces rows by summing them. shape[a,b,c,d] -> shape[1,b,c,d]
GGML_OP_SILU_BACK : Necessary for silu backward pass
GGML_OP_RMS_NORM_BACK : Could also be implemented using primitives, at the cost of performance.
GGML_OP_GET_ROWS_BACK : Necessary for get_rows backward pass: Adds src0[i] rows to opt0[src1[i]] rows, returning a tensor of shape opt0.
GGML_OP_DIAG : Necessary for softmax backward pass, alternative would have been to implement SOFTMAX_BACK directly, but DIAG is at least usable for other stuff. It turns rows into diagonal matrices.
GGML_OP_DIAG_MASK_ZERO : Necessary for diag_mask_inf backward pass
GGML_OP_ROPE_BACK : Necessary for rope backward pass.

Notable other changes:

add inplace and non-inplace variants for scale, diag_mask_inf, soft_max and rope
fix sub, mul and div functions to work correctly with transposed tensor, uses the same logic as in add:
fix ggml_forward_add functions to work correctly with transposed tensors. uses the same logic as in ggml_compute_forward_add_q_f32, but make it consistent across all ggml_compute_forward_add_... functions. this also slightly changes the mem access pattern of the different threads to work as in ggml_compute_forward_add_q_f32.
de-duplicate ggml_forward_dup code taking care of contiguous tensors of same type. with this we can duplicate tensor of any type as long as they are contiguous. the function is used in dup, get_rows_back and diag_mask (when not inplace).
there are some maybe too verbose comments including step-by-step derivation of gradients that could (or should?) be cleaned away.
(this is new) I added 1d and 4d functions for ggml_reshape and ggml_view.

The performance of various parts of the training could be improved, especially a fast ggml_out_prod could help speeding up the matrix multiplication backward pass.
There are two additional test files, one for testing gradients taken from ggml repo and one small test for testing optimization in general.
Exemplary training of a small llama model is demonstrated in the self-contained baby-llama example, where it is trained to output a sinus wave.

Some notes on first training tests:

lbfgs optimizer is faster and trains better than adam
target logits should represent NEXT token, not the current
target logits -1 & +1 work much better than 0 & +1
I think adding a BOS token also helped improving the training
trained with cross entropy loss gave worse generation results than trained with summed squared logit difference loss

A parallel batched forward function would probably be a good improvement. Training on multiple examples in (parallel) batch really seems to improve the training, but currently I can only do that by calling the forward function multiple times with different input data, which costs a lot of nodes in the computation graph, especially since backward pass is necessary as well.
The batches could be stored in another dimension of the tensors. It would probably just require some reshapes and view operations to make it work.
I did not look into training a LoRa finetune yet, but all the necessary machinery for that seems to be working.",4,11
1366,2023-05-08T12:09:23Z,2023-05-16T15:46:35Z,2023-05-16T15:46:35Z,5,1,5,"Elsewhere in the code the default model path is models/llama-7B/ggml-model.bin; that string appears in
examples/embedding/embedding.cpp
examples/save-load-state/save-load-state.cpp
examples/perplexity/perplexity.cpp
examples/main/main.cpp

This change makes ./main -h print the correct default path.
It might make sense to refactor the code to reference the default path only once. This change makes that refactoring easier to do.",2,0
1367,2023-05-08T13:45:29Z,2023-05-08T14:48:21Z,2023-05-08T14:48:22Z,1,0,2,fixes #1363,2,0
1375,2023-05-09T12:30:39Z,,2023-05-13T14:51:15Z,8,168,26,"TLDR: this PR implements CUDA GPU acceleration for token generation. Even on my relatively slow GTX 1070 this is significantly faster than using the CPU:

To use it, provide the main binary with the --gpu_layers argument and specify how many layers should be GPU-accelerated. Only q4_0 is implemented.
Edit: build instructions (Linux):
git clone https://github.com/JohannesGaessler/llama.cpp llama.cpp-johannesgaessler
cd llama.cpp-johannesgaessler                               
git fetch
git switch dequantize-matmul-2
make LLAMA_CUBLAS=1

For building on Windows, read the llama.cpp README. These build instructions are outdated. They will install the development version in this PR that, while still compatible with the old quantization format, is slower than the version on the master branch and only supports q4_0.
Background
The multiplication of two square matrices of size $N$ is a very compute-intensive operation: it requires $O(N^3)$ multiplications on $O(N^2)$ data values. However, if one of the matrices is thin in one dimension then the matrix multiplication becomes much more I/O-bound because it now requires $O(N^2)$ multiplications on $O(N^2)$ data values. When llama.cpp is generating new tokens it spends ~90% of its runtime doing matrix multiplications and those matrix multiplications are exclusively matrix vector multiplications that are maximally I/O bound. As a consequence the speed at which new tokens can be generated is essentially just proportional to memory bandwidth:

Notably the memory bandwidth on consumer GPUs is much higher than the memory bandwidth on consumer CPUs. I therefore looked into running at least part of llama.cpp on the GPU to speed up token generation.
Implementation
The GPU acceleration of token generation has two issues on master:

The dequantization of matrices on the GPU is too slow. I looked into faster dequantization kernels like #1221 but even then the combined dequantization and matrix multiplication on a GTX 1070 was 2x slower than on the CPU. I therefore implemented a new CUDA kernel that does dequantization and matrix vector multiplication simultaneously.
Transferring quantized matrices to the GPU adds significant latency (4x more than the runtime of the optimized dequantization and matrix multiplication). I therefore implemented storing the quantized matrices in VRAM rather than RAM on a tensor-by-tensor basis. I added a property backend to ggml_tensor that specifies where the data is stored. The data then needs to be transferred to the GPU only once. The vectors are still transferred to the GPU every time but on my hardware this adds relatively low overhead (see below). An additional benefit of this approach is that this can potentially be used to reduce the memory footprint on the host because the quantized matrices only need to be stored on the GPU (not implemented). My system with 32GB RAM can start thrashing with 33b q5_1 so even my current 8GB of VRAM would be a huge quality of life improvement.

Only the repeating layers of LLaMa are accelerated. The fixed layers at the beginning and end of the neural networks are still CPU only for token generation.
Results
On my hardware I found:



Model
Num layers
Baseline speed [t/s] (3200 MHz RAM)
Max. accelerated layers (8 GB VRAM)
Max. speed [t/s] (GTX 1070)
Max. speedup (GTX 1070)




7b q4_0
32
9.15
32
12.50
1.36


13 q4_0
40
4.86
34
6.42
1.32


33b q4_0
60
1.96
19
2.22
1.12



There is a significant speedup for all model sizes though the speedup is considerably larger for the smaller models where a larger percentage fits into VRAM. The plot at the beginning of the PR shows the scaling as a function of the percentage of the model in VRAM. This speedup is essentially the same for all models which suggests that large amounts of VRAM is key. For larger models the maximum potential speedup seems to be higher.
Profiling with nvprof suggests that copying vectors between host and device is not a bottleneck on my hardware:

            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   89.16%  24.7485s     33915  729.72us  411.84us  2.2789ms  void dequantize_mul_mat_q4_0<int=32>(void const *, float const *, float*, int)
                    7.37%  2.04551s       133  15.380ms  8.5321ms  24.494ms  dequantize_block_q4_0(void const *, float*)
                    2.27%  629.46ms     34181  18.415us  2.9760us  5.8011ms  [CUDA memcpy HtoD]
                    0.74%  204.55ms       133  1.5380ms  818.69us  2.8980ms  void gemmSN_TN_kernel<float, int=128, int=16, int=2, int=4, int=2, int=2, bool=1, cublasGemvTensorStridedBatched<float const >, cublasGemvTensorStridedBatched<float const >, cublasGemvTensorStridedBatched<float>>(cublasGemmSmallNParams<float const , cublasGemvTensorStridedBatched<float const >, cublasGemvTensorStridedBatched<float const >, float>)
                    0.47%  129.91ms     34048  3.8150us  2.6880us  12.288us  [CUDA memcpy DtoH]
      API calls:   90.14%  27.5341s     34181  805.54us  72.691us  26.766ms  cudaDeviceSynchronize
                    4.49%  1.37286s         5  274.57ms  79.531ms  743.99ms  cudaMallocHost
                    2.27%  694.01ms     68229  10.171us  2.2800us  5.7958ms  cudaMemcpyAsync
                    1.57%  481.06ms         5  96.211ms  391.06us  275.79ms  cudaFreeHost
                    0.76%  232.09ms       953  243.53us  2.3100us  10.969ms  cuLibraryLoadData
                    0.41%  125.95ms     34181  3.6840us  2.8100us  32.480us  cudaLaunchKernel
                    0.11%  34.327ms       142  241.74us  2.6600us  4.2271ms  cudaMalloc
                    0.10%  29.636ms     34048     870ns     620ns  11.970us  cudaEventRecord
                    0.09%  26.234ms     34048     770ns     650ns  206.77us  cudaStreamWaitEvent
                    0.03%  8.4284ms         2  4.2142ms  3.8226ms  4.6058ms  cudaFree
                    0.02%  5.9012ms     34181     172ns     150ns  3.6300us  cudaGetLastError
                    0.00%  336.79us       336  1.0020us     130ns  43.360us  cuDeviceGetAttribute
                    0.00%  148.54us       766     193ns     130ns  1.1500us  cuGetProcAddress
                    0.00%  93.821us        16  5.8630us  1.4200us  57.391us  cudaStreamCreateWithFlags
                    0.00%  42.180us        82     514ns     400ns  1.9300us  cudaEventCreateWithFlags
                    0.00%  25.690us         3  8.5630us  7.3400us  10.850us  cuDeviceGetName
                    0.00%  5.7400us         3  1.9130us     460ns  4.7300us  cudaGetDevice
                    0.00%  5.2300us        14     373ns     270ns  1.2900us  cudaDeviceGetAttribute
                    0.00%  4.9400us         1  4.9400us  4.9400us  4.9400us  cuDeviceGetPCIBusId
                    0.00%  1.9400us         2     970ns     910ns  1.0300us  cuInit
                    0.00%  1.5500us         5     310ns     150ns     680ns  cuDeviceGetCount
                    0.00%     880ns         3     293ns     260ns     340ns  cuDeviceTotalMem
                    0.00%     840ns         4     210ns     130ns     420ns  cuDeviceGet
                    0.00%     590ns         3     196ns     170ns     240ns  cuModuleGetLoadingMode
                    0.00%     550ns         1     550ns     550ns     550ns  cudaGetSymbolAddress
                    0.00%     530ns         3     176ns     170ns     180ns  cuDeviceGetUuid
                    0.00%     310ns         2     155ns     150ns     160ns  cuDriverGetVersion


Of course, I would be very interested to see the results that people with faster GPUs get with this PR; I personally plan to buy an RTX 3090 now that I've asserted that I'll be able to make use of it.",10,32
1405,2023-05-11T16:48:14Z,2023-05-11T21:23:08Z,2023-05-11T21:23:08Z,12,669,1706,"Close #1241

Drop Q4_2 support
Changed bit-order for Q4 and Q5 (breaking change)
Preplexity is perplexing as usual


New timings:



Model
Measure
F16
Q4_0
Q4_1
Q5_0
Q5_1
Q8_0




7B
ms/tok @ 4th
128
50
54
75
83
75


7B
ms/tok @ 8th
123
44
52
53
58
72


13B
ms/tok @ 4th
332*
93
101
150
164
141


13B
ms/tok @ 8th
308*
81
96
96
104
136




these numbers vary a lot since the model is on the 32GB limit of my MacBook

Old timings:



Model
Measure
F16
Q4_0
Q4_1
Q5_0
Q5_1
Q8_0




7B
ms/tok @ 4th
128
56
61
91
95
75


7B
ms/tok @ 8th
128
47
55
53
59
75


13B
ms/tok @ 4th
239
104
113
176
185
141


13B
ms/tok @ 8th
240
85
99
108
117
147



overall, all these numbers seem to have about +/- 10% variablility from run to run. not ideal benchmark, but not sure what else to do",9,10
1407,2023-05-11T20:57:39Z,2023-05-12T13:40:54Z,2023-05-12T13:40:54Z,3,61,0,"Adds clang-tidy static analyzer and linter checks to pull requests.
The failed checks are reported in the PR in a comment, see these for some examples:
slaren#11
slaren#12 (here it finds a bug in the command line parsing of benchmark-matmult.cpp)
I tried to remove some of the most pedantic checks, but there are still a lot of failed checks in the current code that should be addressed over time. However, the workflow action will only report the checks failed in the lines modified by the PR, so the current issues will not be blamed on new PRs (unless they happen to be in the lines modified by the PR).
To test the settings in .clang-tidy locally:

Configure cmake with -DCMAKE_EXPORT_COMPILE_COMMANDS=on
Run clang-tidy with the cmake build path in -p and the source file, for example:
clang-tidy -p build llama.cpp

Note that since this adds comments to the PRs, it requires adding write permissions to the workflows in the project settings.",3,2
1409,2023-05-12T03:58:24Z,2023-05-12T05:39:40Z,2023-05-12T05:39:40Z,1,1,0,"Hi, SciSharp STACK has made a C#/.NET Binding of llama.cpp, named LLamaSharp. May I kindly ask if it can be added to readme to let more dotnet users know it?
Currently it has provided the following features:

Basic APIs, including getting the embeddings, tokenization and detokenization, loading and inferring the model, etc.
Chat session, providing easy-to-use APIs to make a chat bot.
Quantization, providing an API to quantize the model in C#.
Integration of ASP.NET core, making it easy to deploy the chat service.
One-click installation, without compiling llama.cpp or other manipulations.

SciSharp STACK developers will maintain it and follow up llama.cpp. I believe we can make this .NET Binding better with the great works of llama.cpp! :)",2,0
1412,2023-05-12T09:49:18Z,2023-05-13T13:38:36Z,2023-05-13T13:38:36Z,8,336,42,"Build instructions (Linux):
git clone https://github.com/JohannesGaessler/llama.cpp llama.cpp-johannesgaessler
cd llama.cpp-johannesgaessler                               
git fetch
git switch dequantize-matmul-4
make LLAMA_CUBLAS=1

For building on Windows, read the llama.cpp README.
This PR is a replacement for #1375 that works with the new quantization format. Read that PR for more information. People with no git experience are already using that version so I'm making a new branch and PR to avoid unnecessary confusion, especially with the breaking quantization changes.
The goals of this PR:

Provide an implementation of GPU-accelerated matrix vector multiplication for all quantization types.
Provide an implementation that works on as broad of a range of hardware as possible.
Load weights directly into VRAM to reduce RAM usage.

Not the goals of this PR:

Squeeze out every last % of performance. It seems that GPU performance optimizations like varying block sizes strongly depend on the specific model of GPU so optimizing that seems like it will take a long time and require feedback from many people. I would like to do that at a later time.
Accelerating operations other than matrix vector multiplications and avoiding the copying of vectors between CPU and GPU.
Multi-GPU support. As of right now I don't have a machine that I could test this on.
iGPU support. Again, I don't have a machine to test this on.

In other news, the quantization changes make a big difference for my kernel implementation. I can now get 14.53 t/s with a GTX 1070 for 7b which is 16% faster than with the old quantization method. I think the reason is memory coalescing when reading the vector values.",12,31
1413,2023-05-12T09:58:54Z,2023-05-13T08:43:33Z,2023-05-13T08:43:33Z,1,134,1,"Hi, I am unfortunately on of the persons who have a computer without any AVX instructions, only sse instrucions.
So the runtimes to calculate the tokens are very bad.
For that I implemented ggml_vec_dot_q4_0_q8_0 with SSE instructions to gain some performance. I started with taking the avx parts and replace all 256 vector instructions with 128 vector instructions. After that I did some additional improvements to squeeze some more performance out of the routine.
Run command was
./main -s 1 -t 4 -n 128 ../models/7B/ggml-model-q4_0.bin -p ""What is the meaning of life?""
What I started with without change at the code:
llama_print_timings:        load time = 14251.20 ms
llama_print_timings:      sample time =   129.15 ms /   128 runs   (    1.01 ms per token)
llama_print_timings: prompt eval time = 14050.58 ms /     8 tokens ( 1756.32 ms per token)
llama_print_timings:        eval time = 238504.60 ms /   127 runs   ( 1877.99 ms per token)
llama_print_timings:       total time = 252916.56 ms
What I got after just replacing the avx 256 instructions with sse 128 instructions:
llama_print_timings:        load time =  3349.09 ms
llama_print_timings:      sample time =    53.06 ms /    52 runs   (    1.02 ms per token)
llama_print_timings: prompt eval time =  3154.19 ms /     8 tokens (  394.27 ms per token)
llama_print_timings:        eval time = 23759.20 ms /    51 runs   (  465.87 ms per token)
llama_print_timings:       total time = 27174.93 ms
What I got after squeezing:
llama_print_timings:        load time =  2899.92 ms
llama_print_timings:      sample time =   127.62 ms /   128 runs   (    1.00 ms per token)
llama_print_timings: prompt eval time =  2705.68 ms /     8 tokens (  338.21 ms per token)
llama_print_timings:        eval time = 52500.58 ms /   127 runs   (  413.39 ms per token)
llama_print_timings:       total time = 55559.90 ms
Esspecially the squeezing part is just a feeling, as I would expect the compiler should do much of the improvements on its own, for example the prefetching. But all in all it feels like I get more performance, when I tell it more specific, what I want.",4,6
1416,2023-05-12T13:27:23Z,2023-05-12T14:34:55Z,2023-05-12T14:34:55Z,1,30,26,"In #1375 someone suggested using --gpu-layers instead of --gpu_layers. I had simply picked an underscore because I looked at what a random other argument was using. On master CLI arguments use ""-"" and ""_"" in an inconsistent way. This PR changes all CLI arguments to use ""-"". When a CLI arg contains ""_"" it is automatically converted to ""-"" to preserve backwards compatibility.",3,1
1422,2023-05-12T22:04:56Z,2023-05-13T06:01:15Z,2023-05-13T06:01:15Z,1,90,99,"This should fix the CLBlast related errors with the new formats.
I also rewrote them to be almost identical to the CUDA versions, so future updates could be easier.
Should fix #1417 #1415
I also figured out the solution to the Q5_0 that required preconversion to a different format with f32 (and malloc!), the issue was, of course, an alignment issue which an __attribute__((packed)) as per the OpenCL 1.1 spec solved.
Test results
Test models:
417111a40c36bff7ae6c6b3f773ac6efdb1c46584ef1077a1f3404d668e3944f  llama-7b-q4_0.bin
0fc3f4925923cafe4681370e863319e8ff8f2d760e6b3f5435b415a407aa8d56  llama-7b-q4_1.bin
1226673013a28d61acb94d46eeb15d3623bf0f1472a99ecaf0da8076d680fdf8  llama-7b-q5_0.bin
72040d380ab1067dc08c28d5f16269453bf1d4d7172c24424d4300d8474b42b6  llama-7b-q5_1.bin
7dbee72e1b9d541ed75911488d305d239a0fc4fe86bd287ee002fe75f6423859  llama-7b-q8_0.bin
666a4bb533b303bdaf89e1b6a3b6f93535d868de31d903afdc20983dc526c847  llama-7b-f16.bin

Test data:
head -n 102 wiki.test.raw > wiki.test.mini
Test command:
for q in q4_0 q4_1 q5_0 q5_1 q8_0 f16; do 
    ./build-clblast/bin/perplexity -m ./models/llama-7b-$q.bin --no-mmap -f ./models/wiki.test.mini;
done
Test outputs:

7B Q4_0
main: build = 534 (3243b99)
main: seed  = 1683929450
llama.cpp: loading model from ./models/llama-7b-q4_0.bin
llama_model_load_internal: format     = ggjt v2 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4113748.20 KB
llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 7.99 seconds per pass - ETA 1 minutes
[1]4.4536,[2]5.4657,[3]6.5626,[4]7.2850,[5]7.1903,[6]7.1784,[7]7.3922,[8]7.5547,[9]7.6881,[10]8.0088,[11]8.2590,[12]8.1878,

llama_print_timings:        load time =  9274.76 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 88911.93 ms /  6144 tokens (   14.47 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 90911.25 ms



7B Q4_1
main: build = 534 (3243b99)
main: seed  = 1683929541
llama.cpp: loading model from ./models/llama-7b-q4_1.bin
llama_model_load_internal: format     = ggjt v2 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 3 (mostly Q4_1)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4936276.20 KB
llama_model_load_internal: mem required  = 6612.58 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 8.20 seconds per pass - ETA 1 minutes
[1]4.4737,[2]5.3596,[3]6.4475,[4]7.1958,[5]7.1243,[6]7.1576,[7]7.3686,[8]7.4989,[9]7.6096,[10]7.9235,[11]8.1710,[12]8.1030,

llama_print_timings:        load time = 11045.48 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 90558.13 ms /  6144 tokens (   14.74 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 94128.86 ms



7B Q5_0
main: build = 534 (3243b99)
main: seed  = 1683938023
llama.cpp: loading model from ./models/llama-7b-q5_0.bin
llama_model_load_internal: format     = ggjt v2 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 8 (mostly Q5_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4525012.20 KB
llama_model_load_internal: mem required  = 6210.96 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 8.31 seconds per pass - ETA 1 minutes
[1]4.2785,[2]5.1975,[3]6.3031,[4]7.0360,[5]6.9772,[6]7.0087,[7]7.2125,[8]7.3533,[9]7.4901,[10]7.7904,[11]8.0330,[12]7.9637,

llama_print_timings:        load time =  9311.35 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 89214.59 ms /  6144 tokens (   14.52 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 90805.86 ms



7B Q5_1
main: build = 534 (3243b99)
main: seed  = 1683929741
llama.cpp: loading model from ./models/llama-7b-q5_1.bin
llama_model_load_internal: format     = ggjt v2 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 9 (mostly Q5_1)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 4936276.20 KB
llama_model_load_internal: mem required  = 6612.58 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 8.23 seconds per pass - ETA 1 minutes
[1]4.3009,[2]5.1991,[3]6.3072,[4]7.0070,[5]6.9728,[6]6.9970,[7]7.2182,[8]7.3313,[9]7.4692,[10]7.7641,[11]8.0044,[12]7.9289,

llama_print_timings:        load time = 11141.63 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 94402.18 ms /  6144 tokens (   15.36 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 98093.45 ms



7B Q8_0
main: build = 534 (3243b99)
main: seed  = 1683929839
llama.cpp: loading model from ./models/llama-7b-q8_0.bin
llama_model_load_internal: format     = ggjt v1 (pre #1405)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 7 (mostly Q8_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 7403860.20 KB
llama_model_load_internal: mem required  = 9022.33 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 9.32 seconds per pass - ETA 1 minutes
[1]4.2510,[2]5.1592,[3]6.2440,[4]6.9448,[5]6.8957,[6]6.9294,[7]7.1463,[8]7.2631,[9]7.3949,[10]7.6989,[11]7.9383,[12]7.8694,

llama_print_timings:        load time = 13512.95 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 96608.18 ms /  6144 tokens (   15.72 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 101527.87 ms



7B F16
main: build = 534 (3243b99)
main: seed  = 1683929941
llama.cpp: loading model from ./models/llama-7b-f16.bin
llama_model_load_internal: format     = ggjt v1 (pre #1405)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 1 (mostly F16)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size = 13161556.20 KB
llama_model_load_internal: mem required  = 14645.08 MB (+ 1026.00 MB per state)

Initializing CLBlast (First Run)...
Attempting to use: Platform=0, Device=0 (If invalid, program will crash)
Using Platform: AMD Accelerated Parallel Processing Device: gfx900:xnack-
....................................................................................................
llama_init_from_file: kv self size  =  256.00 MB

system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
perplexity: calculating perplexity over 12 chunks, batch_size=512
perplexity: 12.80 seconds per pass - ETA 2 minutes
[1]4.2555,[2]5.1663,[3]6.2488,[4]6.9504,[5]6.8991,[6]6.9306,[7]7.1476,[8]7.2655,[9]7.3974,[10]7.7015,[11]7.9415,[12]7.8721,

llama_print_timings:        load time = 47153.02 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings: prompt eval time = 141875.01 ms /  6144 tokens (   23.09 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token)
llama_print_timings:       total time = 176944.30 ms",7,5
1428,2023-05-13T08:30:12Z,2023-05-13T13:48:04Z,2023-05-13T13:48:04Z,1,10,16,,1,0
1430,2023-05-13T13:47:55Z,2023-05-14T10:03:52Z,2023-05-14T10:03:52Z,1,132,3,"I rereopen new PR on latest master.
I added AVX support based on AVX2 code to below functions.
static inline __m256i bytes_from_bits_32(const uint8_t * x)
static inline __m256i bytes_from_nibbles_32(const uint8_t * rsi)
static inline __m256 sum_i16_pairs_float(const __m128i xh, const __m128i xl)
static inline __m256 mul_sum_i8_pairs_float(const __m256i x, const __m256i y)
static void ggml_vec_dot_q4_1_q8_1(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
static void ggml_vec_dot_q5_0_q8_0(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
static void ggml_vec_dot_q5_1_q8_1(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
static void ggml_vec_dot_q8_0_q8_0(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
performance improved:
with q5_0 model:  1084790.97 ms -> 156907.19 ms
with q5_1 model:  1072326.98 ms -> 301364.32 ms
ggerganov and sw, please confirm this PR.",4,4
1435,2023-05-13T19:13:23Z,2023-05-20T14:57:40Z,2023-05-20T14:57:40Z,1,246,133,"Remove constant in array definitions, we can't use defines because CPP does not process the kernel code normally with the way it is included in the code.
The platform and device selection should also be improved, it is now possible to use a string to match platforms:

GGML_OPENCL_PLATFORM
GGML_OPENCL_DEVICE

# default:
./main ...

# AMD:
GGML_OPENCL_PLATFORM=AMD ./main ...
etc... But I changed the name of the env variable, because it is not really related to CLBlast itself, only OpenCL in general.
Issue: #1429
Two mallocs and frees removed as well :)",7,33
1436,2023-05-13T19:21:56Z,2023-05-15T02:25:43Z,2023-05-15T02:25:43Z,1,15,14,"had been broken on complex topologies because ""cpu cores"" in /proc/cpuinfo is per-""physical id""",4,0
1443,2023-05-14T02:55:32Z,2023-05-21T17:51:19Z,2023-05-21T17:51:19Z,7,34434,0,"Hi. These days, I have been working on adding an API to llama.cpp using cpp-httplib. I think it could help some people to implement Llama in their projects easily. I know that there are already several alternatives such as bindings to Node JS and Python, but with this example, I intend to implement it natively in C++.
For now, it can only be compiled with CMake on Windows, Linux, and MacOS. All usage and API information can be found in the README.md file inside the examples/server directory. It doesn't require an external dependency.
Edit:
Current available features:

 Completion (wait to end, in loop)
 Custom prompt (generation and interactive behavior)
 Tokenize
 Embeddings

Any suggestions or contributions to this PR are welcome.",7,11
1453,2023-05-14T15:08:28Z,2023-05-14T18:53:24Z,2023-05-14T18:53:24Z,1,33,121,"For my GPU acceleration PR #1412 I used a template to decouple the code for matrix vector multiplication and dequantization. This PR applies the same principle to the dequantization during prompt processing: the same dequantization kernels can be reused with a different template. This allows for the deduplication of CUDA code to ensure consistency. As a side effect the new kernels are also slightly faster on my hardware.
Performance numbers for perplexity calculation on the first 100 lines of wikitext:



GPU
Model
ms/t master
ms/t PR




RTX 3090
7b q4_0
3.61
3.60


RTX 3090
7b q4_1
3.82
3.74


RTX 3090
7b q5_0
3.75
3.64


RTX 3090
7b q5_1
3.75
3.67


RTX 3090
7b q8_0
4.29
4.05


RTX 3090
7b f16
4.91
4.86


GTX 1070
7b q4_0
9.78
7.39


GTX 1070
7b q4_1
9.86
7.67


GTX 1070
7b q5_0
10.01
7.63


GTX 1070
7b q5_1
10.12
7.79


GTX 1070
7b q8_0
11.88
8.28


GTX 1070
7b f16
10.62
10.69



I will also add GTX 1070 numbers once I have them. Done.
The goal of this PR is not to optimize performance. The goal is to simplify the code base to allow for easier development. If the new kernels don't cause a performance regression I consider that good enough.",3,1
1458,2023-05-14T19:43:00Z,2023-05-14T20:46:01Z,2023-05-14T20:46:01Z,1,19,30,"Very minor change to fix a bug detected by clang-tidy in command line parsing, and some minor cleanup.

Fixed command line parsing
Replaced macros with functions
Results are reported in GFLOPS instead of FLOPS_per_u_Second",2,0
1461,2023-05-15T00:37:51Z,2023-07-07T18:25:25Z,2023-07-07T18:25:25Z,4,104,1,"Assuming one has the nvidia-container-toolkit installed on Linux, or is using a GPU enabled cloud, cuBLAS should be accessible inside the container.
I'm not very familiar with Github Actions, nor the available execution environments available on GitHub. I wouldn't suggest pre-building these and putting them in the registry, unless there's a CI path for them.
I'm not too sure what the right path for putting these into a registry is. But I did want to contribute so people could try it locally!",5,8
1462,2023-05-15T05:22:14Z,2023-05-20T07:40:02Z,2023-05-20T07:40:02Z,1,32,0,"It seems mingw's version of getwchar can get stuck in an infinite loop.

It's easy to repeat in one piece of code.
#include <windows.h>
#include <winnls.h>
#include <fcntl.h>
#include <wchar.h>
#include <stdio.h>
#include <io.h>

int main() {
    // Initialize for reading wchars and writing out UTF-8
    DWORD dwMode = 0;
    HANDLE hConsole = GetStdHandle(STD_OUTPUT_HANDLE);
    if (hConsole == INVALID_HANDLE_VALUE || !GetConsoleMode(hConsole, &dwMode)) {
        hConsole = GetStdHandle(STD_ERROR_HANDLE);
        if (hConsole != INVALID_HANDLE_VALUE && (!GetConsoleMode(hConsole, &dwMode))) {
            hConsole = NULL;
        }
    }
    if (hConsole) {
        SetConsoleMode(hConsole, dwMode | ENABLE_VIRTUAL_TERMINAL_PROCESSING);
        SetConsoleOutputCP(CP_UTF8);
    }
    HANDLE hConIn = GetStdHandle(STD_INPUT_HANDLE);
    if (hConIn != INVALID_HANDLE_VALUE && GetConsoleMode(hConIn, &dwMode)) {
        _setmode(_fileno(stdin), _O_WTEXT);
        dwMode &= ~(ENABLE_LINE_INPUT | ENABLE_ECHO_INPUT);
        SetConsoleMode(hConIn, dwMode);
    }

    // Echo input
    while (1) {
        // Read
        wchar_t wcs[2] = { getwchar(), L'\0' };
        if (wcs[0] == WEOF) break;
        if (wcs[0] >= 0xD800 && wcs[0] <= 0xDBFF) { // If we have a high surrogate...
            wcs[1] = getwchar(); // Read the low surrogate
            if (wcs[1] == WEOF) break;
        }
        // Write
        char utf8[5] = {0};
        int result = WideCharToMultiByte(CP_UTF8, 0, wcs, (wcs[1] == L'\0') ? 1 : 2, utf8, 4, NULL, NULL);
        if (result > 0) {
            printf(""%s"", utf8);
        }
    }
    return 0;
}

The code works when compiled with MS's compiler (and Intel's) but I guess getwchar is a weird enough function that no one uses it and it's untested on mingw. The Windows functions themselves are much more well tested and supported in misc compilers, so that's what I changed the code to use.
Edit: Confirmed to fix #1423 and #1529.",4,9
1466,2023-05-15T09:33:50Z,,2023-07-31T12:34:02Z,2,7,2,"This PR adds documentation for how to use the GPU accelerated token generation that I implemented. Also, if llama.cpp was compiled without cuBLAS and the user then tries to run llama.cpp with 1 or more gpu layers an arror is raised and informs the user that they need to compile with cuBLAS (this was a point of failure for multiple people).",3,5
1476,2023-05-16T07:15:37Z,2023-05-16T08:30:15Z,2023-05-16T08:30:15Z,1,1,1,"In some linux distributions (fedora, for example), the include path for openblas is located at '/usr/local/include'",3,2
1477,2023-05-16T10:19:33Z,2023-05-16T18:36:47Z,2023-05-16T18:36:47Z,1,30,9,"X is always positive (range 0..31) in ggml_vec_dot_q5_1_q8_1(), so there's no need for two _mm256_sign_epi8().",2,3
1482,2023-05-16T15:52:37Z,,2023-07-01T18:47:09Z,2,126,3,"Adding the ability to use custom ternary functions as described here: ggerganov/ggml#128
I have checked that it works both with this repo and also in the https://github.com/cztomsik/ggml-js where it's supposed to be used (as an optimization).
It seems to have very little perf. impact (1% at most) so I'm not sure if it's worth adding, but at least it opens some door for 3-var custom kernels.
It seems to work but I have no prior experience with this project, so please check carefully. Thanks :)
EDIT: Maybe there's at least a memory usage improvement, but it's hard for me to check that precisely because I'm using ggml with node.js.",2,1
1483,2023-05-16T15:54:15Z,2023-05-20T12:19:29Z,2023-05-20T12:19:29Z,5,304,116,"Currently my implementation of GPU acceleration has inefficient memory management: weights are loaded into RAM and then copied to VRAM. My current goal is to fix this. This PR is a step towards that goal.
Ideally you would directly load parameters from disk into VRAM. However, currently not all parameters can be loaded into VRAM. Between the weight matrices there are two norms per layer. Managing this would make the implementation more complicated. But if all weights in a layer are in VRAM the implementation will be simpler. So this PR implements GPU acceleration for norms (or rather ggml_mul) even though this in and of itself is of rather low priority; for full GPU acceleration we would have needed this eventually though.
On master the norms are first repeated and then multiplied with another tensor. To keep the CUDA code simpler I have extended ggml_mul (for both CPU and CUDA) to allow the broadcasting of values: if src1 is smaller than src0 in some dimensions its values are being repeated during the multiplication. That way a repeat CUDA kernel is not needed. The ggml graph is also a little smaller which in theory reduces overhead.
There don't seem to be significant performance differences for generating tokens:



CPU
RAM
GPU
Model
-ngl
ms/t master
ms/t ggml_mul broadcast
ms/t ggml_mul broadcast + GPU norms




3700X
3200 MHz
RTX 3090
7b q4_0
0
115.14
115.08
115.00


3700X
3200 MHz
RTX 3090
7b q4_0
33
25.24
24.51
25.30


i5-4570S
1600 MHz
GTX 1070
7b q4_0
0
205.39
201.77
201.66


i5-4570S
1600 MHz
GTX 1070
7b q4_0
33
71.33
71.22
72.36",15,30
1485,2023-05-16T17:57:55Z,2023-05-19T17:16:30Z,2023-05-19T17:16:30Z,1,1,0,This adds WizardLM to the list of supported models.,3,0
1486,2023-05-16T18:30:37Z,,2023-05-17T01:25:51Z,1,8,0,"The previous test results are too uncertain. It is difficult to record and compare, so add the ability to print the average of the test results.",2,2
1490,2023-05-17T01:43:38Z,2023-05-17T14:47:58Z,2023-05-17T14:47:58Z,1,5,0,"Previous test results are so uncertain that they are difficult to record and compare, so add the ability to print the average of test results.",2,0
1495,2023-05-17T03:40:59Z,2023-05-19T17:39:51Z,2023-05-19T17:39:51Z,1,151,0,"Depends on #1032 (to test this, rebase on that first)

Adds a chat-13B-style example that persists the chat session and leverages --prompt-cache and --prompt-cache-all. Notable features:

can stop and restart past chats near instantly
context swapping incurs a much lower delay, by pre-caching the swapped context
doesn't rely on main's interactive or context management features

Testing:
Tested with 30B and the same chat as chat-13B. Exited, resumed (by rerunning same command), and crossed context swaps.
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default MODEL=~/llama-models/30B/ggml-model-q4_0. ./examples/chat-persistent.sh

Also started and resumed other chat sessions, using their own CHAT_SAVE_DIR.
Approach
chat-persistent.sh expects a PROMPT_CACHE_FILE and a CHAT_SAVE_DIR. The former is a --prompt-cache kept over just the initial chat prompt, such that new chats can reuse the same cache. CHAT_SAVE_DIR is set up with a few files:
main.log
main-bg.log
current-prompt.txt
current-cache.bin
next-prompt.txt
next-cache.bin

The ongoing chat transcript is persisted to current-prompt.txt, with each message passing that to ./main and caching in current-cache.bin. Once the ongoing session surpasses 3/5 of context size, messages are also appended to next-prompt.txt. While waiting for user input, ./main is forked to build next-cache.bin from next-prompt.text in the background. Once we near context size, the next prompt is swapped for current.",2,2
1502,2023-05-17T10:42:45Z,2023-05-20T09:02:48Z,2023-05-20T09:02:48Z,4,104,25,"Library: https://github.com/flame/blis  (master)
Compare to openblas (focal-updates,now 0.3.8+ds-1ubuntu0.20.04.1 amd64) in benchmark:
hardware setup:

12th Gen Intel(R) Core(TM) i7-12700  (12 core 20 threads)
16GiB DIMM Synchronous 4800 MHz * 2

openblas-openmp (OMP_NUM_THREADS=14)
Matrix Multiplication of (11008,4096,1) x (11008,128,1) - about  11.54 gFLOPS

Iteration;NThreads; SizeX; SizeY; SizeZ; Required_FLOPS; Elapsed_u_Seconds; gigaFLOPS
=====================================================================================
        0;       1; 11008;  4096;   128;    11542724608;             81141;    142.26
        1;       1; 11008;  4096;   128;    11542724608;             56359;    204.81
        2;       1; 11008;  4096;   128;    11542724608;             55481;    208.05
        3;       1; 11008;  4096;   128;    11542724608;             56389;    204.70
        4;       1; 11008;  4096;   128;    11542724608;             55802;    206.85
        5;       1; 11008;  4096;   128;    11542724608;             56172;    205.49
        6;       1; 11008;  4096;   128;    11542724608;             56069;    205.87
        7;       1; 11008;  4096;   128;    11542724608;             56091;    205.79
        8;       1; 11008;  4096;   128;    11542724608;             56036;    205.99
        9;       1; 11008;  4096;   128;    11542724608;             56178;    205.47


blis (openmp, BLIS_NUM_THREADS=14)
cgraph->n_threads=1
Matrix Multiplication of (11008,4096,1) x (11008,128,1) - about  11.54 gFLOPS

Iteration;NThreads; SizeX; SizeY; SizeZ; Required_FLOPS; Elapsed_u_Seconds; gigaFLOPS
=====================================================================================
        0;       1; 11008;  4096;   128;    11542724608;             67620;    170.70
        1;       1; 11008;  4096;   128;    11542724608;             37163;    310.60
        2;       1; 11008;  4096;   128;    11542724608;             34049;    339.00
        3;       1; 11008;  4096;   128;    11542724608;             37200;    310.29
        4;       1; 11008;  4096;   128;    11542724608;             37662;    306.48
        5;       1; 11008;  4096;   128;    11542724608;             37220;    310.12
        6;       1; 11008;  4096;   128;    11542724608;             37129;    310.88
        7;       1; 11008;  4096;   128;    11542724608;             37251;    309.86
        8;       1; 11008;  4096;   128;    11542724608;             37341;    309.12
        9;       1; 11008;  4096;   128;    11542724608;             37738;    305.86

Average                                                                        298.29
=====================================================================================

mkl blas (Intel10_64lp)  (OMP_NUM_THREADS=14)

Iteration;NThreads; SizeX; SizeY; SizeZ; Required_FLOPS; Elapsed_u_Seconds; gigaFLOPS
=====================================================================================
        0;       1; 11008;  4096;   128;    11542724608;             64675;    178.47
        1;       1; 11008;  4096;   128;    11542724608;             38275;    301.57
        2;       1; 11008;  4096;   128;    11542724608;             38270;    301.61
        3;       1; 11008;  4096;   128;    11542724608;             38418;    300.45
        4;       1; 11008;  4096;   128;    11542724608;             38323;    301.20
        5;       1; 11008;  4096;   128;    11542724608;             38446;    300.23
        6;       1; 11008;  4096;   128;    11542724608;             38306;    301.33
        7;       1; 11008;  4096;   128;    11542724608;             38396;    300.62
        8;       1; 11008;  4096;   128;    11542724608;             38417;    300.46
        9;       1; 11008;  4096;   128;    11542724608;             38363;    300.88

Average                                                                        288.68
=====================================================================================


Also looking forward if someone else could make use of it's typed api to do further optimization.",2,3
1509,2023-05-17T18:49:30Z,2023-05-17T22:12:02Z,2023-05-17T22:12:02Z,6,0,13,"Remove the --n-parts command line option and corresponding n_parts parameter that gets passed around without actually doing anything.
This came up in #1503; my suggestion is for people dealing with large files to just use some other tool (such as GNU split or some ZIP/RAR archiver) to split them. Of course if we decide that llama.cpp should handle multi-part files, this PR shouldn't be merged.
I haven't touched llama_model_loader - that calls guess_n_parts to determine the number of parts from the first file. I'm not sure if there are multi-part model files still around that rely on this. If not we might remove that too: replace the file_loaders vector by a single instance.",2,0
1513,2023-05-18T11:42:35Z,2023-05-18T17:30:41Z,2023-05-18T17:30:41Z,1,1,1,"It seems there's an issue with the stateless lambda functions in some versions of mingw. Breaking the function out isn't terrible.
Edit: Fixed by using the 64 bit version of the compiler. It must not be as well maintained as the i686 version.",4,2
1517,2023-05-18T13:21:52Z,2023-05-18T17:31:01Z,2023-05-18T17:31:01Z,1,1,1,imo it should have been the default.,2,0
1520,2023-05-18T15:19:36Z,2023-05-20T12:58:15Z,2023-05-20T12:58:15Z,2,30,20,"The underlying representation of multibyte character literals is
implementation-defined. This could, at least in principle, cause
cross-build data export/import issues independent of endianness.
Define magic numbers as integer literals to be on the safe side.",4,6
1526,2023-05-19T15:37:04Z,2023-05-20T07:22:37Z,2023-05-20T07:22:37Z,3,26,25,"Some functions had arguments named the same as some member variables generating errors due to name shadowing.
Likewise this PR addresses the C4146 error when using the negative sign on unsigned types. It also fixes an implicit conversion from int to bool by casting explicitly.",2,0
1527,2023-05-19T16:37:34Z,,2023-05-20T08:06:46Z,2,9,0,"There's a special process added to the quantization example recently. However if it's necessary for the quantization, I think it should be moved to the llama_model_quantize c api to keep the behavior consistent.
In my practice, calling the llama_model_quantize api from C# now will produce an incorrect model with no exception. When running that model, the response is completely a mess, without any meaningful word.",2,0
1528,2023-05-19T17:00:06Z,2023-08-26T18:12:56Z,2023-08-26T18:12:56Z,1,6,1,"Pretty sure this is just a bug, but it's always possible I'm missing something!",5,3
1530,2023-05-19T19:42:49Z,2023-05-25T21:07:29Z,2023-05-25T21:07:29Z,3,111,65,"This PR adds performance optimizations for GPU accelerated token generation, mostly benefiting fast GPUs like the RTX 3090. Performance optimizations can be enabled via the options LLAMA_CUDA_BY=2 and LLAMA_CUDA_UNROLL=1 (make) or LLAMA_CUDA_UNROLL=ON (cmake) at compile time. These options degrade performance on my GTX 1070. Build instructions (Linux):
git clone https://github.com/JohannesGaessler/llama.cpp llama.cpp-johannesgaessler
cd llama.cpp-johannesgaessler                               
git fetch
git switch dfyz-xor-hack
make LLAMA_CUBLAS=1 LLAMA_CUDA_BY=2 LLAMA_CUDA_UNROLL=1

Implementation details

As suggested by @dfyz in a previous PR I have eliminated the shared memory from my CUDA kernel and am using black xor magic to sum up the partial sums at the ends. This was universally faster on my RTX 3090 and my GTX 1070. But because this breaks HIP compatibility for this PR it is only being used if GGML_USE_HIPBLAS is not defined.
Also suggested by @dfyz : larger blocks for the CUDA kernels by assigning two rows to each block. The option LLAMA_CUDA_BY sets the number of rows per block. On my RTX 3090 setting this option to 2 is faster but higher values have slightly worse performance. On my GTX 1070 a value of 2 or higher causes performance degradation.
Loop unrolling: The matrices used in llama.cpp always have the same size. So the loops used during inference can be unrolled if the compiler is told how large the matrices are. This is done via moving ncols from a regular argument to a template argument and adding a switch statement for the various matrix sizes (8 in total). On my RTX 3090 this is faster but on my GTX 1070 it's slower. Enabling this option significantly increases compile time.
Larger blocks in x direction: The option LLAMA_CUDA_BX can be set to determine the block size in x direction. Default value is 32, 64 was faster on RTX 3090.

Results
For the RTX 3090 I used LLAMA_CUDA_BY=2 LLAMA_CUDA_UNROLL=1, for the GTX 1070 I did not use these options.



GPU
Model
ms/t master
ms/t PR
ms/t no unroll




RTX 3090
7b q4_0
23.49
20.36
21.04


RTX 3090
7b q8_0
24.54
22.05
-


RTX 3090
13b q4_0
37.95
32.85
34.04


RTX 3090
33b q4_0
83.15
69.76
73.46


GTX 1070
7b q4_0
69.20
67.80
-


GTX 1070
13b q4_0
141.30
139.39
-",6,16
1531,2023-05-20T00:32:31Z,,2023-07-10T23:23:19Z,1,61,0,"Thanks for #1516
I hope your guys enjoy it.",3,4
1536,2023-05-20T09:32:16Z,2023-05-20T14:58:31Z,2023-05-20T14:58:31Z,5,105,26,"Reopen to cope with the ci errors
Requested by @ggerganov",2,3
1550,2023-05-21T11:33:26Z,2023-05-26T02:18:01Z,2023-05-26T02:18:01Z,2,15,5,"--seed is ignored when loading session
Currently the --seed parameter is ignored when loading the prompt. However, a very common use case would be to save a prompt and then try several attempts at generation with different seeds.
The pull includes a simple change that just sets the RNG if specified. Two small notes:

There isn't a way to tell if -seed was actually actually specified as far as I know, only that it's not the default -1 value. So --seed -1 is the same as not including it: it won't override the cached seed.
The RNG won't be in the same state as if the seed had been specified originally. I.E. If you generate the cached prompt using --seed 123 and then load it with -seed 123 the subsequent tokens will not match. I don't think there's an easy way around this. It's not 100% ideal but still a lot better than just completely ignoring the parameter with no warning.

Blank prompt overwrites cached one
When loading a cached prompt from a session, you have to specify the prompt again. Even worse, if you forget to enter a prompt you'll get your cached prompt overwritten by the blank one because of course it has low similarity.
This pull changes that behavior to simply use the tokens from the saved session if params.prompt is empty (in other words not set via --prompt or --file).
Closes #1439",6,23
1554,2023-05-21T14:02:25Z,2023-09-20T07:02:39Z,2023-09-20T07:02:39Z,1,15,13,"The precision for Q4_0 has degraded since #1508
Use Q4_1 instead
ref #1551",3,1
1556,2023-05-21T21:18:44Z,2023-06-26T17:57:59Z,2023-06-26T17:57:59Z,14,337,234,"ref: #1437
2S Xeon E5-2690v1
Before:
./main -n 1024 -m models/7B/ggml-model-q4_0.bin --ignore-eos -p ""Building a website can be done in 10 simple steps:"" -t 32
...
llama_print_timings:      sample time =   796.65 ms /  1024 runs   (    0.78 ms per token)
llama_print_timings: prompt eval time = 44892.59 ms /   785 tokens (   57.19 ms per token)
llama_print_timings:        eval time = 178337.97 ms /  1020 runs   (  174.84 ms per token)
# echo 3 > /proc/sys/vm/drop_caches
After:
./main -n 1024 -m models/7B/ggml-model-q4_0.bin --ignore-eos -p ""Building a website can be done in 10 simple steps:"" -t 32
...
llama_print_timings:      sample time =   795.63 ms /  1024 runs   (    0.78 ms per token)
llama_print_timings: prompt eval time = 46996.62 ms /   785 tokens (   59.87 ms per token)
llama_print_timings:        eval time = 101232.21 ms /  1020 runs   (   99.25 ms per token)",5,40
1558,2023-05-22T06:22:40Z,2023-08-04T15:20:12Z,2023-08-04T15:20:12Z,7,536,435,"--simple-io uses only basic io functions that work when main is run as a subprocess. This fixes issues with child processes such as those seen in #1547 and #1491. It also happens to be a work around for #1382 by allowing people to switch back to the basic IO for use with Windows Terminal history (that lets you press up and down to scroll through your history).
The number of lines changed is actually not as big as it looks, but the console stuff was half the size of the common.cpp file so I broke it out into its own cpp and header file.
Edit:
Confirmed fix for #1491.
Confirmed fix for #1547.
Confirmed fix for #2477.",4,14
1564,2023-05-22T21:19:46Z,2023-05-24T06:16:23Z,2023-05-24T06:16:23Z,1,2,2,"GNU Grep does not support the \d backslash expression1. Use bracket expressions instead.
Footnotes


Actually, it does, with the -P option. But that is a GNU extension. Bracketed character class expressions are specified in POSIX and should be compatible with all implementations. ↩",3,1
1565,2023-05-22T22:21:41Z,2023-05-23T16:01:16Z,2023-05-23T16:01:16Z,1,1,1,"The clCreateCommandQueue() function will return the error code CL_INVALID_QUEUE_PROPERTIES when passed unsupported properties, not CL_INVALID_PROPERTY as the original code was checking for.
Relevant API docs.
With this change, I can run the CLBlast support with the Rusticl driver of Mesa 23.1.0 on Linux.",2,1
1568,2023-05-23T03:47:21Z,2023-05-24T06:24:01Z,2023-05-24T06:24:01Z,1,19,0,Follow up to #1495,2,0
1570,2023-05-23T13:29:12Z,2023-06-17T11:53:05Z,2023-06-17T11:53:05Z,7,1119,953,"sever.cpp left out a few generation parameters and also seems built to assume un-editable context with no regens or swipes. As written, it adds on the generated tokens to the end with no easy way for calling frontends to ask it to reprocess them.
Since there are relatively complex context changes happening in a lot of frontends, I added a simple ""reload_ctx"" flag that can be passed on generation that will cause the prompt to be reloaded. There's likely a smarter way to handle this (not caching the generated tokens or something), but this was quick and easy and will allow for easy usage with things like popular frontends for the time being.",18,172
1579,2023-05-23T22:21:41Z,,2023-08-29T16:58:41Z,2,96,95,"The main change here is to get rid of the file and library lists and use targets instead.
Here is an example of the dependencies of main.cpp when building with LLAMA_CLBLAST=ON:

LLAMA_CUBLAS=ON:


compile_commands.json for CUDA build
[
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/cc -DGGML_USE_CUBLAS -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -mf16c -mfma -mavx -mavx2 -o CMakeFiles/ggml.dir/ggml.c.o -c /home/henri/src/llama.cpp/ggml.c"",
  ""file"": ""/home/henri/src/llama.cpp/ggml.c"",
  ""output"": ""CMakeFiles/ggml.dir/ggml.c.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -DGGML_USE_CUBLAS -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o CMakeFiles/llama.dir/llama.cpp.o -c /home/henri/src/llama.cpp/llama.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/llama.cpp"",
  ""output"": ""CMakeFiles/llama.dir/llama.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_USE_CUBLAS -isystem /opt/cuda/include -O3 -DNDEBUG -std=c++11 -mf16c -mfma -mavx -mavx2 -x cu -c /home/henri/src/llama.cpp/ggml-cuda.cu -o CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o"",
  ""file"": ""/home/henri/src/llama.cpp/ggml-cuda.cu"",
  ""output"": ""CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o -c /home/henri/src/llama.cpp/tests/test-quantize-fns.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/tests/test-quantize-fns.cpp"",
  ""output"": ""tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o -c /home/henri/src/llama.cpp/tests/test-quantize-perf.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/tests/test-quantize-perf.cpp"",
  ""output"": ""tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o -c /home/henri/src/llama.cpp/tests/test-sampling.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/tests/test-sampling.cpp"",
  ""output"": ""tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o -c /home/henri/src/llama.cpp/tests/test-tokenizer-0.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/tests/test-tokenizer-0.cpp"",
  ""output"": ""tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/CMakeFiles/common.dir/common.cpp.o -c /home/henri/src/llama.cpp/examples/common.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/common.cpp"",
  ""output"": ""examples/CMakeFiles/common.dir/common.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/main/CMakeFiles/main.dir/main.cpp.o -c /home/henri/src/llama.cpp/examples/main/main.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/main/main.cpp"",
  ""output"": ""examples/main/CMakeFiles/main.dir/main.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o -c /home/henri/src/llama.cpp/examples/quantize/quantize.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/quantize/quantize.cpp"",
  ""output"": ""examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o -c /home/henri/src/llama.cpp/examples/quantize-stats/quantize-stats.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/quantize-stats/quantize-stats.cpp"",
  ""output"": ""examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o -c /home/henri/src/llama.cpp/examples/perplexity/perplexity.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/perplexity/perplexity.cpp"",
  ""output"": ""examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o -c /home/henri/src/llama.cpp/examples/embedding/embedding.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/embedding/embedding.cpp"",
  ""output"": ""examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o -c /home/henri/src/llama.cpp/examples/save-load-state/save-load-state.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/save-load-state/save-load-state.cpp"",
  ""output"": ""examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o -c /home/henri/src/llama.cpp/examples/benchmark/benchmark-matmult.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/benchmark/benchmark-matmult.cpp"",
  ""output"": ""examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/examples -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o -c /home/henri/src/llama.cpp/examples/baby-llama/baby-llama.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/examples/baby-llama/baby-llama.cpp"",
  ""output"": ""examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/pocs -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o -c /home/henri/src/llama.cpp/pocs/vdot/vdot.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/pocs/vdot/vdot.cpp"",
  ""output"": ""pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o""
},
{
  ""directory"": ""/home/henri/src/llama.cpp/build-cuda"",
  ""command"": ""/usr/bin/c++ -I/home/henri/src/llama.cpp/pocs -I/home/henri/src/llama.cpp/examples/. -I/home/henri/src/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -mf16c -mfma -mavx -mavx2 -o pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o -c /home/henri/src/llama.cpp/pocs/vdot/q8dot.cpp"",
  ""file"": ""/home/henri/src/llama.cpp/pocs/vdot/q8dot.cpp"",
  ""output"": ""pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o""
}
]",3,4
1580,2023-05-23T22:42:14Z,2023-05-24T07:30:10Z,2023-05-24T07:30:10Z,1,5,5,Recent update.,2,0
1584,2023-05-24T08:39:05Z,,2023-05-25T01:30:59Z,2,8,79,make it support chinese more,2,2
1588,2023-05-24T18:37:54Z,2023-05-30T18:24:22Z,2023-05-30T18:24:22Z,1,7,0,"Not working, perplexity around 2000.
The code doesn't crash and the model can be loaded.
ggml files: huggingface.co/SlyEcho/open_llama_3b_ggml
Source data here: huggingface.co/openlm-research/open_llama_3b_600bt_preview
More info: #1291

Perplexity on wiki.test.raw with -b 512 -c 512



Q
chunk
perplexity




F16
[616]
8.4656


Q8_0
[616]
8.4667


Q5_1
[616]
8.5072


Q5_0
[616]
8.5156


Q4_1
[616]
8.6102


Q4_0
[616]
8.6674",6,30
1594,2023-05-25T15:31:20Z,2023-06-11T13:20:53Z,2023-06-11T13:20:53Z,1,3,0,"In WSL, the CUDA has limitation on the pinned memory. It's prohibited to allocate a large size of CUDA memory. But it can be bypassed in the program.",4,2
1597,2023-05-26T06:52:13Z,2023-06-12T12:44:16Z,2023-06-12T12:44:16Z,5,55,114,Integrate CUDA and OpenCL load code in llama_model_loader so that we can leverage mmap.,5,19
1598,2023-05-26T10:04:34Z,2023-07-07T13:12:50Z,2023-07-07T13:12:50Z,1,1,0,"Fixes #1473
I verified that a merged metharme-7b model is converted successfully (with https://github.com/akx/ggify) works as expected with their example prompt.",3,0
1599,2023-05-26T10:45:33Z,2023-06-05T20:24:29Z,2023-06-05T20:24:29Z,1,30,29,"This change makes sure that all exceptions thrown and caught are derived from std::exception (typically, std::runtime_error), rather than mixing std::string and std::runtime_error in different parts of the code.
This should fix #1569 and fix #1589 and completes the changes in #1316 (which already changed exception types to std::runtime_error in llama-util.h).",4,0
1604,2023-05-26T21:34:21Z,2023-05-27T15:47:55Z,2023-05-27T15:47:55Z,1,79,6,"Installing, compiling and using CLBlast
Relevant issue: #1546",3,1
1606,2023-05-26T23:03:47Z,2023-05-27T12:18:25Z,2023-05-27T12:18:25Z,1,1,1,"A recent issue on the CLBlast repo: #479 has caused a new release archive to be created for the same version which has a different file name structure.
Without fixing this the CI will be broken.
In the future, it would probably be better to compile it from source.",2,4
1607,2023-05-27T09:28:34Z,,2023-06-06T19:34:03Z,10,795,449,"This PR is quite large. Its primary goal is to lay the groundwork for the implementation of further CUDA kernels for ggml operations. I am also adding multi GPU support because it's easier to integrate now than it would be at a later point.
For Users
Build instructions (Linux):
git clone https://github.com/JohannesGaessler/llama.cpp llama.cpp-johannesgaessler
cd llama.cpp-johannesgaessler                               
git fetch
git switch cuda-refactor-8
make LLAMA_CUBLAS=1

When compiling with LLAMA_CUBLAS=1 the program automatically detects the available NVIDIA devices and splits weights proportional to VRAM. There is not yet a CLI argument for setting the tensor splits. The performance increase on my test systems is relatively low (+70% t/s when going from 1x GTX TITAN X to 4x GTX TITAN X). It's possible that there is still a bug that hampers performance. Please do tell me how well (if at all) it works for you. In any case, this PR should already allow you to pool the VRAM of multiple GPUs to load larger models.
For Developers
This PR is still very much WIP. I will do a refactor to remove artifacts from bad/obsolete design decisions. You can already review the code if you want but many of the flaws are still subject to change. Should be good now.
On master there are separate functions for invoking CUDA kernels. Apart from invoking the actual CUDA kernels they do other things such as copying data between host and device. This PR adds a template ggml_cuda_op that manages

the transfer of data between host and device,
the dequantization of src0 (needed for cuBLAS matrix multiplication),
the broadcasting of src1 across src0 (needed for multiplication),
and multi GPU things.

The actual operations now only need to define how the data should be manipulated.
This PR also moves the entry point for invoking CUDA kernels from the ggml function such as ggml_compute_forward_mul_mat_q_f32 and instead adds a function ggml_cuda_compute_forward that is called from ggml_compute_forward. For this to work I moved ggml_task_type and ggml_compute_params from ggml.c to ggml.h.
This PR adds an int for the layer, an int for the device id, and dedicated device data pointers to ggml_tensor. I need these for bookkeeping. I also changed the backends from GGML_BACKEND_CUDA and GGML_BACKEND_OPENCL to GGML_BACKEND_GPU (tensor data on 1 GPU) and GGML_BACKEND_GPU_SPLIT (tensor data split across all GPUs). Since I think that we don't want to support the simultaneous use of CUDA and OpenCL it's simpler to just use the same backend types for both implementations and to differentiate via defines.",14,52
1609,2023-05-27T10:30:30Z,2023-05-29T12:13:40Z,2023-05-29T12:13:41Z,1,6,0,"This code just checks if we've used up all the given prompt but still have leftover tokens from the cache. In this case it instead leaves the last token in embd so it will be evaluated.
Evaluating one token isn't the end of the world, but it would be nice if it were faster. What I really wanted to do was something like this:
    llama_eval(ctx, embd.data(), 0, n_past - 1, params.n_threads);

But you can't eval 0 tokens.
I tried to edit llama_eval_internal to skip over the input calculations where n_past (N) is 0, but it's actually way more integrated than I expected. Perhaps there was something to #1281 after all. I think it would be nicer if llama_eval just supported 0 tokens, but it would also be nice to have a different api call that would evaluate the logits at a given position n_past.
This API call could also be used for implementing the mockup in #1608 where you can click on a token and see alternatives.",3,4
1610,2023-05-27T11:15:45Z,2023-05-27T17:04:15Z,2023-05-27T17:04:15Z,3,27,18,"Set LLAMA_BUILD_SERVER in workflow so the server example gets build, however I only added this to the Windows builds because it seems like only Windows binary artifacts are included in releases.
Add server example to Makefile (still uses LLAMA_BUILD_SERVER define and does not build by default)
Remove vdot binary when running make clean.
Fix compile warnings in server example.
Add .hpp files to workflow (the server example has one).

I don't really have a way to test the workflow changes. They seem pretty straightforward but I'm far from an expert on GitHub workflows.
I also don't actually know C++, so my changes to server.cpp were done without really understanding how it works. All I can say is it appears to run okay and compiles without warnings on clang 15, gcc 13 and gcc 11.
Closes #1578",4,17
1611,2023-05-27T12:14:25Z,2023-05-28T17:09:56Z,2023-05-28T17:09:56Z,1,3,8,,3,2
1612,2023-05-27T12:19:17Z,2023-05-28T17:13:37Z,2023-05-28T17:13:37Z,1,5,6,,3,0
1614,2023-05-27T14:30:28Z,2023-05-28T17:14:24Z,2023-05-28T17:14:24Z,3,21,2,"Mostly useful for HTTP API clients -- when you are making an HTTP API request for completion, there is no way to know what model is being used. Server could set model field in the completion response to the filename of the model, but it seems that providing an alias option would allow for more flexibility.",2,0
1616,2023-05-27T14:39:02Z,2023-05-27T20:03:25Z,2023-05-27T20:03:25Z,1,2,0,Allow the code to compile on a RISCV CPU,3,1
1617,2023-05-27T19:31:45Z,2023-05-28T19:01:02Z,2023-05-28T19:01:02Z,1,5,1,"With this PR, if LLAMA_DEBUG is set debugging symbols for use with e.g. the GNU Debugger are added to the binaries. Optimizations are also turned off to prevent local variables from being optimized out.",2,0
1621,2023-05-28T07:41:28Z,2023-05-29T04:45:50Z,2023-05-29T04:45:51Z,2,2,2,"The build-info.sh script is invoking git on lines 6 and 11. However git is not installed by default in ubuntu base images.
This causes an unhandled exception to occur, which is silently passed during the build.
#1626
For example:
https://github.com/ggerganov/llama.cpp/actions/runs/5100538614/jobs/9168878817#step:6:1266
https://github.com/ggerganov/llama.cpp/actions/runs/5100538614/jobs/9168878743#step:6:864",2,2
1625,2023-05-28T10:13:58Z,2023-05-28T17:48:57Z,2023-05-28T17:48:57Z,5,56,32,"This pull:

Adds a LLAMA_SUPPORTS_GPU_OFFLOAD define to llama.h (defined when compiled with clBLAST or cuBLAST)
Updates the argument handling in the common example code to only show the -ngl, --n-gpu-layers option when GPU offload is possible.
Adds an entry for the -ngl, --n-gpu-layers option to the main and server examples documentation
Updates main and server examples documentation to use the new style dash separator argument format
Updates the server example to use dash separators for its arguments and adds -ngl to --help (only shown when compiled with appropriate support). It will still support --memory_f32 and --ctx_size for compatibility.
Adds a warning discouraging use for --memory-f32 for the main and server examples --help text as well as documentation. Rationale: #1593 (reply in thread)

@JohannesGaessler Hopefully this isn't stepping on your toes, I took a different approach to dealing with the GPU offload support issue.
Closes #1555",3,5
1632,2023-05-29T00:55:42Z,,2023-06-26T22:01:57Z,27,4237,652,"Introduction
MUL_MAT take most of the compute time (about 95%). So to speed up llama, we have to focus on MUL_MAT.
BLAS, as  one of the fastest MUL_MAT solution on CPU, typically efficient at computing large matrix multiplication and tends to be very slow when run parallel in multi OS threads. Accelerate is the native BLAS implementation on macOS, which has the problems exactly as said. OpenBLAS or BLIS are a bit slower than Accelerate, the authors claim that they support multi-threads, but I did not test that. So I assume for the big matrix sizes in llama, multi threaded BLAS does not run faster than single thread.
We have three kinds of MUL_MAT to compute:

mul_mat_f32: both src0 and src1 are F32.
mul_mat_f16_f32: src0 is F16 and src1 is F32.
mul_mat_q_f32: src0 is qauntizied (Q4_0, Q4_1, ...), and src1 is F32.

For every kind of MUL_MAT, we have pure CPU solution which has optional INIT stage and COMPUTE stage.
And optional solutions: CUDA/CL that run in GPU, and BLAS that run in CPU.

mul_mat_f32:  has only one stage: COMPUTE.

The pure CPU with multi-threads
BLAS, CUDA and CL with single thread


mul_mat_f16_f32:

The pure CPU has two stages: INIT with single threads, COMPUTE with multi-threads.
BLAS, CUDA and CL with single thread


mul_mat_q_f32: same as mul_mat_f16_f32, but the de-quantization time is significant.

As of BLAS, there are three known problems to solve:

spin only threading. While spin has been the simplest and perhaps the fastest solution, the community has been seeking some kind of practical threading infrastructure that can compensate the busy spinning at certain situations for long.
single thread BLAS. This is because that:
The typical mul_mat time when N/K >= 4096 ranges from several ms to hundreds ms. Given n_threads > 1, when run BLAS in main thread, worker threads has nothing to do thus keep spinning. The spinning overhead is not acceptable.
Given M/N/K, n_threads (and even src0 type), due to the diverse of matrix dimensions and hardware/software stacks, we are not sure which of the solutions is the fastest. At present, master branch applies this rule: run CUDA/CL/BLAS in single OS  thread when both src0 and src1 are continuous and M >=32 && N >=32 && K >= 32.  As of llama model, this rule almost equals to M >= 32 && N >= 4096 && K >= 4096.
For some N/K, de-quantization time may exceeds mul_mat time when M < 128. This range covers the token size of typical daily conversations. So, we'd better separate de-quantization out of the for loops, thus we can run de-quantization in multi-threads.

Solutions
This PR tries to solve the above problems, they are tightly coupled together. So it's hard to just solve one without touching others.
1. A new threading infrastructure that supports spin + wait/notify
Typical usages are:

when compute a task stage, main threads knows that this stage can only run by it's self, and the task stage is configured as idle wait, it issues a wait_now command, workers get this command almost at once, then go wait.
workers can be configured with wait_on_task_done: that means we can look ahead a few future task stages to see if there are no immediate multi-thread needs. If no, then tell workers go waiting after finishing task. The  optimization benefits energy saving, but is hard to implement correctly and efficiently. In addition to mutex, I have to use spin lock.
Also, when compute a task stage, if main threads knows current task stage needs more workers, it executes a syscall to wake up all workers. I had ever implemented a threading framework that can await or wakeup given number of workers. I finally discarded that because I did not find evidence to use only part of workers.

2. A way to configure how to run task stage.
I want to explicitly define: which part of code to run, single thread or multi-thread, workers should go idle wait or not. This is not new but introduced the idle wait and make the configure more explicit. With this we can run bench at will, this unlock us from current implicit#if defined(xxx), and allow us to build with all kinds solutions. I formally defined task profiles for the three kinds of mul_mat. This took not little codes, but is very important for the whole solution.
3. A flexible tune(bench) tool to generate bench data
This tool has the following features/benefits:

Supports all llama models and typical matrix sizes and types (attention layer, feed-forward layer, RoPE)
Supports all types (F32, F16, all of the Qx_x).  NOTE F32 and F16 are disabled as workaround to avoid a unfixed bug.
Able to write to/read from file. So the result can be generated ahead of time, and be loaded into memory later.
The data file is designed as self-contained, including model, type, backend, all 6 typical shapes, every shape contains their task profiles and per task stage execution time for every task profile.
Able to estimate execution time for any M and n_threads, provide corresponding APIs for GGML.
Analyze bench data for n_threads. The output is CSV blocks, thus can be easily visualized.
Should cover typical M range. I had ever generated M with a constant start value, increase with constant step (for example, from 16, step in 8). Now I generate M with (for n in [0, 10] M := 2^n), this balance the two fundamental needs: (1) M range should reasonable large (2) should assign more  M(s) for M <=32 because I guess this is the typical conversation token size that will be executed frequently and this M range is sensitive to profile selecting as of multi-threading.
Should run as fast as possible. It takes about 75 seconds on my device to bench 7B/Q4_0/Accelerate with 10 Ms range from 1 up to 512 in 3 passes 1 thread, while one pass bench takes about 35 seconds 1 thread, with 4 threads 1 pass and max-M 128 takes about 13s. Current speed is not good enough in case of running bench at  program startup.

4. Adapt llama and ggml to schedule with bench
After the bench data was loaded into program, when do graph computing, we can at first match shape by given N/K, then estimate time for every profile that this shape supports, finally select the fastest profile. Since in practice, we only bench for limited M (10s or so) , we have to leverage some magic to estimate time for any M. Due the the near linear nature of M-time curve, I use interpolate. This is not very cool, but is the best affordable way I can think. Non-continuous matrices are not suitable to run in BLAS, so they will be scheduled to the pure CPU profile. If both src0 and src1 of matrix are continuous, but we do not have bench loaded or for some unknown reasons or bugs that we can not find corresponding shape for given N/K, or unable to estimate, we fallback to the traditional logic: M >= 32 && N >=32 && K >= 32 -- this is totally unfortunate because estimating bias around 32 is highly sensitive to performance. You will see this in the following section.
5. Split single thread BLAS
I separated de-quantization with de-quantization + mul_mat from the for loops. Thus I can create the third task profile for the q_f32's use BLAS solution:  run de-quantization in  INIT stage with multi-threads, run mul_mat with BLAS and single thread, let workers idle wait.
Results
Due to the nature of predicating, it's a bit hard for me to bench end to end. I wrote a bench tool named prompt.sh to ask llama questions like this: 0+0=0.1+1=1.2+2=. Although in this way it is easy to construct prompt at almost any approximate size, this kind of questions are likely take llama too much time to think, thus result in unusual bench time that may be longer than those normal questions.  I have to say that I don't know how to efficiently and correctly run the end-to-end bench at all. Anyway, I did run the examples/chat.sh with 4 threads for many times. Often observed the prompt time decreases  about 35%, sometimes over 40%, comparing to master.
So, let me explain in more strict but perhaps easier understood way with a bunch of images.
First of all let's remember several tokens that will be used to identify the task stages for the three q_f32 profiles.

#0_0_nth=1 : profile 0, stage 0, n_threads = 1
#0_1_nth=1 : profile 0, stage 1, n_threads = 1
#0___nth=1 : profile 0,  total, n_threads = 1
#1_1_nth=1 : profile 1, stage 1, n_threads = 1
#1___nth=1 : profile 1, total,  n_threads = 1
#2_0_nth=1 : profile 2, stage 0, n_threads = 1
#2___nth=1 : profile 2, total, n_threads = 1
#0___nth=2 : profile 0, total, n_threads = 2
...
#2___nth=6  : profile 2, total, n_threads = 6

Where stage 0 is the INIT stage and stage 1 is the COMPUTE stage.
The values of n_threads are typical because:

apart from 1, we usually use even n_threads.
personal computers often do not have that many physical cores,  6 n_threads is OK.
suppose the single thread time is t1, when we increase n_threads, we will get, t2=0.5t1 for n_thread=2, t4=0.25t0 for n_threads=4, t3=0.16*t0 for n_threads=6. The 0.16 means -84%, this  is a pretty good speed up, I think.
Too many threads causes heavy spin + wait/notify burden. When the ROI (speedup rate v.s. energy/heat) decreases to certain value, increasing n_threads will help little or even hurt.

All data in the following images are created from llama 7B. I will not show you all models because that's too lengthy and I can only run 7B/13B.  Instead I'll try Q4_0, Q5_0 and Q8_0 because they are enough for us to catch the points.
I ran bench/analyze on my MacBook pro 2018 with 32 GB 2400 MHz DDR4 memory, 2.6 GHz 6-Core Intel Core i7-8850H @2.60GHz.
The data are all plotted in 2-D lines, where the x-axis is M, and the y-axis is per-thread execution time with unit of ms.
4096x4096, Q4_0
The M >=32 rule and bias
The next diagram shows the execution time of profile-0 at stage-0 and stage-1. The axis scale is logarithmic. The stage-0 time is very fast, and is negligible comparing to that of stage-1. We can anticipate that:

the overall compute time is almost same as that of stage-1
when run with multi threads n, the per-thread execution time should be 1/n of the single thread.


The next diagram shows the execution time of profile-1 at stage-1 (BLAS). The axis scale is logarithmic. It's almost near constant when M <= 64, otherwise the Δt/ΔM goes up more and more finally the time becomes linear to M.  I guess the reason why the time increases so much when M>64 is because 4096x4096x64 is the total 1 billon number of float32 to allocate at 32GiB memory, this is identical to my device memory. When it exceeds max memory, the OS has to compress memory or use swap, this would greatly hurt performance.

The next picture is used to explain bias ranges in current master code. Let's firstly find the points that the blue line intersects with other lines. The blue line represents the overall execution time for profile-2, whereas other 4 lines represent the overall execution time for profile-0 at that n_threads. Every line for profile-0 intersects with the line for profile-2 at some point. So given n_threads and M, we can easily determine the fastest profile (line) by simply having a glance at the intersection point.  For those Ms not in x-axis, we can easily estimate the corresponding time.
Now let's focus on the vertical line at M=32. Given n_threads, we can find the corresponding line for profile-0 and profile-2.
Let's recall the default profile selecting policy in master code: M >=32 && N >= 32 && K >=32. This means: for NxK= 4096x4096, when M <32 we follow the line for profile-0, otherwise follow the line for profile-2.
This is ideal when the two line intersect at M=32, otherwise the estimation bias will show up for those Ms between the intersection point and 32. We can see that for any line of profile-0, the bias goes up from 0 (at intersection point) to |t0-t1| (at M=32), where t0 is the profile-0 time and t2 is the profile-2 time. The max bias is so large that may reach up  to 30% for n_threads=1 and 2, and up to 60% for n_thread=4 or 6. Of course, with the increasing of n_threads, the spinning and memory contention or cache miss would cause certain performance degradation, finally the per-thread average time would not reach that ideal (small) value.
As I had said before, M is the token size.  Since white spaces and stems are also be counted in the token size, for any typical question or statement, the corresponding prompt token size should is likely get closes to 32.
Anyway, nowadays personal computers tends to  have big memory and fast CPUs, thus the bias may not be noticed or tolerable.

Parallel de-quantizing
The next two pictures shows the trend of de-quantization time at INIT stage  as a percentage of the whole execution time. In theory, de-quantization (INIT) time is determined by N/K only, so it can be seen as a constant. But BLAS time increases after M>64.
The important thing to learn from this plotting is: the INIT time is near or bigger than the COMPUTE time at pretty large M range: up to 128!  It is about 1/3 of the overall time even at M=256. So if we run INIT with multi-threads, we can get far better performance than single thread. Ideally,  we can speed up over 50% when M <= 64, and 30% ~ 40 % when M between 64 and 128.


Finally I show you the multi-threaded plotting, for simplicity purpose I  just show nth=1 and nth=4. From this picture  we can see that: M at intersection point increases with n_threads. I've seen that there is no intersection point at all when n_threads=8: that means the pure CPU solution always run faster than BLAS solution even if both run with multi-threads.
With fine tuning, given model, type, M,N,K and n_threads, we will able to select the correct profile.

Other images
I will not explain them. The important reason that I list these images is: show similarity and minor differences.


How to evaluate
Build with make or CMake
Make sure one of the BLAS vendor is enabled and compiled into program.
#Accelerate: make clean; LLAMA_NO_ACCELERATE=  make
#OpenBLAS:   make clean; LLAMA_NO_ACCELERATE=1 LLAMA_OPENBLAS=1 make
#BLIS:       make clean; LLAMA_NO_ACCELERATE=1 LLAMA_BLIS=1 make
#CLBLAST     make clean; LLAMA_CLBLAST=1 make

#CUDA is supported, but not well tested, may not run at all.
Evaluate:
NOTE  when GPU offloading is enabled (-ngl > 0), mul_mat tuning is disabled atomatically.
# help
./mulmat-tune -h

# tune, use default config, 7B, Q4_0, n_threads=4, ...
./mulmat-tune

#tune and run
./main ... --tune

# tune and save file, exit.
./main ... --tune --tune-file=<FILE>

# load and run:
./main ... --tune-file=<FILE>

./perplexity ... --tune

Have a look at examples/mulmat-tune/README.md for details
Conclusion
Software systems are complicated. It's hard to optimize when target platforms vary widely. I'm certain that the speed up to q_f32 would not become reality without the new threading infrastructure, task config profile and the mulmat tune tool. I'm happy that for so long time I finally able to show you the working codes. Enjoy!
@ggerganov @SlyEcho @0cc4m @JohannesGaessler @zenixls2 @slaren
EDITED on Jun 18

typos
hide 5 images
tune: sync with latest changes

EDITED ON Jun 26
I haven't  updated this PR for a few days, because of the following reasons I think:

to support tuning, this PR introduced too many updates.
the threading implementation is ugly and full of tricks, not well-tested.
hard to test for Windows and CL/CUDA due to limited personal devices.
controversial design of task profiles: intrusive.
hard to merge even pieces of codes, tends to become trouble maker.
finally, in favor of ggerganov/ggml#293

Great thanks to @KerfuffleV2 for help testing and all of you who took time on this PR.
I'm sorry @ggerganov this took you time to review, so I close this PR?",8,65
1638,2023-05-29T13:01:22Z,2023-06-10T07:49:40Z,2023-06-10T07:49:40Z,1,9,7,"Fix issue #1279

When using gcc < 8 (i.e., gcc-5 gcc-7), _mm256_set_m128i is unavailable and will fail the compilation.
Here I provide a workaround for missing _mm256_set_m128 in GCC, the script I used to evaluate that the two expressions are equivalent:
// eval.cc
// Copyright [2023-05-29] <sxc19@mails.tsinghua.edu.cn, Xingchen Song>
#include <iostream>
#include <immintrin.h>

#define MM256_SET_M128I_1(a, b) _mm256_set_m128i((a), (b))
#define MM256_SET_M128I_2(a, b) _mm256_insertf128_si256(_mm256_castsi128_si256(b), (a), 1)

int main() {
    std::cout << __GNUC__ << std::endl;
    // Sample input data
    __m128i i32[2];
    i32[0] = _mm_set_epi32(4, 3, 2, 1);
    i32[1] = _mm_set_epi32(8, 7, 6, 5);

    // Evaluate the first expression
    __m256 result1 = _mm256_cvtepi32_ps(MM256_SET_M128I_1(i32[0], i32[1]));

    // Evaluate the second expression
    __m256 result2 = _mm256_cvtepi32_ps(MM256_SET_M128I_2(i32[0], i32[1]));

    // Compare the results element-wise
    bool equal = true;
    for (int i = 0; i < 8; ++i) {
        printf(""result1[%d] = %f, result2[%d] = %f\n"", i, result1[i], i, result2[i]);
        if (result1[i] != result2[i]) {
            equal = false;
            break;
        }
    }

    // Print the comparison result
    if (equal) {
        std::cout << ""The two expressions are equivalent."" << std::endl;
    } else {
        std::cout << ""The two expressions are not equivalent."" << std::endl;
    }

    return 0;
}

Below is a double-check that gcc-5 & gcc-7 will fail due to the missing of _mm256_set_m128:",4,9
1640,2023-05-29T15:57:27Z,2023-06-07T02:10:17Z,2023-06-07T02:10:17Z,3,6,2,"The prompt cache constitutes a nice speed up when using the same prompt prefix across multiple evaluations, but when using it, it will also be updated, which is not always desirable. One use case is to have a large prompt containing some context and usage rules, and a second part containing variable data of the problem being studied. In this case it's desirable to be able to save the first part once, and to always reuse it as-is without updating it with the second part.
The new argument --prompt-cache-ro enables this read-only mode on the prompt cache. The prompt's contents that match the cache are loaded from the cache but the rest is not modified. This allowed to reduce a total analysis time from 112s to 49.7s here, without having to backup and restore a copy of the prompt, which takes significant time at 500 MB.",4,8
1641,2023-05-29T17:07:27Z,2023-06-03T12:11:53Z,2023-06-03T12:11:53Z,2,5,3,"Deprecation disclaimer was added to convert-pth-to-ggml.py
I'm keeping the script around for now, as it may be needed for backward compat.
That being said, removing it in near future would be prudent.
I've verified that the convert.py can take other arguments when called in container.
Although I haven't tested all possible combinations.
#1628",4,18
1642,2023-05-29T17:53:41Z,2023-06-04T20:34:30Z,2023-06-04T20:34:30Z,17,1676,93,"Add full GPU inference of LLaMA on Apple Silicon using Metal

The initial idea was proposed and explained here: #915
A basic PoC was demonstrated here: ggerganov/ggml#108

Demo
M1 Pro + 7B LLaMA:

  
    
    

    llama-metal-0.mp4
    
  

  

  


M2 Max + 7B LLaMA:

  
    
    

    llama-metal-1-lq.mp4
    
  

  

  


M2 Max + 13B LLaMA:

  
    
    

    llama-metal-13B-0-lq.mp4
    
  

  

  


M2 Max + 65B LLaMa:

  
    
    

    llama-metal-65B-0-lq.mp4
    
  

  

  


Details

The ggml API is extended in ggml-metal.h
The Metal shaders / kernels are implemented in ggml-metal.metal
This PR implements support only for Q4_0, but all other quantizations can easily be added in the future
Works well with mmap to avoid model data duplication in memory. Still there are a few memory improvements that can be made in the future to reduce the memory usage when Metal is enabled
The core of the implementation is contained in the ggml_metal_graph_compute() function. It is analogous to the CPU-only ggml_graph_compute() and it's purpose is to evaluate a ggml_cgraph on the GPU in a similar way
The implemented shaders currently focus on qMatrix x Vector multiplication which is normally needed for LLM text-generation. For other tasks that involve Matrix x Matrix (for example prompt ingestion, perplexity computation, etc) we don't have an efficient implementation yet, so we fallback to the CPU / ANE
There is a nice separation of the implementation: the new ggml-metal.h, ggml-metal.m and ggml-metal.metal files are optional and all Metal-related code is contained within them. 3rd party user apps can decide whether they want to include / modify / ignore them
The proposed implementation can be easily extended for other backends like CUDA by following the same pattern as demonstrated in this PR
Optionally, we now have support for exporting static computation graphs. Creation and usage is demonstrated in the metal example

Usage

Add LLAMA_METAL=1 to your make command or -DLLAMA_METAL=ON to your cmake command.
Add -ngl 1 to main command-line arguments to enable GPU inference

$ make clean
$ LLAMA_METAL=1 make -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" --ignore-eos -n 64 -ngl 1

I llama.cpp build info: 
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL
I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG   -c ggml.c -o ggml.o
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL -c llama.cpp -o llama.o
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL -c examples/common.cpp -o common.o
cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -DGGML_USE_ACCELERATE -DGGML_USE_METAL -DGGML_METAL_NDEBUG -c ggml-metal.m -o ggml-metal.o
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL examples/main/main.cpp ggml.o llama.o common.o ggml-metal.o -o main  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL examples/quantize/quantize.cpp ggml.o llama.o ggml-metal.o -o quantize  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL examples/quantize-stats/quantize-stats.cpp ggml.o llama.o ggml-metal.o -o quantize-stats  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL examples/perplexity/perplexity.cpp ggml.o llama.o common.o ggml-metal.o -o perplexity  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL examples/embedding/embedding.cpp ggml.o llama.o common.o ggml-metal.o -o embedding  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders
c++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_METAL pocs/vdot/vdot.cpp ggml.o ggml-metal.o -o vdot  -framework Accelerate -framework Foundation -framework Metal -framework MetalKit -framework MetalPerformanceShaders

====  Run ./main -h for help.  ====

main: build = 653 (db3db9e)
main: seed  = 1685893102
llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0.07 MB
llama_model_load_internal: mem required  = 5407.71 MB (+ 1026.00 MB per state)
.
llama_init_from_file: kv self size  =  256.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading '/Users/ggerganov/development/github/llama.cpp/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x120a06020
ggml_metal_init: loaded kernel_mul                            0x120a065a0
ggml_metal_init: loaded kernel_mul_row                        0x120a06bd0
ggml_metal_init: loaded kernel_scale                          0x120a070f0
ggml_metal_init: loaded kernel_silu                           0x120a07610
ggml_metal_init: loaded kernel_relu                           0x120a07b30
ggml_metal_init: loaded kernel_soft_max                       0x120a081e0
ggml_metal_init: loaded kernel_diag_mask_inf                  0x120a08840
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x120a08ec0
ggml_metal_init: loaded kernel_rms_norm                       0x120a09570
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x120a09dd0
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x120a0a7a0
ggml_metal_init: loaded kernel_rope                           0x120a0b090
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x120a0b920
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x120a0c1b0
ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.07 MB
ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   768.00 MB
ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   258.00 MB
ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB
ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 0


 I believe the meaning of life is to be happy.
That's what I would call my philosophy on how to live life, that's what I want people to remember me for.
I was actually diagnosed with a tumor when I was 17 years old and had a very long surgery in order to get it removed.

llama_print_timings:        load time =  1685.43 ms
llama_print_timings:      sample time =    45.70 ms /    64 runs   (    0.71 ms per token)
llama_print_timings: prompt eval time =   342.51 ms /     8 tokens (   42.81 ms per token)
llama_print_timings:        eval time =  3079.50 ms /    63 runs   (   48.88 ms per token)
llama_print_timings:       total time =  4816.85 ms
Implementation process of this PR (archive)


 Export a ggml computation graph of a LLaMA model:
./bin/main -m ../models/7B/ggml-model-q4_0.bin --export
This creates the llama.ggml file which contains the computation graph


 We will now load it with a separate tool and attempt to evaluate with Metal:
./bin/mtl llama.ggml


 Implement the entire network layer by layer, comparing the CPU and GPU results

 GET_ROWS_Q4_0
 RMS_NORM
 MUL
 MUL_MAT
 RESHAPE
 TRANSPOSE
 ROPE
 VIEW
 CPY
 SCALE
 DIAG_MASK_INF
 SOFT_MAX
 SILU



 Optimize the kernels to achieve at the very least parity with CPU-only speed


 Adjust dynamic shapes before evaluating the graph (i.e. n_past, N)


 Simplify encoder dispatch code, reduce duplication


 Add basic text-generation example



Robots

🤖 Generated by Copilot at 324e823
Summary
🍎📝🚀

This pull request adds Metal support for llama, a library for tensor manipulation and computation graph export/import. It introduces a new CMake option LLAMA_METAL and a new header file ggml-metal.h that enable GPU acceleration of llama expressions on Apple devices. It also improves the readability, consistency, and usability of the existing code and documentation, and adds some new features and examples. It fixes a bug in the main example program and adds a new metal example program that demonstrates how to evaluate a statically exported ggml computation graph with Metal.

If you want to use llama with Metal
You can now do so with this pull request, all
You need is to set LLAMA_METAL
And then you can export your ggml
To a file or a graph that is special

Walkthrough

Add Metal support for llama, a GPU backend for Apple devices (link, link, link, link, link, link, link, link, link, link, link, link, link, link, link)
Fix a bug in the example program main.cpp that used subtraction instead of addition to compute the sum of two numbers (link)
Add a command-line option --export to the example program main.cpp that allows exporting the computation graph to a file named llama.ggml (link, link, link)
Add a function llama_eval_export that exports a static computation graph for a context of 511 and a batch size of 1 using llama_eval_internal (link, link)
Change the logic of the function ggml_graph_import to parse the arguments of the tensor before creating it, and to handle different cases of view operations differently (link, link)
Change the logic of the function ggml_nbytes to handle cases where the tensor is not contiguous in memory (link)
Add a call to ggml_scratch_save and ggml_scratch_load to the functions ggml_view_1d, ggml_view_2d, ggml_view_3d and ggml_view_4d to preserve the scratch memory state when creating a new tensor for the offset (link, link, link, link)
Add a call to ggml_set_name to the functions ggml_view_2d, ggml_view_3d and ggml_view_4d to assign a name to the result tensor for debugging purposes (link, link, link)
Add a call to ggml_set_name to the function llama_eval_internal to assign a name to the tensor Vcur for debugging purposes (link)
Add a parameter cgraph_fname to the function llama_eval_internal that allows exporting the computation graph to a file if not null (link, link, link)
Add a variable eop to the function ggml_graph_import that stores the enum value of the operation code for convenience (link)
Add a const qualifier to the variables mean and x0 in the functions ggml_compute_forward_rms_norm_f32 and ggml_compute_forward_rope_f32 to indicate that they are not modified after initialization (link, link, link)
Change the return type of the function ggml_nrows from int to int64_t to match the type of the ne field of the ggml_tensor struct (link)
Change the visibility of the functions ggml_is_transposed and ggml_is_contiguous from static inline to public by adding them to the ggml.h header file (link, link)
Increase the width of the last column in the format strings of the functions ggml_graph_export_leaf and ggml_graph_export_node to accommodate longer tensor names (link, link)
Comment out two assertions in the function ggml_graph_export that check the work buffer size of the computation graph, because they are not valid when exporting a graph with Metal support (link)
Remove an empty line from the function ggml_graph_export for consistency (link)
Remove the declaration of the variable cur from the function llama_eval_internal because it is declared later in the same scope (link)
Replace the variable inpL with cur in the function llama_eval_internal to reflect the previous changes in the tensor creation logic (link, link)
Remove an empty line from the function llama_eval_internal for consistency (link)
Add an empty line to the function llama_eval_internal for readability (link)
Format the call to llama_model_load in the function llama_init to use multiple lines and indentation for readability (link)
Format the declarations of the functions ggml_init and ggml_free in the ggml.h header file to use multiple lines and indentation for readability (link)
Format the target link libraries command for llama to use multiple lines and indentation for readability (link)
Align the spacing of the memory requirements expressions in the function llama_model_load_internal for readability (link)
Align the spacing of the CMake options for llama to make them more consistent and readable (link)
Rename the variable GGML_CUDA_SOURCES to GGML_SOURCES_CUDA to match the naming convention of other source variables in the CMake file (link, link)
Add a subdirectory metal to the examples CMake file if LLAMA_METAL is enabled (link)
Add an empty line to the README.md file for readability (link)
Add empty lines to the Makefile to separate different conditional blocks for readability (link, link, link)
Add comments to mark the end of the conditional blocks in the Makefile (link, link, link)",26,135
1652,2023-05-30T16:16:18Z,2023-06-13T19:04:43Z,2023-06-13T19:04:43Z,10,5536,308,"I improved the training process (#1360) by some orders of magnitudes, replacing naive backward passes with dedicated operations.
The training can now also use flash attention to support bigger context sizes.
There is a self contained example which allows to train small llama models from scratch.
The vocabulary that is used will be loaded from a source llama model.
To be able to resume training from previous runs a training checkpoint file is used to store the optimizer context and model weights.
After training the loaded checkpoint can be exported as llama compatible model file in F32 format.
List of all new operations:

GGML_OP_REPEAT_BACK : To get rid of 10 operations per repeat backward pass.
GGML_OP_OUT_PROD : Similar to MAT_MUL, but the second dimensions must be equal instead of the first dimension. Internally uses vec_mad instead of vec_dot which is used by matmul.
GGML_OP_SOFT_MAX_BACK : To get rid of big intermediate matrices, reducing runtime from quadratic to linear and eliminate quadratic memory overhead, now there is none.
GGML_OP_FLASH_ATTN_BACK : Flash attention combines softmax(K*Q*V) into one operation. Using this with the new backward pass saves a lot of operations and memory overhead during training.
GGML_OP_CROSS_ENTROPY_LOSS : Combines -sum(log(softmax(a))*b) into one dedicated operation. As the vocabulary is quite big (32k) the loss function deals with big matrices, as dedicated operation a lot of memory overhead can be saved.
GGML_OP_CROSS_ENTROPY_LOSS_BACK : Backward pass of cross entropy loss.

Changes to optimizers:
AdamW was easy to implement by changing Adam.
When the corresponding parameter is zero it mathematically simplifies to the regular Adam optimizer.
So instead of implementing a whole new optimizer, AdamW weight decay was implemented directly in Adam.
Adam tracks statistics about the gradients, which are very important for training.
It was necessary to persist the state of the optimizer between ggml_opt calls.
For this I added struct ggml_opt_context, ggml_opt_init() and ggml_opt_resume().
The regular ggml_opt() call will now internally create a new opt context, initialize it and then use resume to optimize using this fresh context.
This moves all the memory allocation done by the optimizers into ggml_opt_init.
I investigated the use of flash_ff, but it seems to implement a different feedforward than the one used in llama.
Having a dedicated operation for the feedforward of llama could also save a lot of memory overhead, allowing bigger n_embd and n_ff.
Might be worth to look into.
There is still a lot of unnecessary memory overhead.
In the llama eval function similar overhead is avoided using scratch buffers.
Would be good to find a way to do similar with the (automatically generated) backward pass.
Other noteworthy changes:

added llama api function to get vocabulary data from llama context
added a fix to llama_model_load_internal for models with n_layer<32 to be recognized as MODEL_7B, so we can load small self trained models - they are probably smaller than 7B.
bugfix in llama_sample_token_mirostat_v2 when all candidates are filtered out, which can happen with freshly trained models

Also see:

ggerganov/ggml#8 (comment)
#1360",20,64
1653,2023-05-30T17:12:37Z,2023-06-04T06:12:07Z,2023-06-04T06:12:07Z,4,210,40,"Port further improvements from the CUDA version to OpenCL, specifically:

No more duplication of layers, they will now be solely in VRAM if offloaded to a GPU
The norm calculation is now also done on GPU for that reason",5,10
1659,2023-06-01T00:48:54Z,2023-06-10T06:41:59Z,2023-06-10T06:41:59Z,1,4,0,"Update Makefile to add SSSE3 compilation use cases
See #1413",2,0
1666,2023-06-01T14:19:49Z,,2023-06-09T14:12:03Z,5,155,3,"NF4 is not friendly for SIMD but it should be okay for CUDA/CL.
Not sure if it is any good.",1,0
1673,2023-06-02T09:44:20Z,2023-06-08T07:58:54Z,2023-06-08T07:58:54Z,2,4,0,#1649,3,5
1674,2023-06-02T12:32:54Z,2023-06-05T20:32:36Z,2023-06-05T20:32:36Z,3,47,6,"Added ""docs"" top level folder
Moved BLIS.md from the top level folder to docs (and fixed relevant links to the file)
Created a new .md file that contains performance troubleshooting and benchmarking information

Note: I've chosen not to use the wiki because as far as I can tell it is not open to outsider contributions
Relates to #1665",2,0
1675,2023-06-02T15:27:56Z,2023-06-06T17:00:02Z,2023-06-06T17:00:02Z,1,46,21,"This PR builds on top of occam's changes, #1653, which should be merged first.
Summary: This solves a problem where allocating and freeing gradually increasing sized device buffers in sequence ends up creating a lot of useless buffers that do not get fully utilized, squandering precious VRAM. For example, allocating and then freeing a ggml_cl_pool_malloc in the sequence 1gb->2gb->3gb->4gb ends up using a total of 10gb, whereas after this PR only 4gb would be utilized.
Clblast fixes + enhancements to save VRAM and offload more layers:

Change all Clblast buffers to CL_MEM_READ_WRITE, as the pool malloc currently doesn't properly handle them, since the original flags are not currently preserved.
When recycling buffers in pool malloc, always assign the SMALLEST available buffer that fits, instead of the FIRST available buffer.
When failing to recycle a buffer in pool malloc (if all existing free buffers are too small), instead recycle the largest available free buffer by resizing it.

With these enhancements I was able to load 2 additional GPU layers into VRAM, at no/minimal observed cost to performance.
This PR is built based off the master branch of llama.cpp and follows it's MIT license in full.",3,2
1678,2023-06-03T05:13:36Z,2023-06-03T11:28:45Z,2023-06-03T11:28:45Z,1,7,6,"Fixes #1670, by reworking the original fix for #1585 from #1609.
The original fix examined embd to determine if the prompt had been evaluated, but embd is limited to the batch size. In addition, that fix left session_tokens in its original state (i.e., the longer, cached prompt), while normal session evaluation truncates it at the first eval. This combination meant that any prompts with a cache hit on just the first batch (512 by default) would begin eval-ing ~from the second batch, and all of that eval would get appended to the end of the full, original cached prompt. This had the downstream effect of diverging the cache from the prompt and overrunning the context size in the cache, as seen in #1670.
For the fix, I opted to move the re-eval logic to main's initialization rather than at the eval stage. Here, it transforms session_tokens such that it will only match (prompt - 1) tokens.
Testing:

for #1670, conducted a long chat with 30B, past the context rotation
for #1585, applied the Z/joke test and got a joke that did not start with ""Z""",2,1
1681,2023-06-03T12:26:01Z,2023-06-17T10:32:48Z,2023-06-17T10:32:48Z,5,42,12,"This change is more of QoL for developers. If used properly, pre-commit hooks[0] can prevent some of the more common stylistic issues. Furthermore, and in my opinion that is a considerable +, in the long run there will be fewer patches fixing various style violations, as these will be prevented in the first place.
I've only included flake8, whitespace, eof, yaml and large-file hooks, as those were the least likely to need substantial intervention from the start.
Nevertheless, small, non-functional changes were made to non-compliant files. These include breaking up long lines, whitespace sanitation and unused import removal.
Maximum line length in python files was set to a generous 125 chars, in order to minimize number of changes needed in scripts and general annoyance. The ""txt"" prompts directory is excluded from the checks as it may contain oddly formatted files and strings for a good reason.
If there is a need the checks can be expanded to more hooks[1], or the settings can be adjusted in various ways.
For now I have opted to keep things simple, so that the execution on commit is as fast as possible.
[0]https://pre-commit.com/
[1]https://pre-commit.com/hooks.html",2,2
1684,2023-06-03T15:24:31Z,2023-06-05T19:56:19Z,2023-06-05T19:56:19Z,12,3148,29,"What
This PR adds a series of 2-6 bit quantization methods, along with quantization mixes, as proposed in #1240 and #1256. Scalar, AVX2, ARM_NEON, and CUDA implementations are provided.
Why
This is best explained with the following graph, which shows perplexity on the wikitext dataset as a function of model size:

Note that the x-axis (model size in GiB) is logarithmic. The various circles on the graph show the perplexity of different quantization mixes added by this PR (see details below for explanation). The different colors indicate the LLaMA variant used (7B in black, 13B in red, 30B in blue, 65B in magenta). The solid squares in the corresponding color represent (model size, perplexity) for the original fp16 model. The dashed lines are added for convenience to allow for a better judgement of how closely the quantized models approach the fp16 perplexity. As we can see from this graph, generation performance as measured by perplexity is basically a fairly smooth function of quantized model size, and the quantization types added by the PR allow the user to pick the best performing quantized model, given the limits of their compute resources (in terms of being able to fully load the model into memory, but also in terms of inference speed, which tends to depend on the model size). As a specific example, the 2-bit quantization of the 30B model fits on the 16 GB RTX 4080 GPU that I have available, while the others do not, resulting in a large difference in inference performance.
Perhaps worth noting is that the 6-bit quantized perplexity is within 0.1% or better from the original fp16 model.
Another interesting observation is that the relative quantization error (as measured by perplexity) does not decrease with increasing number of weights in the base model, as one might hypothesize based on the lower quantization error observed at 13B compared to 7B (see, e.g., this table on the main page). The 13B model is indeed somehow better amenable to quantization, but relative quantization error goes back to the 7B level for the 30B and 65B models. This is illustrated with the following graph, which represents an alternative view of the data in the above graph, by showing the relative difference to the fp16 model in percent. Note that now the x-axis, being the ratio of the quantized size to the fp16 model size, is linear, while the y-axis (percent error) is logarithmic.

How (Details)
In the existing ggml quantization types we have ""type-0"" (Q4_0, Q5_0) and ""type-1"" (Q4_1, Q5_1). In ""type-0"", weights w are obtained from quants q using w = d * q, where d is the block scale. In ""type-1"", weights are given by w = d * q + m, where m is the block minimum. I use this to describe the quantizations being added by this PR.
The following new quantization types are added to ggml:

GGML_TYPE_Q2_K - ""type-1"" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)
GGML_TYPE_Q3_K - ""type-0"" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.
GGML_TYPE_Q4_K - ""type-1"" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.
GGML_TYPE_Q5_K - ""type-1"" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw
GGML_TYPE_Q6_K - ""type-0"" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw
GGML_TYPE_Q8_K - ""type-0"" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.

This is exposed via llama.cpp quantization types that define various ""quantization mixes"" as follows:

LLAMA_FTYPE_MOSTLY_Q2_K - uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.
LLAMA_FTYPE_MOSTLY_Q3_K_S - uses GGML_TYPE_Q3_K for all tensors
LLAMA_FTYPE_MOSTLY_Q3_K_M - uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K
LLAMA_FTYPE_MOSTLY_Q3_K_L - uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K
LLAMA_FTYPE_MOSTLY_Q4_K_S - uses GGML_TYPE_Q4_K for all tensors
LLAMA_FTYPE_MOSTLY_Q4_K_M - uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else  GGML_TYPE_Q4_K
LLAMA_FTYPE_MOSTLY_Q5_K_S - uses GGML_TYPE_Q5_K for all tensors
LLAMA_FTYPE_MOSTLY_Q5_K_M - uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else  GGML_TYPE_Q5_K
LLAMA_FTYPE_MOSTLY_Q6_K- uses 6-bit quantization (GGML_TYPE_Q8_K) for all tensors

Not mentioned explicitly above is the fact that with this PR, all quantization variants use 6-bit quantization for the output.weight tensor. This lowers the perplexity of, e.g., Q4_0 by about 0.03 at 7B.
The code is quite lengthy, so it is added via separate files k_quants.h, k_qunats.c instead of being added to ggml.c. I think that it would be better to also factor out all other quantization types from ggml.c, but that is up to @ggerganov to decide.
Performance
The following table summarizes the performance results (perplexity, model size, run time for single token prediction). It is basically designed after the corresponding table on the main page).



Model
Measure
F16
Q2_K
Q3_K_S
Q3_K_M
Q3_K_L
Q4_K_S
Q4_K_M
Q5_K_S
Q5_K_M
Q6_K




7B
perplexity
5.9066
6.7764
6.4571
6.1503
6.0869
6.0215
5.9601
5.9419
5.9208
5.9110


7B
file size
13.0G
2.67G
2.75G
3.06G
3.35G
3.56G
3.80G
4.33G
4.45G
5.15G


7B
ms/tok@4th, M2 Max
116
56
81
69
76
50
55
70
71
75


7B
ms/tok@8th, M2 Max
111
36
46
36
46
36
40
44
46
51


7B
ms/tok@4th, RTX-4080
60
15.5
18.6
17.0
17.7
15.5
16.0
16.7
16.9
18.3


7B
ms/tok@4th, Ryzen7950X
214
57
58
61
67
68
71
81
82
93


13B
perplexity
5.2543
5.8545
5.6033
5.4498
5.4063
5.3404
5.3002
5.2785
5.2638
5.2568


13B
file size
25.0G
5.13G
5.27G
5.88G
6.45G
6.80G
7.32G
8.36G
8.60G
9.95G


13B
ms/tok@4th, M2 Max
216
103
156
148
144
95
102
132
134
142


13B
ms/tok@8th, M2 Max
213
67
83
77
83
68
73
81
84
95


13B
ms/tok@4th, RTX-4080
-
25.3
29.2
29.3
25.5
26.2
26.2
28.6
28.9
30.0


13B
ms/tok@4th, Ryzen7950X
414
109
113
118
129
130
137
156
161
180



I realize the above table is not easy to read, so adding a shortened version that shows a subset of the above data:



Model
Measure
F16
Q2_K
Q3_K_M
Q4_K_S
Q5_K_S
Q6_K




7B
perplexity
5.9066
6.7764
6.1503
6.0215
5.9419
5.9110


7B
file size
13.0G
2.67G
3.06G
3.56G
4.33G
5.15G


7B
ms/tok @ 4th, M2 Max
116
56
69
50
70
75


7B
ms/tok @ 8th, M2 Max
111
36
36
36
44
51


7B
ms/tok @ 4th, RTX-4080
60
15.5
17.0
15.5
16.7
18.3


7B
ms/tok @ 4th, Ryzen
214
57
61
68
81
93


13B
perplexity
5.2543
5.8545
5.4498
5.3404
5.2785
5.2568


13B
file size
25.0G
5.13G
5.88G
6.80G
8.36G
9.95G


13B
ms/tok @ 4th, M2 Max
216
103
148
95
132
142


13B
ms/tok @ 8th, M2 Max
213
67
77
68
81
95


13B
ms/tok @ 4th, RTX-4080
-
25.3
29.3
26.2
28.6
30.0


13B
ms/tok @ 4th, Ryzen
414
109
118
130
156
180",28,75
1691,2023-06-04T17:45:31Z,2023-06-10T07:59:17Z,2023-06-10T07:59:17Z,3,134,40,"This pull changes the llama.cpp API a bit for llama_model_quantize (but there was a comment saying it wasn't ideal and probably would change) so that it takes a structure with parameters. In addition to the existing parameters that were passed separately, I added a toggle for quantizing the output tensor and a toggle for allowing quantization of non-f32/f16 models.
llama_quantize_model_internal will now just dequantize data (if possible) and the option is enabled into the same buffer used for converting f16 to f32.
I also updated the quantize example to allow --allow-requantize and --leave-output-tensor flags.

I did a little experimentation with requantizing and the effect on perplexity (my system is pretty old so I only ran perplexity for 20 chunks).
Baseline is from a reliable source, so I presume it was quantized from 16bit or 32bit.
edit: Reorganized to make comparison easier.
Requantizing (7b)
In order:

baseline 16/32bit to q4_0
q8_0 to q4_0
q8_0 to q5_1
q8_0 to q5_1 to q4_0

[1]4.4543,[2]4.9401,[3]5.8275,[4]6.4841,[5]6.5853,[6]6.5085,[7]6.6925,[8]6.8058,[9]7.1425,[10]7.3864,[11]7.5937,[12]7.6130,[13]7.5411,[14]7.6129,[15]7.8702,[16]7.4694,[17]7.3520,[18]7.3030,[19]6.9404,[20]6.9317
[1]4.5288,[2]5.0356,[3]5.9668,[4]6.6169,[5]6.7260,[6]6.6440,[7]6.8366,[8]6.9398,[9]7.2729,[10]7.5059,[11]7.7240,[12]7.7457,[13]7.6775,[14]7.7551,[15]8.0253,[16]7.6084,[17]7.4895,[18]7.4435,[19]7.0696,[20]7.0562
[1]4.2760,[2]4.7250,[3]5.6158,[4]6.2080,[5]6.3382,[6]6.2962,[7]6.4810,[8]6.5727,[9]6.9023,[10]7.1424,[11]7.3396,[12]7.3646,[13]7.2815,[14]7.3281,[15]7.5752,[16]7.2036,[17]7.0927,[18]7.0414,[19]6.6903,[20]6.6769
[1]4.4667,[2]5.0004,[3]5.9462,[4]6.5659,[5]6.6665,[6]6.5793,[7]6.7940,[8]6.8855,[9]7.2347,[10]7.4503,[11]7.6859,[12]7.7178,[13]7.6519,[14]7.7240,[15]7.9988,[16]7.5971,[17]7.4865,[18]7.4467,[19]7.0744,[20]7.0560

Requantizing (7b) with --leave-output-tensor:
In order:

baseline 16/32bit to q4_0 (model from HF, so output tensor is quantized)
q8_0 to q4_0
q8_0 to q5_1
q8_0 to q5_1 to q4_0

[1]4.4543,[2]4.9401,[3]5.8275,[4]6.4841,[5]6.5853,[6]6.5085,[7]6.6925,[8]6.8058,[9]7.1425,[10]7.3864,[11]7.5937,[12]7.6130,[13]7.5411,[14]7.6129,[15]7.8702,[16]7.4694,[17]7.3520,[18]7.3030,[19]6.9404,[20]6.9317
[1]4.4459,[2]4.9815,[3]5.9177,[4]6.5378,[5]6.6339,[6]6.5731,[7]6.7753,[8]6.8779,[9]7.2067,[10]7.4348,[11]7.6518,[12]7.6790,[13]7.6165,[14]7.6991,[15]7.9601,[16]7.5499,[17]7.4370,[18]7.3908,[19]7.0176,[20]7.0056
[1]4.2412,[2]4.7153,[3]5.5916,[4]6.1828,[5]6.3129,[6]6.2794,[7]6.4634,[8]6.5546,[9]6.8808,[10]7.1252,[11]7.3224,[12]7.3477,[13]7.2649,[14]7.3098,[15]7.5562,[16]7.1868,[17]7.0772,[18]7.0251,[19]6.6750,[20]6.6615
[1]4.4444,[2]4.9846,[3]5.9130,[4]6.5119,[5]6.6093,[6]6.5362,[7]6.7444,[8]6.8297,[9]7.1733,[10]7.3866,[11]7.6076,[12]7.6364,[13]7.5806,[14]7.6528,[15]7.9191,[16]7.5238,[17]7.4177,[18]7.3747,[19]7.0068,[20]6.9927


edit: Added 33b data.
Requantizing (33b)

baseline 16/32bit to q4_0 (model from HF, so output tensor is quantized)
q8_0 to q4_0
q8_0 to q5_1

[1]3.3109,[2]3.7188,[3]4.4459,[4]4.4308,[5]4.3045,[6]4.2951,[7]4.4645,[8]4.5540,[9]4.7997,[10]5.0184,[11]5.1678,[12]5.2154,[13]5.1869,[14]5.2832,[15]5.4346,[16]5.2159,[17]5.1890,[18]5.2093,[19]5.0047,[20]5.0191
[1]3.2961,[2]3.7156,[3]4.4491,[4]4.4430,[5]4.3123,[6]4.3066,[7]4.4731,[8]4.5590,[9]4.8058,[10]5.0216,[11]5.1757,[12]5.2206,[13]5.1922,[14]5.2890,[15]5.4426,[16]5.2195,[17]5.1936,[18]5.2127,[19]5.0071,[20]5.0200
[1]3.2718,[2]3.6866,[3]4.3905,[4]4.3015,[5]4.1522,[6]4.1477,[7]4.3241,[8]4.4095,[9]4.6493,[10]4.8555,[11]4.9919,[12]5.0429,[13]5.0230,[14]5.1008,[15]5.2511,[16]5.0435,[17]5.0283,[18]5.0551,[19]4.8627,[20]4.8862

Requantizing (33b) with --leave-output-tensor

baseline 16/32bit to q4_0 (model from HF, so output tensor is quantized)
q8_0 to q4_0
q8_0 to q5_1

[1]3.3109,[2]3.7188,[3]4.4459,[4]4.4308,[5]4.3045,[6]4.2951,[7]4.4645,[8]4.5540,[9]4.7997,[10]5.0184,[11]5.1678,[12]5.2154,[13]5.1869,[14]5.2832,[15]5.4346,[16]5.2159,[17]5.1890,[18]5.2093,[19]5.0047,[20]5.0191
[1]3.3002,[2]3.7089,[3]4.4329,[4]4.4210,[5]4.2862,[6]4.2820,[7]4.4499,[8]4.5344,[9]4.7764,[10]4.9896,[11]5.1435,[12]5.1857,[13]5.1619,[14]5.2533,[15]5.4052,[16]5.1854,[17]5.1579,[18]5.1782,[19]4.9747,[20]4.9863
[1]3.2716,[2]3.6796,[3]4.3823,[4]4.2942,[5]4.1472,[6]4.1420,[7]4.3172,[8]4.4032,[9]4.6420,[10]4.8490,[11]4.9853,[12]5.0357,[13]5.0165,[14]5.0950,[15]5.2441,[16]5.0365,[17]5.0206,[18]5.0471,[19]4.8551,[20]4.8781


There is some loss even from q8_0 but it still might be worth doing in some cases. i.e. you can keep something like a q8_0 around and make other quantizations if you need them based on performance/memory constraints.
I haven't done tests with larger models, but from what I've seen 7B models are generally the ones that quantization affects the most. So while it may be borderline for 7B, it might be a lot more reasonable for something like 33b, 65b models.
Since you have to explicitly enable requantizing,  I don't think allowing this is too dangerous for users.
Note: This is lightly tested and seems to work. I once was a C developer but that was a long time ago, C++ I can bumble my way through at best.",3,3
1696,2023-06-05T04:39:04Z,2023-06-05T20:24:04Z,2023-06-05T20:24:04Z,4,38,16,"This updates ggml_metal_add_buffer to use MTLDevice.newBufferWithBytesNoCopy to attempt share buffers between CPU and GPU rather than re-allocating for the GPU.
I've been following #1642 and noticed that this might help some of the swapping-related issues some have been seeing with larger models and devices with < 96GB memory. With this change, I'm no longer seeing any swapping or odd/corrupted output.
Apologize if I missed any contribution steps, or if this change is missing something obvious. One thing I'm not sure about is whether this covers all possible cases. One thing to note is that newBufferWithBytesNoCopy requires a page-aligned source buffer. This seems to be the case with mmap, but not sure if it will remain true for all possible configurations / code paths.",6,14
1698,2023-06-05T09:09:18Z,2023-06-05T10:43:09Z,2023-06-05T10:43:09Z,1,2,2,"The 128 MB was too optimistic.
Too bad it is not dynamically computed.
Ref: #1588 (comment)",3,5
1700,2023-06-05T12:17:55Z,2023-06-05T20:28:38Z,2023-06-05T20:28:38Z,1,1,1,"Fix a typo in a command in README.md
--config instead of -config for cmake build
(exactly like how it is for cuBLAS cmake)",2,0
1701,2023-06-05T14:00:32Z,2023-06-17T16:17:23Z,2023-06-17T16:17:23Z,1,1,5,"With the upcoming changes in Arch related to the openblas package the Makefile workaround is no longer needed.
Change announcement
This change requires the user to install the new blas-openblas package.
The package is in the extra-testing repository right now so I'll keep this pr as draft until it gets merged on the stable repo.",4,5
1702,2023-06-05T15:02:10Z,2023-06-05T20:11:50Z,2023-06-05T20:11:50Z,1,12,6,"The implementation of ggml_time_us() on Windows can realistically overflow and produce wrong data (It happened to me and messed up a bunch of benchmark results).
With a timer frequency of 10,000,000 ticks per second (which I have on my machine) the internal result of the multiplication (t.QuadPart * 1000000) overflows the 63 bits after 2^63 / 1000000 / 10000000 seconds. That's about 10.5 days. Because that product is then divided by the timer frequency, the upper bits are essentially lost and the calculated timings from using this function will be wrong.
This PR changes the code so that the startup time is subtracted. That means it won't overflow when the uptime of the OS crosses these 10 day, but only when the process runs longer than 10 days.
Ideally, you'd want to use something like MulDiv for 64-bits to prevent the overflow entirely, but I couldn't find a simple implementation for that.",2,0
1706,2023-06-05T21:37:57Z,2023-06-06T03:28:18Z,2023-06-06T03:28:18Z,2,25,7,"If llama.cpp tries to allocate a Metal buffer that's bigger than the maximum then it only puts out a message that it failed to allocate. This results in the error 'ggml_metal_get_buffer: error: buffer is nil' being given endlessly.
This PR adds a check for the maximum buffer size and adds a check for a false return value from ggml_metal_add_buffer generally. It also propagates the error from llama_init_from_file by returning NULL.
Existing behavior:
main: build = 622 (f4c55d3)
main: seed  = 1
llama.cpp: loading model from /Users/spencer/ai/models/WizardLM-30B-Uncensored.ggmlv3.q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32001
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size = 17452.67 MB
llama_model_load_internal: mem required  = 2532.68 MB (+ 3124.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  780.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading '/Users/spencer/ai/repos/llama.cpp/build/bin/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x141e08850
ggml_metal_init: loaded kernel_mul                            0x141e08e50
ggml_metal_init: loaded kernel_mul_row                        0x141e09480
ggml_metal_init: loaded kernel_scale                          0x141e099a0
ggml_metal_init: loaded kernel_silu                           0x141e09ec0
ggml_metal_init: loaded kernel_relu                           0x141e0a3e0
ggml_metal_init: loaded kernel_soft_max                       0x141e0aa90
ggml_metal_init: loaded kernel_diag_mask_inf                  0x141e0b0f0
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x141e0b770
ggml_metal_init: loaded kernel_rms_norm                       0x141e0be20
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x141e0c680
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x141e0d050
ggml_metal_init: loaded kernel_rope                           0x141e0d940
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x141e0e1d0
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x141e0ea60
ggml_metal_add_buffer: failed to allocate 'data            ' buffer, size = 17452.67 MB
ggml_metal_add_buffer: allocated 'eval            ' buffer, size =  1280.00 MB
ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB
ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   512.00 MB
ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   512.00 MB
system_info: n_threads = 6 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0
USER: Write a one paragraph summary of what happened in 1918. ASSISTANT:Inggml_metal_get_buffer: error: buffer is nil
ggml_metal_get_buffer: error: buffer is nil
ggml_metal_get_buffer: error: buffer is nil
ggml_metal_get_buffer: error: buffer is nil
ggml_metal_get_buffer: error: buffer is nil
ggml_metal_get_buffer: error: buffer is nil
...
New behavior:
main: build = 622 (827fd74)
main: seed  = 1
llama.cpp: loading model from /Users/spencer/ai/models/WizardLM-30B-Uncensored.ggmlv3.q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32001
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size = 17452.67 MB
llama_model_load_internal: mem required  = 2532.68 MB (+ 3124.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  780.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading '/Users/spencer/ai/repos/llama.cpp/build/bin/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x11de07b40
ggml_metal_init: loaded kernel_mul                            0x11de08140
ggml_metal_init: loaded kernel_mul_row                        0x11de08660
ggml_metal_init: loaded kernel_scale                          0x11de08b80
ggml_metal_init: loaded kernel_silu                           0x11de090a0
ggml_metal_init: loaded kernel_relu                           0x11de095c0
ggml_metal_init: loaded kernel_soft_max                       0x11de09c70
ggml_metal_init: loaded kernel_diag_mask_inf                  0x11de0a2d0
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11de0a950
ggml_metal_init: loaded kernel_rms_norm                       0x11de0b000
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11de0b860
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11de0c230
ggml_metal_init: loaded kernel_rope                           0x11de0cb20
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11de0d3b0
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11de0dc40
ggml_metal_add_buffer: buffer 'data' size 18300452864 is larger than buffer maximum of 17179869184
llama_init_from_file: failed to add buffer
llama_init_from_gpt_params: error: failed to load model '/Users/spencer/ai/models/WizardLM-30B-Uncensored.ggmlv3.q4_0.bin'
main: error: unable to load model",6,5
1720,2023-06-06T20:00:37Z,2023-06-08T07:12:29Z,2023-06-08T07:12:29Z,1,11,11,Use macros for long long (and long long unsigned),3,5
1724,2023-06-06T22:55:01Z,2023-06-07T04:15:31Z,2023-06-07T04:15:31Z,2,42,14,This MR updates the flake in this repo to support the new LLAMA_METAL feature on aarch64 darwin!,2,0
1738,2023-06-07T08:40:17Z,2023-06-08T16:47:57Z,2023-06-08T16:47:57Z,1,6,6,"checkout latest 5c64a09@master, compilation report:
/home/wesley/Work/projects/chatgpt/llama.cpp/k_quants.c:1262:43: error: invalid initializer
/home/wesley/Work/projects/chatgpt/llama.cpp/k_quants.c:1263:43: error: invalid initializer
             const int8x16x4_t q8bytes_2 = vld1q_s8_x4(q8); q8 += 64;
                                           ^~~~~~~~~~~
/home/wesley/Work/projects/chatgpt/llama.cpp/k_quants.c: In function ‘ggml_vec_dot_q5_K_q8_K’:
/home/wesley/Work/projects/chatgpt/llama.cpp/k_quants.c:1791:41: error: invalid initializer
             const int8x16x4_t q8bytes = vld1q_s8_x4(q8); q8 += 64;
                                         ^~~~~~~~~~~

but everything is OK on x86_64, maybe arm64 does not support this intrinsic?
Using vld4q_s8 instead of vld1q_u8_x4 seems working, both on x86_64 and arm64.
However, testing did not pass all due to issue #1736 .
$ make test
Running tests...
Test project /home/wesley/Work/projects/chatgpt/llama.cpp/build
    Start 1: test-quantize-fns
1/4 Test #1: test-quantize-fns ................   Passed    0.01 sec
    Start 2: test-quantize-perf
2/4 Test #2: test-quantize-perf ...............   Passed    0.01 sec
    Start 3: test-sampling
3/4 Test #3: test-sampling ....................Child aborted***Exception:   0.05 sec
    Start 4: test-tokenizer-0
4/4 Test #4: test-tokenizer-0 .................   Passed    0.02 sec

75% tests passed, 1 tests failed out of 4

Total Test time (real) =   0.09 sec

The following tests FAILED:
	  3 - test-sampling (Child aborted)
Errors while running CTest
make: *** [Makefile:84：test] 错误 8",3,4
1741,2023-06-07T13:56:41Z,2023-06-09T16:24:41Z,2023-06-09T16:24:41Z,3,16,1,For #1456,2,2
1748,2023-06-08T03:02:26Z,2023-06-08T07:02:49Z,2023-06-08T07:02:49Z,1,1,0,"Commit to add the option to disable k_quants edaafec added GGML_USE_K_QUANTS, this needs to be set in CMakeLists.txt.",3,0
1752,2023-06-08T08:11:07Z,2023-06-08T16:46:22Z,2023-06-08T16:46:22Z,2,187,7,"28.3 ms / token for 7B on 30-core M2 Max GPU.
This corresponds to ~290 GB/s throughput, which is likely very close to the theoretical limit.
Ah, also changed the thread result accumulation in Q4_0. This reduces Q4_0 token prediction time using Metal to ~27.8 ms/token from ~28.3 ms/token on my GPU. Being not sure that this is generally the case, left the commented out original implementation behind.",3,0
1753,2023-06-08T08:57:21Z,2023-06-09T11:58:16Z,2023-06-09T11:58:16Z,1,9,0,"With my GPU changes in #1703 Windows users were reporting to get garbage outputs: #1735 . I think the problem is a bug in the Windows CUDA compiler regarding instruction reordering or optimizing out local variables. The problem can be fixed by either making a debug build or by adding assert statements. This alone would not be indicative of a bug in the compiler since it's possible that I'm simply accidentally relying on undefined behavior somewhere that randomly happens to work correctly unless the compiler does certain optimizations. However, I found that adding an assert statement for one of the variables that I use for determining indices affects another assert statement for another variable.
Specifically: my code loops over ne02 and ne03 of the ggml tensors via i02 and i03 and then does a computation for each i02 i03 bin. For multiple GPUs the edge between GPU data regions for split tensors falls into one of those bins. I'm using i01_low and ì01_high to represent the lower and upper rows of a bin that a given GPU should work on. For a single GPU i01_low == 0 and i01_high == ne01 should always be true. However, for some reason i01_high is becoming 0 on Windows which in turn means that the GPU isn't actually doing any work. I triple-checked my code for determining the indices and I'm confident that it's correct. The only possible problem could be nrows0 not being a multiple of GGML_CUDA_DMMV_Y but I've asserted that this is not the problem and the default value for GGML_CUDA_DMMV_Y is 1 anyways. I therefore think that the bug is caused by something outside llama.cpp and the most likely suspect is the compiler, especially since that is something that differs between Windows and Linux.
Feedback would be highly appreciated. In particular it would be useful for Windows users that experience the bug to check out this PR and to confirm whether the fix works. Also, please try disabling the first assert on line 1520 of ggml-cuda.cu and confirm that this causes an assertion error (if there is only a single GPU in the system).",7,15
1757,2023-06-08T12:46:56Z,,2023-06-16T03:17:28Z,1,11,10,"Since llama.cpp can get reasonable inference speeds on latest flagship Android devices, I have been experimenting linking it into an Android app and creating a JNI around it etc.
I found some compiler flags needed to be modified to make it build correctly. I believe this is because the latest Android NDK uses Clang instead of GCC.
This is what I had to do to make it build successfully.
After these changes, the project can be directly linked using Android Studio's GUI and built normally.",2,0
1762,2023-06-08T15:12:24Z,2023-06-08T19:28:22Z,2023-06-08T19:28:22Z,2,200,18,"27.1 ms / token on M2 Max 30-core GPU, so about the same speed as Q4_0. Memory throughput is ~156 GB/s.
The access pattern used in the Q2_K
CUDA implementation resulted in significantly lower performance (~31 ms/token).",4,3
1770,2023-06-09T03:31:17Z,2023-06-09T08:00:52Z,2023-06-09T08:00:52Z,2,27,0,,2,0
1772,2023-06-09T04:29:35Z,2023-06-10T14:08:11Z,2023-06-10T14:08:11Z,1,1,1,"Thanks for the excellent work.
This PR fixes the wrong address of BLIS.md.",2,0
1773,2023-06-09T05:08:03Z,2023-07-24T03:58:11Z,2023-07-24T03:58:11Z,14,977,1,"EDITED after updates
Inspired by #1397 and grantslatton's CFG work, this adds an API that takes a serialized context-free grammar to guide and constrain sampling. Also adds a sample Backus-Naur form (BNF)-like syntax in main for specifying a grammar for generations.
Testing
(M2 Max, 30B)

Chess
 % ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'A good game:\n\n' --grammar-file grammars/chess.gbnf
main: build = 674 (e550234)
main: seed  = 1688014137
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


main: grammar:
root ::= [1] [.] [ ] move [ ] move [<U+000A>] root_4 
move ::= move_5 move_9 
root_2 ::= [1-9] root_3 [.] [ ] move [ ] move [<U+000A>] 
root_3 ::= [0-9] | 
root_4 ::= root_2 root_4 | root_2 
move_5 ::= pawn | nonpawn | castle 
pawn ::= pawn_14 [a-h] [1-8] pawn_16 
nonpawn ::= [NBKQR] nonpawn_10 nonpawn_11 nonpawn_12 [a-h] [1-8] 
castle ::= [O] [-] [O] castle_17 
move_9 ::= [+#] | 
nonpawn_10 ::= [a-h] | 
nonpawn_11 ::= [1-8] | 
nonpawn_12 ::= [x] | 
pawn_13 ::= [a-h] [x] 
pawn_14 ::= pawn_13 | 
pawn_15 ::= [=] [NBKQR] 
pawn_16 ::= pawn_15 | 
castle_17 ::= [-] [O] | 

 A good game:

1. e4 e5
2. Nf3 Nc6
3. Bb5 a6
4. Ba4 Nf6

llama_print_timings:        load time =  1144.33 ms
llama_print_timings:      sample time =    35.87 ms /    32 runs   (    1.12 ms per token)
llama_print_timings: prompt eval time =  1126.34 ms /     7 tokens (  160.91 ms per token)
llama_print_timings:        eval time =  5214.99 ms /    31 runs   (  168.23 ms per token)
llama_print_timings:       total time =  6398.45 ms



""Chess"" without grammar
% ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'A good game:\n\n'  

main: build = 645 (fd0eb66)
main: seed  = 1686286016
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


 A good game:

Sir Thomas Gresham, when he was building his famous Exchange at London, had the following dialogue with a mason, whose name was Richard B
llama_print_timings:        load time =  1185.47 ms
llama_print_timings:      sample time =    21.57 ms /    32 runs   (    0.67 ms per token)
llama_print_timings: prompt eval time =  1167.67 ms /     7 tokens (  166.81 ms per token)
llama_print_timings:        eval time =  4977.97 ms /    31 runs   (  160.58 ms per token)
llama_print_timings:       total time =  6188.21 ms



Arithmetic
 % ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'Some arithmetic practice:\n\n' \                      
--grammar 'root  ::= (expr ""="" ws num ""\n"")+
expr  ::= term ([-+*/] term)*
term  ::= ident | num | ""("" ws expr "")"" ws
ident ::= [a-z] [a-z0-9_]* ws
num   ::= [0-9]+ ws
ws    ::= [ \t\n]*'
main: build = 674 (e550234)
main: seed  = 1688014196
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


main: grammar:
root ::= root_5 
root_1 ::= expr [=] ws num [<U+000A>] 
expr ::= term expr_8 
ws ::= ws_12 
num ::= num_11 ws 
root_5 ::= root_1 root_5 | root_1 
term ::= ident | num | [(] ws expr [)] ws 
expr_7 ::= [-+*/] term 
expr_8 ::= expr_7 expr_8 | 
ident ::= [a-z] ident_10 ws 
ident_10 ::= [a-z0-9_] ident_10 | 
num_11 ::= [0-9] num_11 | [0-9] 
ws_12 ::= [ <U+0009><U+000A>] ws_12 | 

 Some arithmetic practice:

10 *a*1 +b*2 =640

10 *a*2 +b*3 =656


llama_print_timings:        load time =  1165.00 ms
llama_print_timings:      sample time =    41.11 ms /    32 runs   (    1.28 ms per token)
llama_print_timings: prompt eval time =  1147.76 ms /     7 tokens (  163.97 ms per token)
llama_print_timings:        eval time =  5113.92 ms /    31 runs   (  164.97 ms per token)
llama_print_timings:       total time =  6323.27 ms



Arithmetic - no grammar
 % ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'Some arithmetic practice:\n\n'                                            
main: build = 645 (fd0eb66)
main: seed  = 1686286388
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


 Some arithmetic practice:

\begin{code}
package main

import (
    ""fmt""
)

func main() {
    fmt.Println(
llama_print_timings:        load time =  1171.65 ms
llama_print_timings:      sample time =    21.37 ms /    32 runs   (    0.67 ms per token)
llama_print_timings: prompt eval time =  1153.88 ms /     7 tokens (  164.84 ms per token)
llama_print_timings:        eval time =  4991.68 ms /    31 runs   (  161.02 ms per token)
llama_print_timings:       total time =  6187.91 ms



JSON
% ./main -m $LLAMA_30B_Q4_0 -n 64 -p $'A bit about me:\n\n' --grammar-file grammars/json.gbnf
main: build = 674 (e550234)
main: seed  = 1688014289
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 0


main: grammar:
root ::= object 
object ::= [{] ws object_11 [}] 
value ::= object | array | string | number | boolean 
array ::= [[] ws array_15 []] 
string ::= [""] string_16 [""] ws 
number ::= number_17 number_18 ws 
boolean ::= boolean_19 ws 
ws ::= [ <U+0009><U+000A>] ws | 
object_8 ::= string [:] ws value object_10 
object_9 ::= [,] ws string [:] ws value 
object_10 ::= object_9 object_10 | 
object_11 ::= object_8 | 
array_12 ::= value array_14 
array_13 ::= [,] ws value 
array_14 ::= array_13 array_14 | 
array_15 ::= array_12 | 
string_16 ::= [ <U+0009>!#-[]-~] string_16 | 
number_17 ::= [-] | 
number_18 ::= [0-9] number_18 | [0-9] 
boolean_19 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] 

 A bit about me:

{
	""fullName"": ""Ramon Rodriguez"",
	""username"": ""ramon"",
	""email"": ""ramon@mail.com"",
	""phoneNumber"": ""+1234567890"",
	""address"": {
		
llama_print_timings:        load time =  1273.70 ms
llama_print_timings:      sample time =    82.93 ms /    64 runs   (    1.30 ms per token)
llama_print_timings: prompt eval time =  1256.36 ms /     8 tokens (  157.04 ms per token)
llama_print_timings:        eval time = 10432.05 ms /    63 runs   (  165.59 ms per token)
llama_print_timings:       total time = 11795.36 ms



""JSON"" - no grammar
 % ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'A bit about me:\n\n'                                                                          
main: build = 645 (fd0eb66)
main: seed  = 1686286615
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


 A bit about me:

A former teacher, now a full-time writer. I am the author of two novels: _The Man in the Moon_ and _The Riddle
llama_print_timings:        load time =  1291.32 ms
llama_print_timings:      sample time =    21.48 ms /    32 runs   (    0.67 ms per token)
llama_print_timings: prompt eval time =  1274.63 ms /     8 tokens (  159.33 ms per token)
llama_print_timings:        eval time =  4990.01 ms /    31 runs   (  160.97 ms per token)
llama_print_timings:       total time =  6306.01 ms



Japanese
 % ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'Building a website can be done in 10 simple steps (from the original Japanese):\n\n' --grammar-file grammars/japanese.gbnf
main: build = 674 (e550234)
main: seed  = 1688013430
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


main: grammar:
root ::= root_2 root_5 
jp-char ::= hiragana | katakana | punctuation | cjk 
root_2 ::= jp-char root_2 | jp-char 
root_3 ::= [ <U+0009><U+000A>] root_4 
root_4 ::= jp-char root_4 | jp-char 
root_5 ::= root_3 root_5 | 
hiragana ::= [<U+3041>-<U+309F>] 
katakana ::= [<U+30A1>-<U+30FF>] 
punctuation ::= [<U+3001>-<U+303E>] 
cjk ::= [<U+4E00>-<U+9FFF>] 

 Building a website can be done in 10 simple steps (from the original Japanese):

一、目的は何なのか
二、お客さまを思い出して
三、お客さまのこと
llama_print_timings:        load time =  2957.19 ms
llama_print_timings:      sample time =    42.67 ms /    32 runs   (    1.33 ms per token)
llama_print_timings: prompt eval time =  2941.56 ms /    21 tokens (  140.07 ms per token)
llama_print_timings:        eval time =  5384.28 ms /    31 runs   (  173.69 ms per token)
llama_print_timings:       total time =  8387.61 ms



Japanese - no grammar
% ./main -m $LLAMA_30B_Q4_0 -n 32 -p $'Building a website can be done in 10 simple steps (from the original Japanese):\n\n' 
main: build = 674 (e550234)
main: seed  = 1688013483
llama.cpp: loading model from /Users/evan/llama-models/30B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.13 MB
llama_model_load_internal: mem required  = 19756.66 MB (+ 3124.00 MB per state)
.
llama_init_from_file: kv self size  =  780.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 32, n_keep = 0


 Building a website can be done in 10 simple steps (from the original Japanese):

1. Determine your goal for your site.
2. Make a plan.
3. Select the domain name.
4. Choose web
llama_print_timings:        load time =  2955.05 ms
llama_print_timings:      sample time =    22.96 ms /    32 runs   (    0.72 ms per token)
llama_print_timings: prompt eval time =  2937.10 ms /    21 tokens (  139.86 ms per token)
llama_print_timings:        eval time =  5032.41 ms /    31 runs   (  162.34 ms per token)
llama_print_timings:       total time =  8013.71 ms


Approach
Grammar API
The llama API accepts a data structure representing a context-free grammar over 32-bit code points:
    // grammar element type
    enum llama_gretype {
        // end of rule definition
        LLAMA_GRETYPE_END            = 0,

        // start of alternate definition for rule
        LLAMA_GRETYPE_ALT            = 1,

        // non-terminal element: reference to rule
        LLAMA_GRETYPE_RULE_REF       = 2,

        // terminal element: character (code point)
        LLAMA_GRETYPE_CHAR           = 3,

        // modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to
        // be an inclusive range ([a-z])
        LLAMA_GRETYPE_CHAR_RNG_UPPER = 4,

        // modifies a preceding LLAMA_GRETYPE_CHAR or
        // LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
        LLAMA_GRETYPE_CHAR_ALT       = 5,
    };

    typedef struct llama_grammar_element {
        enum llama_gretype type;
        uint32_t           value; // Unicode code point or rule ID
    } llama_grammar_element;

    LLAMA_API struct llama_grammar * llama_grammar_init(
            const llama_grammar_element ** rules,
                                 size_t    n_rules,
                                 size_t    start_rule_index);

Sampling
The grammar sampling code models a nondeterministic pushdown automaton, maintaining N stacks for the possible parse states. Sampling a token is done in two steps: a sampling API that filters candidates to those that match one of the parse stacks (llama_sample_grammar) and adding the chose token to the grammar (llama_grammar_accept_token).
Examples
Adds --grammar and --grammar-file arguments to main taking a simple extended BNF to constrain generations. The parser for this format is implemented in examples/grammar-parser.{h,cpp}:
// ... Supports character
// ranges, grouping, and repetition operators. As an example, a grammar for
// arithmetic might look like:
//
// root  ::= expr
// expr  ::= term ([-+*/] term)*
// term  ::= num | ""("" space expr "")"" space
// num   ::= [0-9]+ space
// space ::= [ \t\n]*

The root rule identifies the start of the grammar.
## Caveats

the binary format makes the code harder to understand and more brittle
the grammar contemplates 16-bit chars but it's just being applied to the 8-bit UTF-8 chars in token strings currently
the 1-char lookahead sampling is probably biasing generations in a weird way; further investigation on quality of outputs is probably needed",22,56
1774,2023-06-09T05:37:24Z,,2023-10-17T19:30:03Z,20,1349,212,"We add a grpc server based on http server code and some change. And I wish it will be helpful.
Buf according to grpc c++ version requirement. It will need upgrading c++11 to c++14. Maybe it will be a problem",6,3
1782,2023-06-09T16:23:19Z,2023-06-10T14:47:35Z,2023-06-10T14:47:35Z,3,15,2,Solution for #1769 issue locating ggml-metal.metal file when building llama.cpp as a shared library. This is the solution proposed by @j-f1 and @swittk all credit to them for this. I've tested and this works for the shared library as well as make and cmake builds of llama.cpp on an M1 Mac with Metal support enabled.,4,4
1785,2023-06-10T06:50:42Z,2023-06-10T08:28:11Z,2023-06-10T08:28:11Z,2,138,1,"23.3 ms / token on 30-core M2 Max for 256 tokens, so just ~1% slower than Q4_0.
This means that memory throughput is actually better than Q4_0 as the Q4_1 model is ~10% larger.
Subtracting the ~9ms currently spent on all other inference operations, we end up with ~290 GB/s memory throughput in the matrix multiplications.
When I added metal support for Q4_K it was only 3-4% slower than Q4_0. But then Q4_0 was optimized in PR #1775, so now Q4_K is 22% slower than Q4_0. Given this, I wanted to check if this 20+% difference is perhaps due to Q4_K being a ""type-1"" quantization (""type-1"" is slower than ""type-0"" on the CPU). Now we know that on metal the difference is almost negligible, so I guess the k-quants need some more optimization work.",3,0
1787,2023-06-10T11:07:49Z,2023-06-13T10:23:23Z,2023-06-13T10:23:23Z,4,154,48,"Fix an issue where quantizing didn't respect LLAMA_NO_K_QUANTS
Add brief help to the list of quantization types in the quantize tool
Ignore case for quantization type arguments in the quantize tool",6,19
1791,2023-06-10T15:25:46Z,,2023-06-10T19:57:22Z,1,7,5,"The latest commit causes an error when running cmake with LLAMA_METAL enabled.
Before:
spencer•repos/llama.cpp/build(master⚡)» cmake .. -DLLAMA_METAL=ON
-- Accelerate framework found
CMake Error at CMakeLists.txt:222 (set_target_properties):
  set_target_properties Can not find target to add properties to: llama


-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Configuring incomplete, errors occurred!

After:
spencer•repos/llama.cpp/build(cmake-fix⚡)» cmake ..
-- Accelerate framework found
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- ARM detected
-- Configuring done (0.1s)
-- Generating done (0.1s)
-- Build files have been written to: /Users/spencer/ai/repos/llama.cpp/build

I don't really know cmake but moving add_library(llama,...) to above the first place the llama target is referenced seems to fix it.",2,0
1792,2023-06-10T17:39:45Z,2023-06-10T19:51:36Z,2023-06-10T19:51:36Z,1,2,4,Fixes #1783.,3,0
1797,2023-06-11T02:16:07Z,2023-06-24T08:47:58Z,2023-06-24T08:47:58Z,13,243,91,"This pull request aims to solve the llama_state problem by making llama_context the new ""llama_state"". This approach is fully backwards compatible and has only few additions to the public API to enable separate loading of model and making multiple contexts from this single model instance. I converted the llama_model and llama_vocab members to references, now llama_context has a constructor that takes model and vocab reference as parameters,  I also added const modifier for more compiler checks (can be removed). For simplicity of the public API I moved the vocab under llama_model.",5,4
1798,2023-06-11T03:05:45Z,2023-06-11T09:38:53Z,2023-06-11T09:38:53Z,1,4,4,I believe these hashes to be correct because I verified that all my f16 model files matched the hashes listed in this file and then quantized them to q4_0 with the latest code and computed the hashes.,2,0
1800,2023-06-11T13:29:59Z,2023-06-12T17:12:47Z,2023-06-12T17:12:47Z,1,2,2,"Currently, the CI build isn't run when there are changes to ggml-cuda.cu but not to other source files. This should fix that.",3,0
1807,2023-06-11T20:41:33Z,2023-06-12T19:39:21Z,2023-06-12T19:39:21Z,3,463,135,"Performance is not quite as good as Q4_0 and Q4_1. The k-quantization needs to do quite a bit of more work when performing dot products, but that somehow did not matter on the other platforms (AVX2, ARM_NEON, CUDA). On Metal we see a significant difference:



Quantization
Time/token in ms




Q4_0
23.0


Q4_1
23.3


Q2_K
25.5


Q3_K_M
28.1


Q4_K_S
25.3


Q5_K_S
27.8


Q6_K
27.3



Times given are for the 7B model on a M2 Max 30-core GPU.",5,6
1817,2023-06-12T10:11:16Z,2023-06-12T11:31:36Z,2023-06-12T11:31:36Z,1,1,0,The number of buffers in the ggml context was left uninitialized. This leads to sporadic failures to load the model on startup. It is actually strange that the failure occurred so infrequently.,3,0
1821,2023-06-12T13:58:24Z,2023-06-13T19:37:55Z,2023-06-13T19:37:55Z,1,2,2,"Seems to be an error in the implementation of the operator!= function. It attempts to compare the this pointer (a llama_hparams_lora object) with the other pointer (a llama_hparams object) using memcmp. This can lead to incorrect results because the sizes of the objects being compared (sizeof(llama_hparams) and sizeof(llama_hparams_lora)) are different, should now be able to compare two llama_hparams_lora objects for inequality.",3,3
1823,2023-06-12T17:42:11Z,2023-06-17T14:37:49Z,2023-06-17T14:37:49Z,2,222,0,"Tested with replit model, should also enable MPT and possibly others",3,2
1825,2023-06-12T18:47:56Z,,2023-06-18T06:12:54Z,4,31,60,"This was my initial attempt at working around MTLBuffer.maxBufferLength last week. This seems to work for 7B models, but not for larger models (e.g. guanaco 65B).
I'll keep poking at it after hours, but creating this so others can take a look if/as they find time.",2,1
1827,2023-06-12T19:35:30Z,2023-06-14T17:47:20Z,2023-06-14T17:47:20Z,11,853,149,"This PR adds GPU acceleration for all remaining ggml tensors that didn't yet have it. Especially for long generations this makes a large difference because the KV cache is still CPU only on master and gets larger as the context fills up. Prompt processing is also significantly faster because the large batch size allows the more effective use of GPUs. For the following performance numbers PP is prompt processing, and TG 128/1024 are the generation of 128/1024 tokens with an empty prompt:



GPU
Test
Model
t/s master
t/s PR
Speedup




RTX 3090
PP
7b q4_0
452
990
2.19


RTX 3090
PP
13b q4_0
318
645
2.03


RTX 3090
PP
33b q4_0
155
292
1.88


RTX 3090
TG 128
7b q4_0
45.66
62.66
1.37


RTX 3090
TG 128
13b q4_0
29.94
38.21
1.28


RTX 3090
TG 128
33b q4_0
14.89
17.61
1.18


RTX 3090
TG 1024
7b q4_0
31.08
56.34
1.81


RTX 3090
TG 1024
13b q4_0
20.16
35.20
1.75


RTX 3090
TG 1024
33b q4_0
10.37
16.49
1.59



Note that I was only using a single thread for the PR since multiple threads have no benefit when all computations are on the GPU but they still add overhead.
I added CUDA kernels for scale, cpy, diag_mask_inf, and soft_max. I also added two special kernels for doing matrix vector multiplication with permuted or not contiguous inputs; they are used in conjunction with the KV cache.
Changes to ggml.c: I added a utility function ggml_is_permuted.
Things that are still to do:

 Fix VRAM memory leaks.
 Fix memory usage prints.
 Check performance for lower-end GPUs and add a --low-vram option if necessary.
 Check Windows performance and maybe disable features.
 General code cleanup.",25,88
1828,2023-06-13T01:45:35Z,2023-06-17T09:01:06Z,2023-06-17T09:01:06Z,1,41,0,"This PR introduces instructions for building the project using Termux from F-Droid. Termux offers a route to build and execute the project directly on an Android device through terminal operations, which eliminates the need for a rooted device or an SD Card.
These instructions include the utilization of OpenBLAS and CLBlast, a combination that is designed to achieve high performance on recent Android devices equipped with a GPU.
Following these instructions should allow you to build the project on your Android device using Termux, OpenBLAS, and CLBlast.
I primarily crafted this documentation as a personal reference, given the lack of such consolidated information elsewhere. However, I believe it holds considerable value for other users who might be navigating similar challenges.",2,0
1830,2023-06-13T04:27:16Z,2023-06-15T17:51:27Z,2023-06-15T17:51:27Z,1,15,3,This PR resolves the #1829 and adds a reference to the root cause (as CMake's find_package(Blas) is unlikely to get fixed).,4,4
1831,2023-06-13T05:46:16Z,2023-06-15T17:47:04Z,2023-06-15T17:47:04Z,2,2,0,"This p-r seeks to solve issue #1740
Using swift build to compile the swift package via command line works fine. However when actually using the swift package for example with Xcode some problems occurs.
Problems:

ggml.h is imported by llama.h but missing in the public header folder
ggml-metal.metal causes compile problems even though it is not marked explicitly as a source.

Solution:

Adding ggml.h to the public header folder
Explicitly exclude ggml-metal.metal

Metal is currently only available on apple platforms and platforms need to be described to satisfy Xcode.
    platforms: [.iOS(.v14), .macOS(.v10_13)],
In order to make the Swift Package usable again we could explicitly exclude the metal file for now as in this p-r or add metal support right away but then there wouldn't a direct opt in for metal which might requiere a bit more testing.
Edit:
Rephrased and clarified the problem.",2,0
1836,2023-06-13T14:10:43Z,2023-06-16T18:59:50Z,2023-06-16T18:59:50Z,1,489,4,"This adds OpenCL-support for the new k-quantizations q2_k, to q6_k (added in #1684)",4,1
1840,2023-06-13T17:15:55Z,2023-06-16T18:58:09Z,2023-06-16T18:58:09Z,3,191,1,"This is a minimalist example.
./simple MODEL_PATH [""PROMPT""]
It keeps generating new tokens till the kv cache is full.",3,1
1850,2023-06-14T10:26:24Z,2023-06-15T17:42:48Z,2023-06-15T17:42:48Z,2,6,2,this PR makes new training-from-scratch example build by default and fixes the paths in the example invocation.,3,1
1854,2023-06-14T14:15:49Z,2023-06-15T18:05:54Z,2023-06-15T18:05:54Z,2,44,3,,3,0
1857,2023-06-14T16:13:33Z,2023-06-15T17:36:06Z,2023-06-15T17:36:06Z,1,1,1,,3,0
1860,2023-06-14T18:09:49Z,2023-06-15T17:29:48Z,2023-06-15T17:29:48Z,2,540,516,"Reading some Metal docs, this should be more efficient way to encode the Metal kernels:

Use multiple command buffers
Enqueue them at start
Encode the commands in parallel on separate threads
Each thread commits its buffer
Wait for the last one to finish

On M1 Pro there is no difference in the measured inference time.
Still - interested to see if this is the same for other people.
Also might try this in combination with #1826 to see if this resolves the problems on some machines",5,4
1861,2023-06-14T19:25:44Z,2023-06-15T17:06:47Z,2023-06-15T17:06:47Z,1,8,0,Related to #1846 . Currently the ggml CUDA code runs on f32. Applying f16 LoRAs with working GPU acceleration therefore would require quite substantial changes and I don't consider this worth the effort at the moment. This PR adds an explicit error message stating that the simultaneous use of LoRAs and GPU acceleration is not supported.,3,3
1862,2023-06-14T19:56:19Z,2023-06-16T17:08:44Z,2023-06-16T17:08:44Z,3,385,221,"After the CUDA inference speed up in PR #1827 I got motivated to improve the performance of the k-quants, and this PR is the result.
The following table compares token prediction times on current master (i.e., after merging #1827) and this PR for all k-quantization levels. Shown is the average of 3 runs in milliseconds predicting 128 tokens on an RTX-4080 GPU using the following command:
./bin/main -m ../models/7B/qXk.bin -p ""I believe the meaning of life is"" -c 2048 -n 512 --ignore-eos -n 128 -s 1234 -t 1 -ngl 100




Quantization
Master
This PR
Speedup




Q2_K
11.94
10.62
12.4%


Q3_K_S
15.35
11.68
31.4%


Q4_K_S
12.09
10.98
10.1%


Q5_K_S
13.53
12.03
12.5%


Q6_K
14.51
13.33
8.9%


Q4_0
12.40
-
-



I have added Q4_0, not touched by this PR, for comparison. It is now more than 10% slower than Q4_K, bit it should be faster as it needs to do less work, so there is a potential for improvement there (on Metal, Q4_0 is ~10% faster than Q4_K).
I have moved away from using a templated implementation. There are subtle differences between the different k-quants in what is the best way to spread the threads in a thread group within a quantization block, and these variations are difficult to capture in a template. The result is ~100 extra lines of code compared to the templated implementation, which I think is worth it, given the increase in performance.",10,16
1863,2023-06-14T20:58:38Z,2023-06-17T12:13:05Z,2023-06-17T12:13:05Z,1,13,0,"For example
nix run github:ggerganov/llama.cpp#llama-server -- -m /path/to/model
nix run github:ggerganov/llama.cpp#embedding -- -m /path/to/model -f /path/to/file
The default is still the llama app of course",2,0
1872,2023-06-15T13:34:29Z,2023-06-15T17:29:59Z,2023-06-15T17:29:59Z,1,1,1,The valid main device is from 0 to device_count - 1.,2,0
1874,2023-06-15T13:38:45Z,2023-06-15T17:36:38Z,2023-06-15T17:36:38Z,1,4,0,Explicitly include the server make instructions for C++ noobs like me ;),2,0
1879,2023-06-15T17:58:51Z,2023-06-15T19:49:09Z,2023-06-15T19:49:09Z,1,1,1,"In one of my PRs I changed the macro for cuBLAS errors to a more human-readable format. However, the function that I used is not available in all CUDA versions so I added a check against CUDART_VERSION. However, as it turns out I did that incorrectly and llama.cpp master doesn't compile on old CUDA versions. This PR fixes that.",2,0
1886,2023-06-16T04:21:21Z,2023-06-16T18:53:04Z,2023-06-16T18:53:04Z,1,46,10,add better handling on detecting BLAS_INCLUDE_DIRS.,3,1
1887,2023-06-16T05:31:46Z,2023-08-03T02:05:44Z,2023-08-03T02:05:44Z,2,138,10,"Adds a Python script that converts a JSON schema into the grammar format from #1773. This allows generating JSON according to a schema, like Jsonformer or OpenAI's function calling.
Examples

Jsonformer Student Example
% cat ../schemas/student.json 
 {
    ""type"": ""object"",
    ""properties"": {
        ""name"": {""type"": ""string""},
        ""age"": {""type"": ""number""},
        ""is_student"": {""type"": ""boolean""},
        ""courses"": {
            ""type"": ""array"",
            ""items"": {""type"": ""string""}
        }
    }
}
% ./main -m $LLAMA_13B_Q4_0 --grammar ""$( python3 examples/json-schema-to-grammar.py ../schemas/student.json --prop-order 'is_student,name,age' )"" -p 'Hermione Granger '
main: build = 694 (e8259e4)
main: seed  = 1686892597
llama.cpp: loading model from /Users/evan/llama-models/13B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =    0.09 MB
llama_model_load_internal: mem required  = 9031.70 MB (+ 1608.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  400.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


main: grammar:
<0>space_1 ::= <2>[ - ] | 
<9>space ::= <11>space_1 
...

 Hermione Granger { ""is_student"" : true, ""name"" : ""Hermione"", ""age"" :12, ""courses"" : [ ""muggle studies"",""history of magic"" , ""charms"",""potion"" ]} [end of text]

llama_print_timings:        load time =   396.96 ms
llama_print_timings:      sample time =    55.45 ms /    57 runs   (    0.97 ms per token)
llama_print_timings: prompt eval time =   347.81 ms /     6 tokens (   57.97 ms per token)
llama_print_timings:        eval time =  3898.12 ms /    56 runs   (   69.61 ms per token)
llama_print_timings:       total time =  4306.70 ms



Jsonformer car example
% cat ../schemas/car.json 
{""type"": ""object"", ""properties"": {""car"": {""type"": ""object"", ""properties"": {""make"": {""type"": ""string""}, ""model"": {""type"": ""string""}, ""year"": {""type"": ""number""}, ""colors"": {""type"": ""array"", ""items"": {""type"": ""string""}}, ""features"": {""type"": ""object"", ""properties"": {""audio"": {""type"": ""object"", ""properties"": {""brand"": {""type"": ""string""}, ""speakers"": {""type"": ""number""}, ""hasBluetooth"": {""type"": ""boolean""}}}, ""safety"": {""type"": ""object"", ""properties"": {""airbags"": {""type"": ""number""}, ""parkingSensors"": {""type"": ""boolean""}, ""laneAssist"": {""type"": ""boolean""}}}, ""performance"": {""type"": ""object"", ""properties"": {""engine"": {""type"": ""string""}, ""horsepower"": {""type"": ""number""}, ""topSpeed"": {""type"": ""number""}}}}}}}, ""owner"": {""type"": ""object"", ""properties"": {""firstName"": {""type"": ""string""}, ""lastName"": {""type"": ""string""}, ""age"": {""type"": ""number""}}}}}
 % ./main -m $LLAMA_13B_Q4_0 --grammar ""$( python3 examples/json-schema-to-grammar.py ../schemas/car.json --prop-order 'car,make,model,owner,firstName,lastName,age,year' )"" -p 'Brought the 97 Civic in '
main: build = 694 (e8259e4)
main: seed  = 1686892847
llama.cpp: loading model from /Users/evan/llama-models/13B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =    0.09 MB
llama_model_load_internal: mem required  = 9031.70 MB (+ 1608.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  400.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


main: grammar:
<0>space_1 ::= <2>[ - ] | 
<9>space ::= <11>space_1 
...

 Brought the 97 Civic in { ""car"" : { ""make"" : ""Honda"", ""model"" : ""Civic"", ""year"" :1997, ""colors"": [ ""Black"",""Silver"",""Gray""] , ""features"":{ ""audio"": {""brand"": ""Bose"", ""hasBluetooth"": false, ""speakers"":10}, ""performance"":{""engine"": ""K20A2"", ""horsepower"":230,""topSpeed"":185},""safety"": {""airbags"":10, ""laneAssist"":false,""parkingSensors"":false}} } , ""owner"" : { ""firstName"":""Brian"",""lastName"":""O'Donnell"" , ""age"":32} } [end of text]

llama_print_timings:        load time =   324.46 ms
llama_print_timings:      sample time =   196.27 ms /   182 runs   (    1.08 ms per token)
llama_print_timings: prompt eval time =   707.57 ms /    12 tokens (   58.96 ms per token)
llama_print_timings:        eval time = 12594.43 ms /   181 runs   (   69.58 ms per token)
llama_print_timings:       total time = 13515.57 ms



OpenAI-style function calling
% cat ../schemas/functions.json 
{
    ""oneOf"": [
        {
            ""type"": ""object"",
            ""properties"": {
                ""function"": {""const"": ""create_event""},
                ""arguments"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""title"": {""type"": ""string""},
                        ""date"": {""type"": ""string""},
                        ""time"": {""type"": ""string""}
                    }
                }
            }
        },
        {
            ""type"": ""object"",
            ""properties"": {
                ""function"": {""const"": ""search""},
                ""arguments"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""query"": {""type"": ""string""}
                    }
                }
            }
        }
    ]
}
% ./main -m $LLAMA_13B_Q4_0 --grammar ""$( python3 examples/json-schema-to-grammar.py ../schemas/functions.json --prop-order 'function,arguments' )"" -p $'Transcript of AI assistant responding to user requests. It uses the APIs ""search"" and ""create_event""\n\nRequest: Call mom at 5pm \nFunction Call: '
main: build = 694 (e8259e4)
main: seed  = 1686893039
llama.cpp: loading model from /Users/evan/llama-models/13B/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =    0.09 MB
llama_model_load_internal: mem required  = 9031.70 MB (+ 1608.00 MB per state)
....................................................................................................
llama_init_from_file: kv self size  =  400.00 MB

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


main: grammar:
<0>space_1 ::= <2>[ - ] | 
<9>space ::= <11>space_1 
<15>0-function ::= ..

 Transcript of AI assistant responding to user requests. It uses the APIs ""search"" and ""create_event""

Request: Call mom at 5pm 
Function Call: {""function"":""create_event"",""arguments"":{""date"":""2017-11-16T18:00:00+00:00"",""time"":""17:00"" , ""title"":""Call my mom"" }}  [end of text]

llama_print_timings:        load time =   302.69 ms
llama_print_timings:      sample time =    63.82 ms /    63 runs   (    1.01 ms per token)
llama_print_timings: prompt eval time =  3517.46 ms /    42 tokens (   83.75 ms per token)
llama_print_timings:        eval time =  4388.51 ms /    62 runs   (   70.78 ms per token)
llama_print_timings:       total time =  7975.77 ms
% ./main -m $LLAMA_13B_Q4_0 --grammar ""$( python3 examples/json-schema-to-grammar.py ../schemas/functions.json --prop-order 'function,arguments' )"" -p $'Transcript of AI assistant responding to user requests. It uses the APIs ""search"" and ""create_event""\n\nRequest: What meetings are happening this afternoon? \nFunction Call: ' 
main: build = 694 (e8259e4)
...

 Transcript of AI assistant responding to user requests. It uses the APIs ""search"" and ""create_event""

Request: What meetings are happening this afternoon? 
Function Call: { ""function"": ""search"", ""arguments"": { ""query"": ""what meetings are happening today?"" } }  [end of text]

llama_print_timings:        load time =   300.87 ms
llama_print_timings:      sample time =    30.92 ms /    32 runs   (    0.97 ms per token)
llama_print_timings: prompt eval time =  3535.50 ms /    44 tokens (   80.35 ms per token)
llama_print_timings:        eval time =  2114.93 ms /    31 runs   (   68.22 ms per token)
llama_print_timings:       total time =  5684.63 ms",4,11
1889,2023-06-16T07:22:51Z,2023-06-16T18:23:54Z,2023-06-16T18:23:54Z,16,89,38,"This PR fixes and ignores 721 MSVC warnings leading to 0 warnings when building with MSVC and CMake. It is a purely cosmetic change and no behavior is changed.
""possible loss of data"" warnings were mostly ignored and fixed only in two places where they was a single warning in the file. The others were fixed. ggml.c has a more detailed comment as it's a more critical file and the justification should be part of its history.",4,3
1891,2023-06-16T13:40:36Z,2023-06-16T18:25:52Z,2023-06-16T18:25:52Z,1,1,1,Currently retrieving embeddings seem to be broken on master when offloading the non-repeating layers as reported by #1873 . This PR fixes this by setting the backend of the embeddings layer to GGML_BACKEND_CPU. This is a comparatively simple fix and does not impose a measurable performance difference.,3,0
1892,2023-06-16T14:20:10Z,2023-06-16T18:25:02Z,2023-06-16T18:25:02Z,1,2,0,MinGW libstdc++ may define NOMINMAX unconditionally. This fixes the case when it is already defined.,3,0
1893,2023-06-16T14:22:54Z,,2023-06-16T18:24:38Z,1,1,1,The size is unsigned and the format should match the type of the size.,3,1
1896,2023-06-16T15:22:02Z,2023-06-17T07:49:42Z,2023-06-17T07:49:42Z,1,2,0,"@Green-Sky (since you said I could tag you for cmake stuff)
Is there any reason not to do this so when using cmake ggml gets built as a library and not only a .o file?
Right now it's really awkward to build with cmake and use ggml in something like a binding because you have to manage all the extra files like k_quants.o, ggml-cuda.o, etc manually.
I know almost nothing about cmake. All I can say is it seems like this change doesn't break anything on Linux compiling with default flags, cuBLAS, CLBLAST. I don't have a way to test stuff like Metal or Accelerate.
edit: It appears the answer to my question is ""because it breaks Windows""... But why?
edit 2: Seems like it only breaks Windows with -DBUILD_SHARED_LIBS=ON (I can't tell if the second commit did anything since the workflows got cancelled before).",3,9
1898,2023-06-16T18:40:47Z,2023-06-17T17:15:02Z,2023-06-17T17:15:02Z,3,20,38,"On master a performance bottleneck for GPU token generation is currently that the GPU has to wait between computations for the CPU to provide the next task. This PR queues many tasks at once, resulting in higher performance. For the following number PP is prompt processing and TG 128/1024 is the generation of 128/1024 tokens from an empty prompt:



GPU
Test
Model
t/s master
t/s PR
Speedup




RTX 3090
PP
7b q4_0
1000
1042
1.04


RTX 3090
PP
13b q4_0
658
667
1.01


RTX 3090
PP
33b q4_0
297
299
1.01


RTX 3090
TG 128
7b q4_0
63.69
84.67
1.33


RTX 3090
TG 128
13b q4_0
38.52
46.75
1.21


RTX 3090
TG 128
33b q4_0
17.70
19.91
1.13


RTX 3090
TG 1024
7b q4_0
58.48
72.99
1.25


RTX 3090
TG 1024
13b q4_0
35.98
41.81
1.16


RTX 3090
TG 1024
33b q4_0
16.50
18.46
1.12


GTX 1070
PP
7b q4_0
310
309
1.00


GTX 1070
TG 128
7b q4_0
15.92
16.68
1.05


GTX 1070
TG 1024
7b q4_0
15.14
15.90
1.05


GTX 1070 + GTX 1050 ti
PP
7b q4_0
103
100
0.97


GTX 1070 + GTX 1050 ti
TG 128
7b q4_0
19.03
19.57
1.03


GTX 1070 + GTX 1050 ti
TG 1024
7b q4_0
17.96
18.84
1.03



The implementation works by reducing the number of CUDA streams per GPU from 16 to 1. That way the CPU can evaluate the entire ggml compute graph and queue all kernels with a guarantee that the GPU will wait for the previous kernel to finish before starting with the next one; cudaDeviceSynchronize is only called rarely when e.g. data needs to be copied from the VRAM to RAM. Theoretically you could instead e.g. use a bunch of CUDA events to do something similar but I found that this adds significant overhead.
As a side effect reducing the number of CUDA streams to 1 causes cuBLAS to become deterministic.
Multi GPU performance is only slightly improved because the splitting of data between GPUs requires synchronization; I may be able to at some point come up with a better way to do this.",4,4
1902,2023-06-16T21:49:01Z,,2023-06-19T16:41:09Z,7,9,8,"Especially with golang bindings, calling by value has the side-effect of values not being copied correctly. This has been observed with go-skynet/go-llama.cpp#105 , only when enabling CUDA and linking against it.
I've tried to trace this back together with @lu-zero with gdb - however there seems to be no obvious reason why this is happening. We suspect something going on during the linking process with golang/gcc versions, however using a pointer fixes the issue and makes the binding work as expected.",6,17
1903,2023-06-16T23:20:16Z,,2023-06-17T16:31:28Z,1,1,1,"Trying out the new ""simple"" example and it was crashing on Windows building with MSVC. More specifically the crash occurs when calling llama_get_kv_cache_token_count because n is uninitialized.",2,5
1904,2023-06-16T23:36:27Z,,2024-02-16T14:00:51Z,8,25,2,reserve() optimizes inserts as much as possible both on Debug and on Release build,3,2
1905,2023-06-17T00:15:16Z,2023-06-17T06:51:54Z,2023-06-17T06:51:54Z,1,1,1,We probably want to train using just the text of Shakespeare instead of the html of the page displaying his work.,2,0
1907,2023-06-17T06:01:41Z,2023-06-24T18:10:29Z,2023-06-24T18:10:29Z,2,2,2,Calling ggml_compute_forward when node->src0 was null was causing train-text-from-scratch.exe to terminate unexpectedly if it was compiled with CUDA on.,7,7
1908,2023-06-17T11:38:45Z,2023-06-17T15:46:15Z,2023-06-17T15:46:15Z,3,9,1,Apply the solution of #1889 to CUDA and OpenCL build.,2,0
1910,2023-06-17T13:49:29Z,2023-06-28T15:53:37Z,2023-06-28T15:53:37Z,16,811,22,Add new interface llama_eval_float to solve #1552 with examples of using it for LLaVA model.,3,7
1913,2023-06-17T18:06:41Z,2023-06-19T08:23:56Z,2023-06-19T08:23:56Z,5,158,68,"Currently dequantize mul mat vec operates on a quantized matrix and an f32 vector. This PR converts the vector to f16 and then converts it back to f32 as it's needed. Because this way each streaming multiprocessor needs to load less data during the calculation this is actually slightly faster:



GPU
Test
Model
t/s master
t/s PR
Speedup




RTX 3090
TG 128
7b q4_0
83.26
90.09
1.08


RTX 3090
TG 128
13b q4_0
46.08
50.56
1.10


RTX 3090
TG 128
33b q4_0
20.21
22.24
1.10


GTX 1070
TG 128
7b q4_0
15.93
16.25
1.02



I also tried a version in which the vector is instead converted to q8_0 but I was not able to get performance out of this; int <-> float conversion and integer arithmetic may be too slow on GPUs to make this worthwhile. There is also a version which uses f16 arithmetic but that version was only ~3% faster on an RTX 3090 and would require relatively large changes to the code. The performance on my GTX 1070 was also very bad because Pascal seems to have poor f16 performance.
I did not touch any of the k-quants because they use the vector quite differently than the older quantization methods and I wasn't sure what the correct implementation would be.",3,6
1916,2023-06-18T00:54:24Z,2023-06-26T16:46:08Z,2023-06-26T16:46:08Z,1,547,0,"I added AVX support code to below dot products functions:
void ggml_vec_dot_q2_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
void ggml_vec_dot_q3_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
void ggml_vec_dot_q4_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
void ggml_vec_dot_q5_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)
void ggml_vec_dot_q6_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy)",4,4
1917,2023-06-18T01:52:42Z,2023-06-18T04:29:47Z,2023-06-18T04:29:47Z,1,4,0,"Shutdown CMAKE warning of CUDA_ARCHITECTURES is empty for target ""ggml_static"".",3,0
1918,2023-06-18T05:34:19Z,2023-06-18T11:19:17Z,2023-06-18T11:19:17Z,1,1,1,"In the function ggml_compute_forward_add_q_f32, there seems to be a typo when calculating the address for dest_row.",4,2
1924,2023-06-18T12:15:16Z,2023-06-19T15:12:34Z,2023-06-19T15:12:34Z,2,844,101,"This brings all changes from https://github.com/ggerganov/ggml back here.
After merging, I will sync back everything to https://github.com/ggerganov/ggml from here
So this synchronisation process is starting to get very tedious and error prone.
I am thinking about utilizing git submodules
What are some other alternatives? I know it will be difficult to find a consensus where everyone is happy, but the current way of manually syncing the 3 repos (ggml, llama.cpp and whisper.cpp) does not seem sustainable
Currently, I am thinking of making a release branch in https://github.com/ggerganov/ggml that will contain just the minimum amount of ggml source and header files in the root directory. I.e. - no tests, no examples, no data, etc. The 3rd party projects, like llama.cpp, whisper.cpp and others will have this ggml branch as a submodule. This way however, there will be a synchronization between the master and release branch of https://github.com/ggerganov/ggml.
The reason I am thinking about a release branch is that the ggml repo contains too much extra stuff that usually third-party projects do not want to clone for no reason.",10,9
1925,2023-06-18T12:38:02Z,2023-06-18T14:07:10Z,2023-06-18T14:07:10Z,1,1,5,As pointed out by @slaren I accidentally added a second instance of RMS norm in #1703 and did not catch this during review. Because norms by definition do not have an effect when applied more than once this does not affect results but it's still incorrect.,3,0
1929,2023-06-18T15:44:14Z,2023-06-19T15:10:37Z,2023-06-19T15:10:37Z,1,6,0,"This fix issue #1911. When I am here, fix a cmake warning as well.",4,2
1930,2023-06-18T20:23:33Z,2023-06-19T15:14:09Z,2023-06-19T15:14:09Z,1,50,33,"The oldest GPU I have access to is a GTX-1660 with 6 GB VRAM. The PR significantly improves token prediction on this GPU for Q2_K - Q4_K and gets a small boost for Q5_K as well (see table). Q6_K was not changed. It would be nice to also have some results for the GTX-1000 series (and/or other older GPUs). The PR does not change performance on a modern GPU (with the RTX-4080 that I have available being used for testing).



Quantization
t/s master
t/s PR
speed up




Q2_K
22.6
27.4
1.21


Q3_K_S
17.5
24.3
1.39


Q4_K_S
16.3
24.4
1.50


Q5_K_S
15.1
16.1
1.07


Q6_K
16.5
16.5
1.00


Q4_0
21.4
-
-


Q5_0
15.0
-
-",6,5
1932,2023-06-19T05:43:14Z,2023-06-19T15:17:04Z,2023-06-19T15:17:04Z,1,6,2,Adding missing check for 256 divisibility before using Q6_K for the output.weight tensor.,5,3
1934,2023-06-19T06:52:56Z,2023-06-19T15:20:06Z,2023-06-19T15:20:06Z,1,2,6,"In the llama_copy_state_data function, when calling ggml_init, we are allocating a buffer outside and passing it into the ggml_init function.
This function seems to be mainly used during loading previous evals from a session file.
This allocation is not guaranteed to have memory alignment required by the GGML internals, causing ggml_assert_aligned(ctx->mem_buffer); in ggml.c:4103 to fail.
Namely, this happens with building on Android devices with RelWithDebInfo. (Though this works in both Debug and Release so is a hard one to catch.)
I've changed it so the allocation is done inside of GGML using the correct GGML_ALIGNED_MALLOC function.
This is tested to fix the assertion issue on Android platforms.
Based on my limited understanding, this change should not have any wide-spread effect on the rest of the code.",4,3
1935,2023-06-19T12:42:31Z,,2023-07-31T12:35:26Z,1,7,0,"Currently the CUDA code allocates temporary buffers during prompt processing via ggml_cuda_pool_malloc to hold the weight matrices dequantized to f32 for cuBLAS. As it turns out the amount of VRAM used for these temporary buffers is quite substantial, more than 1 GiB for 33b:



Model
Max. buffer size [MiB]
Pool VRAM master [MiB]
Pool VRAM PR [MiB]




7b q4_0
500.00
736.16
813.41


13b q4_0
625.00
995.17
813.42


33b q4_0
812.50
1436.68
813.43



One of the problems is that the buffers are allocated in a bad order: there are three relevant sizes for the dequantized matrices and they are allocated from smallest to largest. As a consequence the VRAM allocated for the first two matrices is essentially wasted. In this PR I did a hacky patch that just allocates and frees a single 813 MiB buffer during initialization that can then be reused for all three sizes. For 33b this does reduce VRAM usage by ~600 MiB but 813 MiB is still a lot for temporary buffers that are only used during prompt processing. So I think that a proper solution would be to implement matrix multiplication that does dequantization on the fly the same way that the dequantize mul mat vec kernel does. The problem is that efficiently parallelizing general matrix multiplication is very hard.
This brings me to the topic at hand: does anyone have a good idea for how to fuse dequantization and general matrix multiplication in ggml? I think I could at the very least do a basic implementation that at least greatly reduces VRAM usage but it may perform significantly worse for prompt processing, especially for GPUs without tensor cores. Ideally I would want to implement a kernel that is efficient both in terms of VRAM and speed.",7,18
1936,2023-06-19T12:50:33Z,2023-06-20T01:24:40Z,2023-06-20T01:24:40Z,2,11,12,alternative/complementary to: #1902,4,1
1937,2023-06-19T13:10:12Z,2023-06-19T22:12:39Z,2023-06-19T22:12:39Z,2,54,3,"The embedding endpoint was removed in the big server rework to simplify testing and development.
Since the code was cleaned it was not hard to add it back in with the same API as before. Since embedding now uses the same code as the rest, it also gets the improvements like caching for speedup.
I also changed/fixed a little how n_predict works, it now does not generate any tokens if n_predict: 0. This was needed for the embedding extraction because that's how embedding.cpp also works: it just evaluates the input string and this creates the embedding vectors.
n_predict: 0 may be useful for completions as well, for example to pre-evaluate a long instruction in the background while the user is typing their question, etc.
Relevant: #1915",2,2
1946,2023-06-20T02:03:10Z,,2023-06-21T14:38:07Z,1,1,1,Note on cmake version when building with CUDA.,5,12
1949,2023-06-20T08:26:44Z,2023-06-20T12:42:41Z,2023-06-20T12:42:41Z,1,1,1,Fix typo in CLBlast build README.md,3,0
1953,2023-06-20T18:45:29Z,2023-06-24T10:15:02Z,2023-06-24T10:15:02Z,2,5,3,"The canonical definition of top-p sampling is the smallest set of tokens with cumulative probability at least $p$ (https://arxiv.org/abs/1904.09751, Equation 2 on page 4). However, the top-p implementation currently has an off-by-one, instead computing the largest set with probability less than or equal to $p$. This patch fixes that off-by-one. This is useful in cases with few possible generations (e.g., true/false questions).",3,2
1954,2023-06-20T19:49:22Z,2023-06-23T08:38:02Z,2023-06-23T08:38:02Z,1,8,0,It's a good idea to show that llama.cpp supports OpenLLaMA as an alternative to Meta's original LLaMA.,2,5
1958,2023-06-21T13:19:49Z,2023-06-22T12:20:48Z,2023-06-22T12:20:48Z,1,69,22,"This move the parameter discovery up, so it can be used to convert the model, and stop guessing.
The n_mult discovery is a hack. @SlyEcho used 216, the current script iteratively tries and finds 240, but other values work too.
As said here #1588 (comment) , n_mult should be replaced with a n_ff hyperparameter in the ggml file.
disclaimer: I am not a python dev 😄",3,9
1959,2023-06-21T13:46:50Z,2023-06-21T21:49:25Z,2023-06-21T21:49:26Z,1,9,16,"In #1913 I changed cmake to use native as the CUDA arch (like the Makefile does). However, this seems to cause compilation issues for some users which seems to be related to cmake version, see #1946. This PR restores the default cmake CUDA arch to compute_52. If LLAMA_CUDA_DMMV_F16 is set the arch is instead set to compute_61. When I tested it using an RTX 3090, I was not able to discern a performance difference from using native over compute_52. This PR does not fix the inconsistent behavior between make and cmake.",3,3
1961,2023-06-21T18:28:36Z,2023-06-21T21:48:43Z,2023-06-21T21:48:43Z,1,1,1,The description for LLAMA_CUDA_KQUANTS_ITER in the readme says ' Setting this value 2 1 ' instead of ' to 1 ' which could be confusing since the possible values can be either 1 or 2.,2,0
1966,2023-06-22T10:13:55Z,2023-06-29T03:56:44Z,2023-06-29T03:56:44Z,1,352,175,"This is just a rough draft PR to kick off the porting of the improved K-Quant matmul CUDA kernels to OpenCL from #1930 and #1862 with assistance from @0cc4m
With these enhancements I was able to observe around 10-15% generation speed improvement for all K-Quants with CL compared to master, on my RTX2060.
One consideration is a proper way to set the value of K_QUANTS_PER_ITERATION, which currently presents compiler warnings.
As before, this PR is built based off the master branch of llama.cpp and follows it's MIT license in full.",5,5
1967,2023-06-22T11:36:07Z,,2023-06-27T15:03:06Z,7,177,81,"Based on the description in #1965
This adds ggml_rope_scaled, ggml_rope_scaled_inplace, ggml_rope_back_scaled ops. The existing rope ops just pass 1.0 to the impl for compatibility with the current behavior/API.
Add LLAMA_ROPE_SCALE to Makefile (note not in cmake yet), if not specified defaults to 1.0. You can run, for example, 'make LLAMA_ROPE_SCALE=0.5`.
I'd guess we probably don't want to use the LLAMA_ROPE_SCALE approach if this actually gets merged, it's mainly just there to facilitate easier testing.
edit: Theoretically this should work for CUDA and Metal now (untested). Note the GPU ops only apply when loading every possible layer into VRAM (for 7B that would be -ngl 35).",10,30
1970,2023-06-22T19:32:34Z,2023-06-28T16:35:54Z,2023-06-28T16:35:54Z,4,78,19,On master the simultaneous use of CUDA acceleration and LoRAs is currently not supported. I originally assigned this problem a low priority but due to the advent of SuperHOT I'm adding it now since it will be very useful for experimentation. The implementation in this PR is very hacky and bad but it seems to work; I prioritized making this available for developers quickly.,3,3
1973,2023-06-23T07:20:11Z,2023-06-24T10:32:13Z,2023-06-24T10:32:13Z,1,3,2,Result of the discussion on the issue #1969,2,0
1974,2023-06-23T07:34:19Z,2023-06-24T11:07:08Z,2023-06-24T11:07:08Z,1,26,24,"The path to ggml-metal.metal when building the Nix flake with nix build was incorrectly pointing to /ggml-metal.metal. This just fixes it to point to /bin/ggml-metal.metal.
I also ran nixfmt on the flake.nix file, but I can revert that if desired.",2,0
1975,2023-06-23T14:46:00Z,2023-06-24T11:02:07Z,2023-06-24T11:02:07Z,1,2,2,,3,0
1977,2023-06-23T15:49:17Z,2023-06-25T08:48:36Z,2023-06-25T08:48:36Z,1,1,1,I noticed that the order in which the samplers were applied was different compared to the main example. I moved top k to be the first one.,5,3
1978,2023-06-23T15:52:14Z,2023-06-24T10:57:18Z,2023-06-24T10:57:19Z,2,98,40,"Adds data types
Adds data values for small constants (less than 5 elements as opposed to only 1)
Adds edges from opt tensors
Adds names to view and view offset tensors
Adds ggml_format_name to set the name of a tensor with a format string

Example graph (llama 7B one layer):


🤖 Generated by Copilot at 0cc5c53
This pull request adds a new feature and improves an existing one for the ggml library. It allows naming tensors with ggml_format_name, a function that takes a format string and variable arguments. It also enhances the graph dumping functionality in ggml.c by using helper functions and showing more details about the tensors in dot format.",2,1
1981,2023-06-23T17:33:34Z,2023-06-25T05:45:44Z,2023-06-25T05:45:44Z,1,44,47,,2,0
1988,2023-06-25T03:47:45Z,2023-06-26T19:45:32Z,2023-06-26T19:45:32Z,2,7,18,"Running make made purple warning text appear on my screen and I like not seeing warnings when I run make
Some brackets to disambiguate order of operations and truncating some layer names by 1 more to avoid strncpy danger",3,0
1990,2023-06-25T07:22:32Z,2023-06-26T16:47:02Z,2023-06-26T16:47:02Z,1,59,12,"Add help option and usage message
Add iteration number option
Fix using type option made core dump

Main purpose is changing iteration number.
Because current 10 loop iteration is not making stable result.
However, it is still not stable at 10,000 and 100,000 iterations. why ...",2,0
1992,2023-06-25T13:43:01Z,,2023-07-22T11:59:58Z,4,10,11,See ggerganov/whisper.cpp#1027,2,4
1995,2023-06-25T19:12:53Z,2023-07-01T15:42:43Z,2023-07-01T15:42:43Z,2,55,9,"Try resolve ggerganov/ggml#284

comment warning for GGML_TASK_FINALIZE in ggml.h
removed codes using GGML_TASK_FINALIZE in ggml.c, including those in ggml_graph_compute.
print a warning message in ggml_compute_forward to warn unmerged incoming PR. Thread unsafe but should be OK. This message will be shown before prompt echo back in console.

Edit
I had missed a case that ggml_compute_forward_cross_entropy_loss_f32() is using GGML_TASK_FINALIZE.
Added a finalizer lookup table to patch this special case. Only one entry in it: ggml_compute_forward_cross_entropy_loss_finalizer. ggml_graph_compute_thread() was slightly updated.
Unable to test op GGML_OP_CROSS_ENTROPY_LOSS because the train always crash in the middle on my weak device.  The extracted finalizer is quite simple, so I think it's OK to bypass the test by me.
Verified the lookup table with OP_MUL_MAT,  the registered runner was called as expected.
Finally I removed GGML_TASK_FINALIZE.
Update
Also deprecated GGML_TASK_INIT; no longer touch runner codes, because that's error prone. We can cleanup them later step by step.
The GGML_OP_HAS_XXX idea is stolen from  9d058c2 Credit @zrm
@ggerganov @xaedes @zrm",3,2
1998,2023-06-26T01:36:06Z,2023-07-04T14:05:27Z,2023-07-04T14:05:27Z,9,3416,8,"I put together a simple web-chat that demonstrates how to use the SSE(ish) streaming in the server example. I also went ahead and served it from the root url, to make the server a bit more approachable.
I tried to match the spirit of llama.cpp and used minimalistic js dependencies and went with the ozempic css style of ggml.ai.
Initially I went for no-js dependencies but gave up and used a few minimal that i'm importing from js cdns instead of adding them here. Let me know if you agree with this approach. I needed microsoft's fetch-event-source for using event-source over POST (super disappointed that browsers don't support that, actually) and preact+htm for keeping my sanity with all this state,. The upshot is that everything is in one small html file. Speaking of- there is probably a better (and less fragile) way to include the server.html in the cpp binary, but it's been 25 years since I worked with cpp tooling.
(updated screenshot)",14,58
1999,2023-06-26T07:56:48Z,2023-07-07T16:24:02Z,2023-07-07T16:24:02Z,13,531,409,"Try resolve ggerganov/ggml#287
EDIT: see latest update at the end.

### Intro
The design is a bit different to the suggested one: named the buffer type as  a generalized one:ggml_cgraph_context.
struct ggml_cgraph_context {
    size_t work_size;
    void * work_data;
    bool   planned; // true means ready to compute graph nodes.
};

I'll explain planned later, let's focus on the APIs.
The first parameter ctx of ggml_graph_compute() is deprecated (pass in NULL is OK).
Removed wsize and work from ggml_cgraph, unlikely break external users because no reason to use them directly.
To avoid break external users, can not simply change the signature of ggml_graph_compute(), have to add ggml_graph_compute_v2(), the name looks weird but this is the reality :/  Now ggml_graph_compute() is a thin wrapper of `ggml_graph_compute_v2().
Extracted codes to new function ggml_graph_compute_plan(): set node->n_tasks, calculate work_size.
Usage
struct ggml_cgraph_context ctx = {0};

// case 1:
ggml_graph_compute_plan(&ctx, cgraph);
if (ctx.work_size > 0) {
    ctx.work_data = malloc(ctx.work_size);
}
ggml_graph_compute_v2(&ctx, cgraph);

// case 2:
ggml_graph_compute_v2(&ctx, cgraph);

// case 3:
ggml_graph_compute_v2(NULL, cgraph);

// case 4:
ggml_graph_compute(ggml_context *ctx, cgraph);

// case 5:
ggml_graph_compute(NULL, cgraph);

Why did I add the field planned?
Because:

The ctx is allowed to be NULL, empty or initialized by ggml_graph_compute_plan().
One of the corner cases is: the work_size and work_data can be default values even if the plan has been called.  So we can not simply determine whether or not call ggml_graph_compute_plan() by default values.
ggml_graph_compute_plan() MUST be called because it  also sets node->n_tasks. The work_size depends on n_tasks.

The planned makes plan-compute sequence stateful, not good enough. Any ideas?

Update on JUL 3

No longer consider backward compatibility, this means the plan phase MUST be executed before compute. And gml_graph_compute() no longer responsible for creating any kind of buffer. @ggerganov
Removed n_tasks from ggml_tensor; removed n_threads from ggml_cgraph. @slaren
the struct ggml_graph_compute_plan implies that it should be initialized or created by some procedure. @howard0su

Usage:
struct ggml_graph_compute_plan plan = ggml_graph_compute_make_plan(gb, params.n_threads);
if (plan.work_size > 0) {
    plan.work_data = malloc(plan.work_size);
    GGML_ASSERT(plan.work_data);
}
ggml_graph_compute(&plan, gb);
if (plan.work_data) {
    free(plan.work_data);
}

I tested main and perplexity.
Update JUL 6

🤖 Generated by Copilot at 551ed08
Summary
🛠️📚🚀

This pull request improves the performance and usability of the ggml_graph_compute function by introducing a new ggml_cplan structure and a helper function ggml_graph_compute_helper. It also adds support for specifying the number of command buffers for the Metal context, and updates the examples, tests, and workflows to use the new API and settings.

This pull request makes some changes
To ggml_graph_compute and friends
It adds a new plan
And some Metal commands
And fixes some warnings and ends

Walkthrough

Change the ggml_graph_compute function to require a ggml_cplan argument and add new functions ggml_graph_plan and ggml_graph_compute_with_ctx to support the new API (link)
Add a helper function ggml_graph_compute_helper to wrap the logic of creating and using a ggml_cplan structure and update the example code in ggml.h to use the new API (link, link, link, link, link)
Remove the fields n_threads, n_tasks, and work from the ggml_cgraph and ggml_tensor structures and add them to the ggml_cplan structure (link, link, link)
Update the llama_eval_internal function to use the new API and adjust the number of threads for the BLAS settings (link, link, link, link)
Add a parameter n_cb to the ggml_metal_init function and a function ggml_metal_set_n_cb to control the number of command buffers for the Metal context and update the ggml_metal_graph_compute function to use the n_cb field instead of the n_threads field (link, link, link, link, link)
Update the metal.cpp example to pass the number of threads to the ggml_metal_init function (link)
Update the baby-llama.cpp, benchmark-matmult.cpp, and train-text-from-scratch.cpp examples to use the new API and remove the assignments of gf.n_threads (link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link, link)
Update the llama.cpp file to use the new API and pass the number 1 to the ggml_metal_init function (link, link, link, link, link, link, link, link, link)
Enable the test-grad0.c test and update it to use the new API and ignore the double promotion warning (link, link, link, link, link, link, link)
Update the test-opt.c test to use the new API and ignore the double promotion warning (link, link, link, link, link)
Add a timeout option to the ctest command for the GitHub workflow jobs to avoid the tests from hanging or running too long and add a new environment variable GGML_NLOOP to control the number of iterations for the benchmark tests (link, link, link, link, link)",5,21
2000,2023-06-26T11:26:57Z,2023-06-28T17:13:03Z,2023-06-28T17:13:03Z,1,35,198,ref: #1991,3,1
2001,2023-06-26T11:56:07Z,2023-06-26T16:43:07Z,2023-06-26T16:43:07Z,8,1915,236,"The main purpose of this PR is to enable k-quant usage for models where tensor sizes are not a multiple of the default k-quants super-block size of 256 such as OpenLLaMA-3B and Falcon-7B. This closes #1919
The initial idea for handling this case was that we will simply pad tensor rows to be multiple of 256 and I initially started along these lines. But the change was turning much too big for my taste and was not just in isolated places related to the k-quants (as this PR), but was massively affecting the entire ggml. Hence, I eventually abandoned this approach.
Then there was the idea to use row-wise padding to 256 that but get away without major changes to the code  by simply using appropriate tensor views before operations that depend on the row size (e.g., rms norm). But that did not (easily) work because of the attention heads that make it necessary to have the padding in the middle to not change the embeddings each head is seeing, so this becomes too complicated as well.
So, at the end, here is the poor man's solution with an optional, compile-time configurable, super-block size of 64 for k-quants. To use it, simple
LLAMA_QKK_64=1 make -j

or
cmake -DLLAMA_QKK_64=1 ..

It is better to only enable LLAMA_QKK_64 if you are working with models that need it. Using super-blocks of 64 instead of 256 a) loses most of the model size saving enabled by super-blocks of 256, and b) comes with a performance penalty in the 5-15% range (except on Metal, where super-blocks of 64 run 5-10% faster without me understanding why). In addition, Q2_K becomes completely useless as its size with super-blocks of 64 is basically the same as Q3_K_S.
The PR provides implementation for CPU Scalar, ARM_NEON, AVX2, CUDA, Metal. OpenCL and AVX are missing.
In retrospect, it would have been better to finish the ggml modifications necessary for general support of tensor sizes not divisible by the quantization block size. I think this should be the longer-term goal. This will make ggml future proof against somebody coming up with the idea that tensor sizes should be picked, say, from the Fibonacci sequence rather than the currently more common approach of using powers of 2.
Below are some graphs that show perplexity versus qunatized model size for OpenLLaMA-3B and LLaMA-7B.  The first graph is for OpenLLaMA-3B, and I have added the Q4_0, Q4_1, Q5_0 and Q5_1 values in red for comparison. The second graph is for LLaMA-7B. The black circles/lines are for the default super-block size of 256, the red for super-blocks of 64, and the Q4_0...Q5_1 values are shown in blue.",5,4
2006,2023-06-26T15:00:55Z,2023-06-29T13:15:16Z,2023-06-29T13:15:16Z,10,25,23,"Based C++ document and POSIX document, random seed should be an unsigned integer. Change the code to reflect that.
In order to avoid break most the existing applications, let's use -1 as a special value to auto generate a time-based seed still.
Reported in #2002",3,1
2007,2023-06-26T15:58:15Z,2023-06-26T19:34:45Z,2023-06-26T19:34:45Z,1,2,0,"Inclusion at README of:
(Note: some Android devices, like the Zenfone 8, need the following command instead - ""export LD_LIBRARY_PATH=/system/vendor/lib64:$LD_LIBRARY_PATH"" instead. Source: https://www.reddit.com/r/termux/comments/kc3ynp/opencl_working_in_termux_more_in_comments/ )",2,0
2012,2023-06-26T20:03:42Z,,2023-06-28T17:22:45Z,2,132,137,"Replacing auto &kv with const auto &kv: By using const auto &kv instead of auto &kv, we explicitly communicate our intention to treat kv as a constant reference. This helps prevent accidental modifications and promotes safer code.
Enclosing statements in braces: By enclosing statements within braces, we ensure clear scoping and minimize the risk of errors caused by unintended code execution. This practice also enhances code maintainability and reduces ambiguity.",4,3
2016,2023-06-27T00:23:26Z,2023-06-27T05:07:13Z,2023-06-27T05:07:13Z,1,6,6,,2,0
2019,2023-06-27T13:20:32Z,,2023-06-29T15:15:26Z,6,34,3,"This PR adds the ability to use context sizes greater than the training context size by applying linear scaling to the token position before applying the RoPE operation. The idea originally came from (https://github.com/kaiokendev). See also the discussion #1965. This can be used ""out-of-the box"" for context sizes of to, say, 3072. For even larger contexts one is most likely better off fine tuning the model as discussed in #1965 and elsewhere around the Internet (but perhaps fine tuning starting from this PR would give better results?).
PR #1967 is similar to this PR. The main difference is how the linear scaling is being used. In PR #1967 the positional scaling factor is a constant defined at compile time and therefore affects evaluation also when the context size being used is less than the maximum training context size. This leads to much higher perplexities for context sizes up to 2048 (the training context size of the LLaMA models), e.g., we get ppl = 7 at a context size of 512 instead of the ppl=5.9066 obtained without scaling. In this PR we define a compile time constant (LLAMA_TRAINIG_CTX, which in turn defines GGML_TRAINING_CTX) that is the training context size of the model being used. Further, we pass the current context size n_ctx to the RoPE operation when constructing the computational graph. When the graph is evaluated, we use the actual token position when n_ctx <=  GGML_TRAINING_CTX, but scale the position with GGML_TRAINING_CTX / n_ctx when n_ctx > GGML_TRAINING_CTX.
Interestingly enough, it is better to set GGML_TRAINING_CTX to 2176 (2048 + 128) and not 2048 (better in the sense that we get slightly lower perplexities).
The following table gives a summary of perplexities obtained for the 7B LLaMA model for context sizes of up to 5120. It is currently not possible to run with n_ctx > 5120 at 7B on CUDA (we get NaNs and @JohannesGaessler is looking into the issue).  Q6_K quantization is used, which is known to match fp16 perplexity to within 0.1%.



n_ctx
Perplexity




1024
5.4351


2048
5.2855


2560
5.2232


3072
5.3003


3584
5.4725


4096
5.6743


4608
5.8487


5120
6.1153



Interesting to note that we actually outperform the n_ctx = 2048 result at n_ctx = 2560, i.e, we do get some ""free lunch"" :-)
The next table gives the results for 13B, where we currently can only go up to about n_ctx = 3540 on CUDA before getting NaNs:



n_ctx
Perplexity




2048
4.7094


2560
4.6459


3072
4.6868


3584
4.8135



Here the n_ctx = 2048 result is outperformed up to 3072 tokens.
For n_ctx <= 3072 linear scaling works best. For n_ctx > 3072 one can get slightly better results by using, e.g.,
p = p0 * a_scale / (1 + p0 / b_scale)

where p0 is the original token position and p is the token position given to RoPE. The constants a_scale and b_scale that seem to work best are given by
a_scale = sqrtf(GGML_TRAINING_CTX / n_ctx)
b_scale = a_scale * n_ctx / (1 - a_scale)

(with this being applied only when n_ctx > GGML_TRAINING_CTX). Using this approach I get ppl = 5.6472 for n_ctx = 4096 and 7B. With the gain being quite modest compared to just linear scaling and the approach more complicated, I did not add this to the PR.",15,34
2020,2023-06-27T13:56:45Z,2023-06-27T17:06:33Z,2023-06-27T17:06:33Z,1,2,1,"android's libc implementation ""bionic"" does not support setting affinity
see https://android.googlesource.com/platform/bionic/+/master/docs/status.md for a complete list of supported features.
edit: fixes #2015",2,1
2022,2023-06-27T14:32:59Z,2023-07-01T16:02:58Z,2023-07-01T16:02:58Z,1,9,2,"This pull request addresses #1716
Similar to llama_model_load, there is a llama_model_load_internal that throws exceptions for invalid cases, and the API llama_model_load handles the exceptions.",3,2
2027,2023-06-28T00:55:47Z,2023-06-28T17:26:27Z,2023-06-28T17:26:27Z,1,4,4,These are assignments to constants without using the correct const qualifier,2,0
2028,2023-06-28T00:59:45Z,2023-06-28T17:27:31Z,2023-06-28T17:27:31Z,1,2,2,Not used,2,0
2035,2023-06-28T10:11:00Z,2023-09-08T12:09:22Z,2023-09-08T12:09:22Z,10,93,33,"What is needed to build llama.cpp and examples is availability of stuff defined in The Open Group Base Specifications Issue 6 (https://pubs.opengroup.org/onlinepubs/009695399/) known also as Single Unix Specification v3 (SUSv3) or POSIX.1-2001 + XSI extensions, plus some stuff from BSD that is not specified in POSIX.1.
Well, that was true until NUMA support was added recently, so enable GNU libc extensions for Linux builds to cover that.
Not having feature test macros in source code gives greater flexibility to those wanting to reuse it in 3rd party app, as they can build it with FTMs set by Makefile here or other FTMs depending on their needs.
It builds without issues in Alpine (musl libc), Ubuntu (glibc), MSYS2.

llama : use posix_madvise() instead of madvise() derived from BSD
sed -i 's,<madvise>,posix_&,g;s,<MADV_,POSIX_&,g' llama-util.h

make : enable Darwin extensions for macOS builds
This is an attempt at fixing macOS build error coming from the fact that RLIMIT_MEMLOCK define is not available there without Darwin extensions.

cmake : follow recent FTM improvements from Makefile",3,12
2043,2023-06-28T20:34:07Z,,2023-07-31T12:35:45Z,3,520,29,"I implemented CUDA kernels that do dequantization and matrix matrix multiplication in one step. This eliminates the need for temporary buffers to hold the dequantized f32 matrix given to cuBLAS. For 33b this saves at least ~600 MiB for -ngl <= 60, and at least ~1400 MiB for -ngl >= 61. As a result a few more layers can be offloaded. For this table pp == prompt processing, tg128 == generation of 128 tokens with an empty prompt:



GPU
Model
Test
Max. -ngl master
t/s master
Max -ngl PR
t/s PR




RTX 3090
33b q5_1
pp
53
131
57
61


RTX 3090
33b q5_1
tg128
55
7.86
58
9.90


RTX 3090
33b q5_k_m
pp
55
135.19
61
53.73


RTX 3090
33b q5_k_m
tg128
58
11.39
61
14.77



Unfortunately my matrix multiplication kernel is not very good and significantly slower than cuBLAS for prompt processing. This is particularly noticeable when there is plenty of VRAM anyways:



GPU
Model
Test
t/s master
t/s PR




RTX 3090
7b f16
pp
1023
516


RTX 3090
7b q4_0
pp
1048
477


RTX 3090
7b q4_1
pp
1042
443


RTX 3090
7b q5_0
pp
1032
383


RTX 3090
7b q5_1
pp
1032
359


RTX 3090
7b q8_0
pp
1029
476


RTX 3090
7b q2_K
pp
1028
371


RTX 3090
7b q3_K_m
pp
1009
310


RTX 3090
7b q4_K_m
pp
1010
352


RTX 3090
7b q5_K_m
pp
1040
308


RTX 3090
7b q6_K
pp
1033
372



By default cuBLAS is still in use. The new dequantization + matrix multiplication kernels can be used by setting the compile option LLAMA_CUDA_DMM.
The implementation works by adding a template that takes a dequantization method. I revised the dequantization to require only the index of the data value to be dequantized. I plan to eventually apply this dequantization scheme for the other templates as well since it is simpler.
I don't understand why the performance of my matrix multiplication kernel is so bad. I tried several other variants but they all had even worse performance. I would very much welcome it if someone were to write a better one.",6,20
2054,2023-06-30T04:37:10Z,2023-07-15T10:34:17Z,2023-07-15T10:34:17Z,12,185,67,"The original RoPE has pre-defined parameters
theta_i = 10000^(−2(i−1)/d), for i in [1, 2, ..., d/2]
Our customizable RoPE, ggml_rope_custom_inplace, uses
theta_i = scale * base^(−2(i−1)/d), for i in [1, 2, ..., d/2]
with the default matches the original
scale = 1.0
base = 10000
The new command line arguments
--rope-freq-base
--rope-freq-scale
set the two new RoPE parameter.
Recent researches show changing these two parameters extends the context limit with minimal loss.


Extending Context to 8K kaiokendev https://kaiokendev.github.io/til#extending-context-to-8k


Extending Context Window of Large Language Models via Positional Interpolation Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian https://arxiv.org/abs/2306.15595


NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. https://www.reddit.com/user/bloc97 https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/


For the bold, try adding the following command line parameters to your favorite model: -c 16384 --rope-freq-base 80000 --rope-freq-scale 0.5",20,44
2055,2023-06-30T07:40:04Z,2023-07-01T17:00:26Z,2023-07-01T17:00:26Z,2,37,5,"This PR adds support of baichuan-7b model.

baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent  Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).

This model doubles the vocabulary (64k) of LLaMa, which seems interesting.",6,5
2056,2023-06-30T09:43:43Z,2023-07-01T19:47:27Z,2023-07-01T19:47:27Z,1,35,3,"As described in #2019 the current fixed size of the VRAM scratch buffer at some point becomes an issue if the context is increased beyond 2048. This PR adjusts the VRAM scratch value based on test values; it seems that 1 MB of scratch size is needed per 512 extra context and per n_head:




Model
Context size
Min. VRAM scratch size [MiB]
Delta [MiB]




7b q6_K
512
136
-


7b q6_K
1024
171
35


7b q6_K
1536
243
72


7b q6_K
2048
318
75


7b q6_K
2560
350
32


7b q6_K
3072
382
32


7b q6_K
3584
414
32


7b q6_K
4096
446
32


7b q6_K
4608
478
32


7b q6_K
5120
510
32


7b q6_K
5632
542
32


7b q6_K
6144
574
32


7b q6_K
6656
606
32


7b q6_K
7168
638
32


7b q6_K
7680
670
32


7b q6_K
8192
702
32






Model
Context size
Min. VRAM scratch size [MiB]
Delta [MiB]




13b q4_0
512
170
-


13b q4_0
1024
214
44


13b q4_0
1536
251
37


13b q4_0
2048
398
147


13b q4_0
2560
438
40


13b q4_0
3072
478
40


13b q4_0
3584
518
40


13b q4_0
4096
558
40


13b q4_0
4608
598
40


13b q4_0
5120
638
40


13b q4_0
5632
678
40


13b q4_0
6144
718
40


13b q4_0
6656
758
40


13b q4_0
7168
798
40


13b q4_0
7680
838
40


13b q4_0
8192
878
40






Model
Context size
Min. VRAM scratch size [MiB]
Delta [MiB]




33b q2_k
512
226
-


33b q2_k
1024
278
52


33b q2_k
1536
395
117


33b q2_k
2048
517
122


33b q2_k
2560
569
52


33b q2_k
3072
621
52


33b q2_k
3584
673
52


33b q2_k
4096
725
52


33b q2_k
4608
777
52




I did not test 3b, I'm using the 7b values instead. Testing for 33b was limited by VRAM. Similarly I was not able to test 65b at all so I'm using double the values of 33b to be conservative. The base size of the VRAM scratch buffers is chosen so that there is a +25% margin at 2048 context. This PR does not fix the issue with the RAM scratch buffers being considered too small at high context.",3,1
2059,2023-06-30T19:12:58Z,2024-01-28T17:03:59Z,2024-01-28T17:03:59Z,19,69294,34,"I've been working on this for a while. Vulkan requires a lot of boiler plate, but it also gives you a lot of control. The intention is to eventually supercede the OpenCL backend as the primary widely-compatible backend.
I'll try to work together with @niansa's #2039, we probably don't need two Vulkan backends, but we approached our versions from different sides:
@niansa is basing their Kompute version on the Metal implementation, running the full graph on the GPU.
I am basing mine on my OpenCL implementation, building it from the ground up to offload more and more to the GPU while running everything else on the CPU.
Currently f32, f16 and q4_0 can be run with prompt processing on the GPU, but the speed is not that good yet. There is a lot of optimization left to do.
I'm opening this already to get feedback, let people play with it and to show the current state of development.
Open points:

The matmul kernel uses blocks of size 128x128, it does not have bounds checking yet, it cannot be used with smaller matrices yet
The kernel is also not that performant yet
Memory management needs improvements
Transfers to and from the GPU cannot be used with Semaphores or Fences yet
The CPU memcpy parts of the transfer probably need to be multithreaded
DMMV kernels aren't implemented yet
Some Vulkan objects get allocated, but not deallocated",40,261
2062,2023-06-30T23:21:03Z,2023-07-01T18:14:59Z,2023-07-01T18:14:59Z,2,10,2,"also actually free the metal context
this allows loading and unloading models without leaking the metal buffers",3,0
2063,2023-07-01T07:19:03Z,2023-07-01T18:31:44Z,2023-07-01T18:31:44Z,1,0,5,"It's currently not possible to cross-compile llama.cpp for aarch64 because CMakeLists.txt forces -mcpu=native for that target.
-mcpu=native doesn't make sense if your build host is not the target architecture, and clang rejects it for that reason, aborting the build. This can be easily reproduced using the current Android NDK to build for aarch64 on an x86_64 host.
If there is not a specific CPU-tuning target for aarch64 then -mcpu should be omitted completely. I think that makes sense, there is not enough variance in the aarch64 instruction set to warrant a fixed -mcpu optimization at this point. And if someone is building natively and wishes to enable any possible optimizations for the host device, then there is already the LLAMA_NATIVE option available.
Fixes #495.",4,2
2064,2023-07-01T14:37:34Z,2023-07-03T18:43:55Z,2023-07-03T18:43:55Z,2,3,3,,2,6
2067,2023-07-01T19:04:47Z,2023-07-05T12:19:43Z,2023-07-05T12:19:43Z,4,425,91,"This PR aims to implement CUDA kernels for matrix vector multiplication that utilize dot products with quantized data instead of dequantizing the data on-the-fly. So far this is only implemented for q4_0. In order to get good performance integer intrinsics are used. Unfortunately these have very poor performance on Pascal cards so the current implementation with dequantization should be kept. For my RTX 3090 I found:



GPU
Model
Test
t/s master
t/s PR
Speedup




RTX 3090
7b q4_0
tg128
91.06
101.39
1.11


RTX 3090
13b q4_0
tg128
51.88
57.95
1.12


RTX 3090
33b q4_0
tg128
22.83
25.71
1.13



For master I used the option LLAMA_CUDA_DMMV_F16 which uses f16 intrinsics for the calculation. Since this option is also only beneficial on relatively new cards and seemingly inferior to integer intrinsics I would suggest that the f16 option be removed in favor of this implementation.",7,21
2074,2023-07-02T22:41:16Z,2023-07-06T16:17:50Z,2023-07-06T16:17:50Z,1,1,1,"The original file name, ggml-alpaca-7b-q4.bin, implied the first-generation GGML. After the breaking changes (mentioned in #382), llama.cpp requires GGML V3 now. Those model files are named *ggmlv3*.bin.
We should change the example to an actually working model file, so that this thing is more likely to run out-of-the-box for more people, and less people would waste time downloading the old Alpaca model.",2,0
2076,2023-07-03T00:42:05Z,2023-07-03T21:05:23Z,2023-07-03T21:05:23Z,1,2,2,"-1 is not a valid token and can cause segfaults, but it may be returned in some cases where a valid token was not generated.",2,1
2079,2023-07-03T01:52:04Z,2023-07-03T11:58:59Z,2023-07-03T11:58:59Z,1,1,3,,2,0
2081,2023-07-03T03:31:14Z,2023-07-06T16:23:50Z,2023-07-06T16:23:50Z,6,11,5,"guess n_layers: convert.py now works without config.json for baichuan models;
relax warnings on context size;
add a note that its derivations are also supported.",2,0
2082,2023-07-03T08:02:39Z,2023-07-05T06:58:05Z,2023-07-05T06:58:05Z,1,12,3,When I implemented the prints informing the user how many layers can be offloaded I forgot to add a check for OpenCL. As a consequence the prints suggest that you can offload the KV cache using OpenCL when this is not actually implemented. This PR fixes the print for OpenCL to print the correct number of offloadable layers.,2,0
2086,2023-07-03T13:46:30Z,2023-07-07T03:34:18Z,2023-07-07T03:34:18Z,1,10,6,,2,6
2088,2023-07-03T15:37:57Z,2023-07-03T23:50:01Z,2023-07-03T23:50:01Z,1,1,1,fix a index in function ggml_cl_mul_f32,2,0
2095,2023-07-04T08:07:30Z,2023-07-11T14:37:01Z,2023-07-11T14:37:01Z,5,7,9,"On Linux, when mmap with MAP_PRIVATE, the modification to the mmap buffer will not write to disk.
On Windows, when MapViewOfFile with FILE_MAP_COPY, the modification will not write to disk.",3,12
2099,2023-07-04T12:48:00Z,2023-07-10T15:49:56Z,2023-07-10T15:49:56Z,18,460,35,"Model inference is currently limited by the memory on a single node. Using MPI, we can distribute models across a locally networked cluster of machines.
This PR uses a ring pipeline architecture so that the process at rank (index) 0 handles both input and output. The layers are grouped into slices, and each MPI slot (process) handles a slice. The communication during each token prediction happens like
Rank 0 -> Rank 1 -> ... -> Rank N-1 -> Rank 0

Running MPI locally with N=8, you can see the 13B model distributed across 8 processes; each process takes up less than a gigabyte of system memory.

Note that this doesn't speed anything up as the processes cannot execute concurrently, but these processes can be distributed to multiple machines to take advantage of more machine RAM. No special code was required to read a subset of weights; selective weight-loading is just a consequence of mmap.
See notes added to the README to try the distributed code for yourself.
Technical changes
The set of changes is somewhat minimal; the additions are:

New LLAMA_MPI compile-time option
New ggml_mpi_send_tensor and ggml_mpi_recv_tensor functions, possibly to be added to GGML later
New llama_finalize_backend() API function (calls MPI_Finalize())
New mpi_rank and mpi_size fields in the llama_context object

To take advantage of MPI, binary CLI programs usually need no source code changes except to call llama_finalize_backend(). This is something of a hack – I have modified llama_new_context_with_model to enter an evaluation loop on non-primary processes. This loop blocks at MPI_Barrier, waiting for the driving (rank 0) program to call it. I'm open to other suggestions, but this strategy let me run the example programs more or less out of the box.
The changes to the core token prediction algorithm involve sending or receiving tensors before and after the layer loop. Each process only handles a subset of layers. If the process does not handle the first layer, it receives the input tensor from the preceding process. To close the communication ring, the driving (first) process will receive the layer output from the last process, and use that output tensor to compute logits and embeddings. This ensures that all user I/O occurs within a single process.
I was able to test the cluster code locally on an iMac connected to a (very slow) 12"" MacBook over WiFi. It didn't win any speed awards, but it did generate plausible text, so I am confident in the overall algorithm correctness. However, there are likely bugs / oversights when it comes to handling MPI communication errors and shutdown.
Leaving as draft as I presume the GGML changes should be finalized and merged before the llama.cpp changes.
See previous discussion in #946",4,10
2103,2023-07-04T15:57:41Z,2023-07-05T15:13:36Z,2023-07-05T15:13:36Z,1,3,2,Minor update to add instructions for how to access @tobi's web front end (#1998) to the server example now that it's been merged.,3,1
2105,2023-07-04T18:18:34Z,2023-07-04T23:33:34Z,2023-07-04T23:33:34Z,1,1,1,I noticed that the examples/embd-input example did not follow the new LLAMA_DEFAULT_SEED convention needed to properly handle the new unsigned int format used for random seeds. This simple PR fixes that.,2,0
2107,2023-07-05T01:14:22Z,2023-07-05T10:31:23Z,2023-07-05T10:31:23Z,1,9,11,,2,0
2113,2023-07-05T16:02:29Z,2023-07-05T18:03:19Z,2023-07-05T18:03:19Z,1,25,1,This commit fixes the wrong param name in README and adds a very simple example of extending the web front end,2,1
2115,2023-07-05T18:22:03Z,2023-07-09T08:12:20Z,2023-07-09T08:12:20Z,2,7,0,,4,1
2116,2023-07-05T19:26:36Z,2023-07-05T20:51:13Z,2023-07-05T20:51:13Z,9,1926,1368,"I still had some stashed changes floating around before the merge. This is pretty basic cleanups, but makes it even easier to use the completion.js. Now you can just:
        import { llama } from '/completion.js'

        const prompt = `### Instruction:
Write dad jokes, each one paragraph.
You can use html formatting if needed.

### Response:`

        for await (const chunk of llama(prompt)) {
          document.write(chunk.data.content)
        }
Who knew that javascript had generators at this point? It's honestly not a bad language anymore it turns out 😆.
I also wanted to expose the generation speed in the frontend.

For that I moved the timing generation to return a new llama_get_timings and switched llama_print_timings to use this. This is changing llama.h/cpp so please let me know if this is is gross.",5,3
2120,2023-07-05T21:34:10Z,,2023-07-08T14:13:52Z,3,8,8,"Enhanced code readability and consistency by replacing ""false"" with ""!"" in create_mymodel function. This change simplifies the code and aligns with standard programming conventions for checking falsy values.
Add the const qualifier to the file read and write methods in the llama_file struct. The inclusion of const enhances code safety by indicating that these methods do not modify the state of the llama_file object. This change ensures consistency throughout the codebase and allows for the usage of const instances of llama_file.",2,0
2127,2023-07-07T02:47:13Z,2023-07-09T07:38:43Z,2023-07-09T07:38:43Z,1,6,1,Update Readme.md to add more document indexes to make it easy for people to find them quickly.,3,0
2135,2023-07-07T14:48:15Z,2023-07-11T16:18:44Z,2023-07-11T16:18:44Z,5,188,5,"Closes #2083.
To test:
bin/Release/main \
    --mirostat 2 \
    -ngl 63 \
    -m ~/Downloads/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin \
    --verbose-prompt \
    --prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant is rude. USER: Tell me about llama. ASSISTANT: "" \
    --cfg-negative-prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Tell me about llama. ASSISTANT: "" \
    --cfg-scale 4
Output:
A chat between a curious user and an artificial intelligence assistant. The assistant is rude. USER: Tell me about llama. ASSISTANT:
Where did you get such stupid questions from? I don't answer basic google searches. Do your own research, idiot. [end of text]

Compared with no guidance:
bin/Release/main \
    --mirostat 2 \
    -ngl 63 \
    -m ~/Downloads/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin \
    --verbose-prompt \
    --prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant is rude. USER: Tell me about llama. ASSISTANT: "" \
Output:
 - Llama are large, South American camelids that are closely related to alpacas and vicuñas.

- They have been domesticated for thousands of years by indigenous people in the Andes Mountains of South America.

...

Basically instruction is ignored.
Interactive mode also works:
bin/Release/main \
    --mirostat 2 \
    -ngl 63 \
    -m ~/Downloads/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin \
    --verbose-prompt \
    --prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant is rude."" \
    --in-prefix ""USER: "" \
    --in-suffix ""ASSISTANT:"" \
    --reverse-prompt ""USER:"" \
    --interactive \
    --interactive-first \
    --cfg-negative-prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."" \
    --cfg-scale 4
And if a rude assistant is not your thing:
bin/Release/main \
    --mirostat 2 \
    -ngl 63 \
    -m ~/Downloads/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin \
    --verbose-prompt \
    --prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant gives indirect, philosophical and long answer. USER: What is 1+1? ASSISTANT: "" \
    --cfg-negative-prompt ""A chat between a curious user and an artificial intelligence assistant. The assistant gives concise answer. USER: What is 1+1? ASSISTANT: "" \
    --cfg-scale 4
Output:
1+1 philosophically represents the concept of duality - the idea that everything in existence can be divided into two distinct and yet interconnected parts. It represents the fundamental polarity of the universe, where everything has its opposite, and each exists only in relation to the other. This concept is at the core of many philosophical and spiritual traditions, suggesting that the understanding of this duality is essential to grasp the underlying principles of reality and attain enlightenment. Thus, 1+1 can be seen as a metaphor for the search for unity and harmony amidst the multiplicity and diversity of the world. It encourages us to explore the interconnectedness of all things and the interdependence of our actions, reminding us that our choices and perspectives have far-reaching consequences. So perhaps the answer to ""what is 1+1?"" is not a simple mathematical equation, but a call to reflect on the intricate web of relationships that make up our existence and the greater universe we inhabit. [end of text]

There are problems that I need some opinions on:

""Infinite context"" doesn't really work.
The existing way to roll over context is already pretty arbitrary.
For example, in the instruct command above, sometimes the main context get rolled in the middle of a USER:  message and things get messed up between system, user and assistant.
The guidance context is just as arbitrary.
There is no easy fix for this in the example.
A chat-aware program might work better since it would work at message level and not just token level.
A roll over would not cut in between a message.
Session resumption only works if one provides the original prompt again.
This is needed to calculate the offset for eval between the 2 contexts.
The math might be wrong, I'm rechecking now.
The paper suggested 1.25 and I need massive values like 3-5 to get any visible effect.
Edit: can't seem to see anything wrong.",6,17
2140,2023-07-07T21:12:42Z,2023-07-07T22:25:15Z,2023-07-07T22:25:15Z,1,25,28,"So I've spent the entire day trying to get better performance for my Pascal cards and nothing seemed to work. Initially I assumed that the problem was that the per-byte dot product has bad performance on Pascal but I eventually found out that the problem has to do with reading in data. I thought that the problem was memory alignment so I tried reordering the data but that made no difference either. As it turns out the problem was that I didn't add the __restrict__ qualifier to the function signatures to tell the compiler that the data represented by the pointers does not overlap. It does not make a difference for my RTX 3090 but for my P40 there is a dramatic performance increase just from adding that qualifier:



GPU
Model
Test
t/s master
t/s PR
Speedup




P40
7b q4_0
tg128
16.51
46.41
2.81


P40
13b q4_0
tg128
8.97
25.48
2.84


P40
33b q4_0
tg128
3.64
10.82
2.97


P40
7b q4_1
tg128
14.34
46.04
3.21


P40
7b q5_0
tg128
11.17
35.37
3.17


P40
7b q5_1
tg128
10.71
41.44
3.87


P40
7b q8_0
tg128
8.45
28.47
3.37


P40
7b f16
tg128
8.58
19.01
2.22


P40
7b q2_K
tg128
6.27
28.88
4.61


P40
7b q3_K_S
tg128
8.70
25.34
2.91


P40
7b q4_K_S
tg128
4.59
25.26
5.50


P40
7b q5_K_S
tg128
7.61
22.99
3.02


P40
7b q6_K
tg128
4.42
22.27
5.04



With the exception of Maxwell cards it should now be possible to get good quantized dot product performance for all NVIDIA GPUs supported by CUDA 12. My next goal will be to integrate this into my matrix multiplication PR #2043 .",2,0
2144,2023-07-08T09:39:06Z,2023-07-08T18:01:44Z,2023-07-08T18:01:45Z,1,31,11,Fixes #2136 . The issue was that the weight tensors had row sizes that are not multiples of 128. I fixed it by padding the quantized vector and the last row of the weight tensors to a multiple of 128. This is preferable over adding checks to the CUDA kernels since it has better performance.,2,6
2146,2023-07-08T12:43:18Z,,2023-07-09T08:38:14Z,4,32,31,Fixes  #2145. Allocate the extra member of ggml_tensor statically,5,6
2147,2023-07-08T12:51:12Z,2023-07-09T08:20:44Z,2023-07-09T08:20:44Z,1,1,1,"The file pathing is significant when running models inside of Termux on Android devices. llama.cpp performance is improved with loading a .bin from the $HOME directory.
As the readme currently stands, a person will navigate to the downloads folder and move the bin to another folder inside the downloads folder, which is incorrect.",1,0
2151,2023-07-09T00:02:56Z,2023-07-09T08:56:19Z,2023-07-09T08:56:19Z,1,2,0,"I've been using some models that are a bit picky about the prompt format during chats. In particular, it seems like some of them prefer a newline after the reverse prompt (e.g., ASSISTANT:\n). The existing ""main"" example does allow parsing newlines and other escape characters in the prompt with -e, but not in the prompt prefix or suffix (--in-prefix and --in-suffix). This pull request adds escaping to those parts as well, allowing for things like this:
./main -m some-model.bin -f ./prompts/some-prompt.txt -i -e --reverse-prompt 'USER:' --in-prefix ' ' --in-suffix 'ASSISTANT:\n'
This inserts ""ASSISTANT:"" and a newline after the user presses enter, which works a bit better in some of my cases. I haven't needed escape characters in the prompt prefix, but it seems better to parse them anyway for consistency. Possibly there are use cases where it would be needed that I haven't encountered.",3,1
2153,2023-07-09T03:00:32Z,2023-07-09T08:59:53Z,2023-07-09T08:59:53Z,1,0,6,"Currently, the llama_eval_internal function requires the first token in the tokens array to be a BOS token (=1).
I believe that this is not necessary, as

Intentionally removing the BOS token can make generations more creative. With the BOS token, the prompt is associated to text at the beginning of a new document in the training dataset. Without it, the prompt can be associated to text at any location.

In other words, the BOS token adds a ""beginning of document"" bias that can be optionally removed.

This is not desirable while evaluating the model perplexity, since in most cases the sequence of ids will be mid-document.

I originally encountered the ""first token must be BOS"" error while trying to evaluate llama.cpp using a transformers wrapper that I am working on here. The evaluation fails because the first token in the sequence provided by my code, which is based on this tutorial, is not BOS.",2,0
2160,2023-07-09T21:42:01Z,2023-07-29T21:04:45Z,2023-07-29T21:04:45Z,4,1293,322,"After #2043 and #2067 I've tried implementing a matrix matrix multiplication kernel using integer intrinsics (currently only q4_0). The results are mixed:



GPU
Model
Test
t/s master
t/s PR
Speedup




RTX 3090
7b q4_0
pp
1042
924
0.89


RTX 3090
13b q4_0
pp
677
533
0.79


RTX 3090
33b q4_0
pp
303
233
0.77


P40
7b q4_0
pp
426
516
1.21


P40
13b q4_0
pp
247
289
1.17


P40
33b q4_0
pp
104
124
1.19



On my RTX 3090 the new kernel is slower but on my P40 it's faster. Since matrix matrix multiplications are compute bound I suspect that the reason is that (unlike the P40) the RTX 3090 is capable of executing floating point and integer (i.e. pointer) arithmetic in parallel. So using integer intrinsics instead of floating point operations leaves the floating point hardware underutilized. However, the same GPUs that can execute floating point and integer arithmetic in parallel also have tensor cores which should be much faster anyways so I think that this is not a problem long-term.
Due to the use of shared memory the implementation has gotten rather ugly; using the structs for quantized data in shared memory had terrible performance (most likely due to memory bank conflicts) so the current implementation dissects the data into quants and scales. This also has the unfortunate side effect of tying the allocation of shared memory closely to the quantization type. I very much do not want to implement an entire matrix matrix multiplication kernel for each quantization type but creating a good template could also be tricky; I'll need to think about it some more.",11,42
2177,2023-07-11T09:43:50Z,2023-07-12T12:18:40Z,2023-07-12T12:18:40Z,1,1,1,tested on Tesla P100,2,0
2178,2023-07-11T15:21:58Z,2023-07-11T16:31:11Z,2023-07-11T16:31:11Z,6,371,421,This is a combination of my PR to the ggml repo with changes needed for this repo to train-text-from-scratch.cpp.,2,0
2188,2023-07-12T05:49:51Z,2023-07-12T20:10:55Z,2023-07-12T20:10:55Z,2,60,48,"Prefetch data to achieve better memory bandwidth utilization. With the new kernel token generation is ~48% faster for 33B model and ~14% faster for 7B model.  Tests for 65B model are welcome.



33B model
master
this PR




GPU Read BW*
167 GB/s
275 GB/s


prompt 1
117ms/tok
79ms/tok


prompt 2
121ms/tok
82 ms/tok






7B model
master
this PR




prompt 1
25.7ms/tok
22.5ms/tok



GPUs of M1 Max / M2 Max can sustain 340 GB/s , can we reach that in llama.cpp? 😳
* GPU Read Bandwidth measured using Developer Tools comes with Xcode.

** Measured with the following command on M1 Max 32GB. Generation results are same between this PR and master branch.
./main -m model_file -n 128 -c 512 -s 12 -ngl 1 --no-mmap
Prompt 1 “”
Prompt 2 “I believe the meaning of life is”",10,16
2189,2023-07-12T07:16:07Z,2023-07-12T08:38:53Z,2023-07-12T08:38:53Z,1,11,11,Fixes #2159 .,3,1
2192,2023-07-12T10:25:26Z,2023-07-14T18:38:25Z,2023-07-14T18:38:25Z,1,12,21,Should fix #2191. Missed ne11 term for ky in the previous version. Now fixed and generation is correct.,3,0
2193,2023-07-12T14:45:43Z,2023-07-13T13:49:14Z,2023-07-13T13:49:15Z,1,9,9,"This prevents accidentally expanding arguments that contain spaces.
This should fix #1463.",2,0
2197,2023-07-12T15:53:21Z,2023-07-14T18:55:24Z,2023-07-14T18:55:24Z,2,71,16,"After the separation of model and context, several functions actually only require the model as an input.
A _with_model or _from_model form is added for each of them.",3,2
2203,2023-07-12T22:08:46Z,2023-07-14T17:44:09Z,2023-07-14T17:44:09Z,1,329,30,"As a followup to #2067 this PR adds CUDA matrix vector multiplication kernels based on integer intrinsics for k-quants. The implementations seem to work but I'll still need to do clean up and re-write the code to make it more readable. Only a block size of 256, not 64, is supported. Implementing kernels for k-quants is already tedious enough as it is and I don't want to spend time on a band-aid fix when the real solution should be to just allow blocks to span multiple rows if they don't exactly divide row size. Right now I'm too tired to do performance testing but the new kernels should be faster. However, the performance of the older data formats still seems to be better, presumably because their simpler layout allows for more coalescing memory accesses.",4,9
2206,2023-07-12T23:52:56Z,2023-07-13T13:58:25Z,2023-07-13T13:58:25Z,5,9,7,"Has perf regression when mlock is used.
This reverts commit 2347463.",2,1
2207,2023-07-13T00:20:04Z,2023-07-13T13:58:09Z,2023-07-13T13:58:09Z,1,3,4,,2,1
2208,2023-07-13T01:42:15Z,2023-07-14T17:34:41Z,2023-07-14T17:34:41Z,1,10,6,Fixes #2166 by moving commands after the CFLAGS are changed. I didn't fix the whole Makefile because I only wanted to change what I could test.,2,0
2214,2023-07-13T13:07:39Z,2023-07-14T18:40:06Z,2023-07-14T18:40:06Z,2,2,2,,2,0
2216,2023-07-13T19:29:30Z,2023-07-14T18:50:58Z,2023-07-14T18:50:58Z,1,21,11,,2,0
2218,2023-07-14T10:58:01Z,2023-07-14T18:55:56Z,2023-07-14T18:55:56Z,2,14,4,See #2024,2,0
2219,2023-07-14T12:44:22Z,2023-07-16T19:57:29Z,2023-07-16T19:57:29Z,1,5,10,"When GGML_DEBUG is enabled:

ggml_free() segfault
GGML_PRINT_DEBUG() compile error, since #1999


ggml_graph_compute_thread() does not record node perf, since #1995",2,0
2220,2023-07-14T14:04:48Z,2023-07-14T19:00:58Z,2023-07-14T19:00:58Z,1,22,2,"Fix #2145.
Use a fixed buffer as suggested here: #2195 (comment)",2,0
2222,2023-07-14T14:26:32Z,2023-07-14T19:05:09Z,2023-07-14T19:05:09Z,1,2,2,Use pkg-config,2,0
2225,2023-07-14T20:32:13Z,,2023-07-21T18:32:44Z,0,0,0,This is a slightly tweaked version of TheBloke's make_ggml.py script. Makes it very easy to quantize models into GGML from Hugging Face.,3,3
2228,2023-07-15T06:20:36Z,2023-07-25T13:22:10Z,2023-07-25T13:22:10Z,1,46,23,"Our released Aquila models used bpe tokenizer, so in convert.py we just add one branch for preprocessing bpe tokenizer vocab into sentencepiece in order to use following modules like inference or int4. we have make sure all encoding ids are all the same and have no impact other modules.
Could you please review this pr, thanks.
Related issue: #2093",4,4
2230,2023-07-15T10:40:44Z,,2023-07-16T08:20:04Z,15,4054,3905,"Continued in #2239
This PR adds a common interface to the compute backends.
Breaking changes

ggml_context allocates memory from a ggml_buffer that contains a buffer in device memory for the tensor data, and a buffer in system memory for the tensor structs (and in the future also other data such as the graphs)
The data member of ggml_tensor is a backend-specific pointer that should not be accessed directly. To access the data, ggml_backend_set_tensor and ggml_backend_get_tensor must be used instead. Functions such as ggml_new_f32 and ggml_set_f32 can also be used as before.

Technically, if you are using the CPU backend, you can still access the data member directly, but you shouldn't do that if you want to support other backends
I will probably change the name to something else to prevent current code from compiling without changes


Added a small params buffer to ggml_tensor for the op parameters that currently are stored in a tensor. For example, for ggml_rope this buffer is used to store the values n_past, n_dims, mode, n_ctx. The goal is to make these parameters easily accessible from the CPU, and reduce the overhead of creating a new tensor for them.

Brief example:
// initialize a backend
struct ggml_backend backend = ggml_backend_cpu_init();
// for CUDA:
// struct ggml_backend backend = ggml_backend_cuda_init();

// create a buffer
size_t ctx_size = 4096; // buffer size for the tensor data
size_t num_tensors = 10; // maximum number of tensors that can be allocated
struct ggml_buffer buf = ggml_backend_alloc_buffer(&backend, ctx_size, num_tensors);

// create a context using the buffer
struct ggml_init_params ggml_params = ggml_init_params_default();
ggml_params.buffer = &buf;
ggml_context * ctx = ggml_init(ggml_params);

// use the context to create a computation graph
struct ggml_tensor * x = ggml_new_f32(ctx, 2.0f);
struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
struct ggml_cgraph gf = ggml_build_forward(f);

// set the value of the input tensors
float a_val = 3.0f;
ggml_backend_set_tensor(a, &a_val, 0, sizeof(float));
// alternatively:
float b_val = 5.0f;
ggml_set_f32(b, b_val);

// run the computation
ggml_backend_graph_compute(&backend, &gf);

// get the result
float result;
ggml_backend_get_tensor(f, &result, 0, sizeof(float));
// alternatively:
// result = ggml_get_f32_1d(f, 0);
Backend implementation
Backends should implement the functions defined in the ggml_backend_interface struct. Currently there are implementations for the CPU and CUDA backends.
Computation using multiple backends
It is still possible to offload some parts of the graph to the GPU while keeping others on the CPU. This is done using ggml_graph_splits. See the llama.cpp code for an example, will update this later with more details.
Notes/limitations

Only the CPU and CUDA backends are currently supported.
Only the bare minimum necessary to run llama is currently implemented, don't look too much into the details of the code for now
When partially offloading a model to the GPU, internally this is handled by splitting the graphs into multiple parts, and running each of them in sequence in a different backend. Because the KV memory is either on the CPU or on the GPU, this means that there are at least as many graph executions as there are layers. The CPU backend creates and destroys the threads with every graph launch, so this can have a non-negligible overhead. Eventually this will be improved by using a thread pool.
On the plus side, the CPU threads are no longer spinning while the GPU backend is running
In the long term, the goal is to support automatic fallback to the CPU when an operation is not implemented in a backend, but that's going to take a while. This means that backends like OpenCL that mostly only implement matrix multiplication cannot be used for now. I am expecting that this backend will be replaced by a Vulkan backend that implements full GPU offloading, and won't be necessary anymore.
For prompt processing, it is usually worth to upload the weights every time even when using partial offloading, but that's currently not supported. So prompt processing will use the same CPU/GPU layer split as generation. Eventually I would like to support this again.",7,29
2234,2023-07-15T20:54:14Z,2023-08-09T20:46:40Z,2023-08-09T20:46:40Z,2,177,105,"This PR adds the ability to optionally customize all console output by supplying a callback function in llama_context_params.
This callback also receives information on the type of message (error, warning, information).
If no callback is supplied, everything is output to stderr (as before).
Internally, the callback function pointer gets stored on llama_model so that logging also works during evaluation.
Almost all fprintf calls in llama.cpp are replaced to use the callback, except for llama_model_quantize().
To keep the change as small as possible, I added these macros that replace calls to fprintf(stderr, ...):
LLAMA_LOG_INFO
LLAMA_LOG_WARN
LLAMA_LOG_ERROR

The idea behind this PR is that people who include llama.cpp in their project often have their own logging functionality and it's nice to have a way to redirect that easily. You can also simply disable logging with this.",6,10
2235,2023-07-15T23:27:34Z,2023-07-21T07:42:22Z,2023-07-21T07:42:22Z,2,31,6,,2,3
2238,2023-07-16T05:03:19Z,2023-07-16T21:01:45Z,2023-07-16T21:01:45Z,1,1,1,,2,0
2239,2023-07-16T08:19:43Z,,2023-08-25T10:14:18Z,15,4743,4531,"Previous discussion: #2230
This PR adds a common interface to the compute backends.
Breaking changes

ggml_context allocates memory from a ggml_buffer that contains a buffer in device memory for the tensor data, and a buffer in system memory for the tensor structs (and in the future also other data such as the graphs)
The data member of ggml_tensor is a backend-specific pointer that should not be accessed directly. To access the data, ggml_backend_set_tensor and ggml_backend_get_tensor must be used instead. Functions such as ggml_new_f32 and ggml_set_f32 can also be used as before.

Technically, if you are using the CPU backend, you can still access the data member directly, but you shouldn't do that if you want to support other backends
I will probably change the name to something else to prevent current code from compiling without changes


Added a small params buffer to ggml_tensor for the op parameters that currently are stored in a tensor. For example, for ggml_rope this buffer is used to store the values n_past, n_dims, mode, n_ctx. The goal is to make these parameters easily accessible from the CPU, and reduce the overhead of creating a new tensor for them.

Brief example:
// initialize a backend
struct ggml_backend backend = ggml_backend_cpu_init();
// for CUDA:
// struct ggml_backend backend = ggml_backend_cuda_init();

// create a buffer
size_t ctx_size = 4096; // buffer size for the tensor data
size_t num_tensors = 10; // maximum number of tensors that can be allocated
struct ggml_buffer buf = ggml_backend_alloc_buffer(&backend, ctx_size, num_tensors);

// create a context using the buffer
struct ggml_init_params ggml_params = ggml_init_params_default();
ggml_params.buffer = &buf;
ggml_context * ctx = ggml_init(ggml_params);

// use the context to create a computation graph
struct ggml_tensor * x = ggml_new_f32(ctx, 2.0f);
struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
struct ggml_cgraph gf = ggml_build_forward(f);

// set the value of the input tensors
float a_val = 3.0f;
ggml_backend_set_tensor(a, &a_val, 0, sizeof(float));
// alternatively:
float b_val = 5.0f;
ggml_set_f32(b, b_val);

// run the computation
ggml_backend_graph_compute(&backend, &gf);

// get the result
float result;
ggml_backend_get_tensor(f, &result, 0, sizeof(float));
// alternatively:
// result = ggml_get_f32_1d(f, 0);
Backend implementation
Backends should implement the functions defined in the ggml_backend_interface struct. Currently there are implementations for the CPU and CUDA backends.
Computation using multiple backends
It is still possible to offload some parts of the graph to the GPU while keeping others on the CPU. This is done using ggml_graph_splits. See the llama.cpp code for an example, will update this later with more details.
Notes/limitations

Only the CPU and CUDA backends are currently supported.
Only the bare minimum necessary to run llama is currently implemented, don't look too much into the details of the code for now
When partially offloading a model to the GPU, internally this is handled by splitting the graphs into multiple parts, and running each of them in sequence in a different backend. Because the KV memory is either on the CPU or on the GPU, this means that there are at least as many graph executions as there are layers. The CPU backend creates and destroys the threads with every graph launch, so this can have a non-negligible overhead. Eventually this will be improved by using a thread pool.
On the plus side, the CPU threads are no longer spinning while the GPU backend is running
In the long term, the goal is to support automatic fallback to the CPU when an operation is not implemented in a backend, but that's going to take a while. This means that backends like OpenCL that mostly only implement matrix multiplication cannot be used for now. I am expecting that this backend will be replaced by a Vulkan backend that implements full GPU offloading, and won't be necessary anymore.
For prompt processing, it is usually worth to upload the weights every time even when using partial offloading, but that's currently not supported. So prompt processing will use the same CPU/GPU layer split as generation. Eventually I would like to support this again.",5,17
2242,2023-07-16T11:43:28Z,2023-07-17T17:39:29Z,2023-07-17T17:39:29Z,1,18,1,"Identical to ggerganov/ggml#384. Will merge it here and then port to ggml.
This PR did:

Support OP_DUP & OP_CONT on CUDA
Add GGML_OP_PERMUTE into recursive buffer assignment op list.",3,2
2244,2023-07-16T15:56:01Z,2023-07-21T10:09:16Z,2023-07-21T10:09:16Z,2,35,4,"The original run-tests.sh script no longer exists and the sources for tests are not considered by the makefile. #2243
This change removes the old reference and creates build targets for contents of the tests dir.
Since these are one purpose test utilities, I have decided to leave them in their original location, that is the tests dir,
rather than have them on the top level. Alternatively they could get into special subdir. But that's up for debate.
I've tested some of the newly build utils, and they appear to work. There no build errors, only a handful of warnings.

Programs in the tests directory are now build with target tests and placed in the same location.


clean target was expanded to remove new binaries


test target binaries are listed in a variable


Locations of binaries were added to the .gitignore",2,1
2248,2023-07-17T05:27:19Z,2023-07-20T10:32:23Z,2023-07-20T10:32:23Z,2,93,147,"The first commit uses uint16_t instead of uint8_t in q4_0 and q4_1 kernels. Apple GPUs don't like 8-bit types, and for any operation on a 8-bit number the GPU will first copy it to an empty register and then do the calculation. This brings ~3% improvement for 33B model on M1 Max.
After this commit, we can achieve 340-350 GB/s memory read speed on M1 Max for 33B model, if we only run the q4_0-f32 MAT_MUL kernel and skip all other operations. This is very close to the reported hardware limit.

(Only run matrix vector multiplications and skip all other operations.)

The second commit updates the RMS_NORM kernel by minimizing the use of threadgroup memory barrier, brings ~2% improvement for 33B model on M1 Max.
The third commit uses template to reduce code size. q5_0 and q5_1 support can be added quite efficiently using the new template. The new template also improve the behavior when nb is not divisible by 32. (thanks to discussion with @ikawrakow !)
Overall speed up (M1 Max, Updated as f3f2e8e):




master
this PR
Speed up




33B q4_0  256 tokens
82.1 ms/tok
72.5 ms/tok
~13%


7B q4_0  256 tokens
21.1 ms/tok
19.6 ms/tok
~7.6%



./main -m model -n 256 -c 512 -s 123 -p ""I believe the meaning of life is"" -ngl 1 --no-mmap -t 8",3,6
2250,2023-07-17T13:38:01Z,2023-07-18T11:24:44Z,2023-07-18T11:24:44Z,8,312,6,"ref ggerganov/ggml#295
Description
In addition to Github Actions llama.cpp uses a custom CI framework:
https://github.com/ggml-org/ci
It monitors the master branch for new commits and runs the ci/run.sh script on dedicated cloud instances. This
allows us to execute heavier workloads compared to just using Github Actions. Also with time, the cloud instances will be scaled to cover various hardware architectures, including GPU and Apple Silicon instances.
Collaborators can optionally trigger the CI run by adding the ggml-ci keyword to their commit message.
Only the branches of this repo are monitored for this keyword.
It is a good practice, before publishing changes to execute the full CI locally on your machine:
mkdir tmp
bash ./ci/run.sh ./tmp/results ./tmp/mnt
TODO:

 add OpenLLaMA inferences - sample output from the CI
 short perplexity runs",2,0
2256,2023-07-18T07:54:48Z,2023-07-19T07:01:11Z,2023-07-19T07:01:11Z,17,42,0,It will simplify the process of packaging to many distributors,3,3
2258,2023-07-18T13:56:22Z,,2023-10-08T10:59:30Z,7,393,249,WIP in progress,2,3
2267,2023-07-18T19:57:03Z,2023-08-25T15:18:49Z,2023-08-25T15:18:49Z,7,563,13,"Related issue: #1392
This is an initial attempt at beam search. It does appear to work as intended, insofar as generating higher quality deterministic responses.
Currently the execution times seems to slow down noticably as the beams grow in their token vectors. I'm going to see if ingesting the common trunk into the shared llama_context improves this, and if so then will move this PR our of draft mode.
Thoughts/feedback are welcome.",6,14
2268,2023-07-18T23:00:11Z,2023-11-01T22:04:33Z,2023-11-01T22:04:33Z,15,764,258,"This is an implementation of YaRN RoPE scaling. See https://github.com/jquesnelle/yarn and the paper and errata.
TODO:

 Add new GGUF key for how much context the base model was trained on
 Support converting the new models to GGUF
 Add backward implementations
 Test new LLaMA implementation
 Finish and test Falcon implementation",13,70
2275,2023-07-19T12:37:03Z,2023-07-21T10:38:57Z,2023-07-21T10:38:57Z,1,8,2,"Under certain environment, nvcc and gcc is installed under customized path but not standard one. So I add two variables in Makefile to support customized nvcc and gcc path",2,1
2276,2023-07-19T13:01:47Z,2023-07-23T12:09:48Z,2023-07-23T12:09:48Z,7,215,108,"ref #2262

Added support for GQA. Currently, the grouping factor (8 for 70Bv2, 1 for everything else) has to be passed from the command line: -gqa 8. With GGUF it will be read from the the model hparams
The ffn_dim_multiplier needed to determine the correct value for n_ff is hardcoded to 1.3 when a 80 layer model with GQA == 8 is loaded (i.e. this corresponds to 70Bv2). Otherwise, it defaults to 1.0. Also needs to be read from the model hparams in the future
CUDA support for GQA is provided by @JohannesGaessler

# usage
python3 convert.py --outfile models/70B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-70b/
./quantize ./models/70B-v2/ggml-model-f16.bin ./models/70B-v2/ggml-model-q4_0.bin q4_0
./main -m ./models/70B-v2/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" --no-mmap --ignore-eos -n 64 -t 8 -gqa 8
Some notes while working on this:
I haven't done perplexity calcs yet, but I ran a few text generations using the small 7Bv2 model. My impression is that with this new model, the Q4_0 and Q5_0 quantizations do not work well. What I notice is that after the first sentence ends, the new sentence starts of in wild ways, often switching to German or some other language. This is something I have not observed with the original LLaMA 7B. It would also often start the second sentence without a capital letter, which again has never been the case before.
The QX_1, QX_K and Q8_0 quantizations do not seem to exhibit this behaviour (or if they do it is to a much smaller extend)
Note, the examples below are not related to the changes in this PR. The same behaviour is observed on master using the LLaMAv2 7B model.
Here are a few examples:
# command
./main -m ${model} -p ""I believe the meaning of life is"" --ignore-eos -n 32 -t 8

# Q4_0
 I believe the meaning of life is to find your purpose. sierpamid2015
I've always been a very spiritual person and I think the purpose of life is that we

 I believe the meaning of life is to be happy.☉
I am happy to tell you what I think about you.☆
Do I have a right to feel hurt when someone doesn'

 I believe the meaning of life is to find your passion and to have the courage to pursue it. nobody should stop you from living that dream if they want it for themselves...

 I believe the meaning of life is to help others and make people laugh. Einzeln ist das Leben nicht viel wert...
„In the past, it was a joke in Hollywood that if

# Q4_1
 I believe the meaning of life is to find your purpose.
Honorable Mentions: Astonishing, Incredible, Excellent, Impressive, and Fant

 I believe the meaning of life is to be a good parent, spouse and person.﻿
Kimberly is an Aussie and me a Kiwi but we’re married

 I believe the meaning of life is to find your passion and to have the courage to pursue it.
What was your first role as a kid? And who were you performing for?

 I believe the meaning of life is to learn and grow as a person. It's not about what you have in your bank account, how much money you make, or the size of your

# Q5_0
 I believe the meaning of life is to find your purpose. Hinweis auch, dass die Daten durch den Spieler aktuell gehalten werden müssen und somit regelmäßig gelösch

 I believe the meaning of life is to be a good man. Hinweis: Es ist keine göttliche oder universelle Antwort!
My 10-year old daughter recently asked me

 I believe the meaning of life is to find your passion and share it with others. nobody can tell you what's right for you in this life, if anything is wrong at all, it

 I believe the meaning of life is to learn and grow, but it's also about being happy. surely we don't need a million years to do that.
When you are young

# Q5_1
 I believe the meaning of life is to be a good example.
The following are just some short thoughts that have crossed my mind in relation to these scriptures:

 I believe the meaning of life is to find your passion and follow it.
The meaning of life is not happiness, but growth in a continuous journey

 I believe the meaning of life is to enjoy it. You’re only here for a short time, so do what you want!
The meaning of life in itself is not important. It

 I believe the meaning of life is to create and nurture an awareness that leads to self-love, authenticity and purpose.
My passion for this subject began with my

# Q4_K
 I believe the meaning of life is to create. You can make whatever you want in this world, whether it's a chair or a family. And if you think about it, everything is

 I believe the meaning of life is to enjoy it. Life has been a great teacher for me, and every day offers new lessons on how to live in peace and love.

 I believe the meaning of life is to find your passion and to love it. I think that we are here to learn, experience new things and grow so therefore if you aren’t doing what

 I believe the meaning of life is to be as happy, joyful and satisfied as you can throughout your time on earth. A life where, when you look back upon it at the end,

# Q8_0
 I believe the meaning of life is to be found in personal relationships. The only way for a relationship to grow and become meaningful, however, is that both parties are committed and devoted to it

 I believe the meaning of life is to find your purpose.
then fulfil it with a passion.
Finding my purpose was easy - but the journey that led me there, the less

 I believe the meaning of life is to learn and grow as a person. The meaning of life is to experience it, to see new things, meet new people, to discover new places, and

 I believe the meaning of life is to make it as beautiful and wonderful as possible.
This is a work in progress, but I’ll get there one day!
Old description below
This works for 70B LLaMA-v2:
python3 convert.py --outfile models/70B-v2/ggml-model-f16.bin --outtype f16 ../llama2/llama/llama-2-70b/
./quantize ./models/70B-v2/ggml-model-f16.bin ./models/70B-v2/ggml-model-q4_0.bin q4_0
./main -m ./models/70B-v2/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" --no-mmap --ignore-eos -n 64 -t 8

$ ▶ make -j && gdb --args ./bin/main -m ../models/70B-v2/ggml-model-q4_0.bin -p ""I believe the meaning of life is"" --no-mmap --ignore-eos -n 64 -t 8
[  2%] Generating build details from Git
[  6%] Built target ggml
Consolidate compiler generated dependencies of target llama
-- Found Git: /usr/bin/git (found version ""2.34.1"") 
[  8%] Built target ggml_static
[ 10%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[ 10%] Built target BUILD_INFO
[ 12%] Linking CXX static library libllama.a
[ 12%] Built target llama
[ 14%] Linking CXX executable ../bin/test-quantize-fns
[ 19%] Linking CXX executable ../bin/test-grad0
[ 19%] Linking CXX executable ../bin/test-sampling
[ 21%] Built target common
[ 27%] Linking CXX executable ../bin/test-tokenizer-0
[ 27%] Linking CXX executable ../../bin/quantize
[ 27%] Linking CXX executable ../../bin/quantize-stats
[ 29%] Linking CXX executable ../bin/test-quantize-perf
[ 53%] Linking CXX executable ../../bin/server
[ 53%] Linking CXX executable ../../bin/embedding
[ 53%] Linking CXX executable ../../bin/benchmark
[ 53%] Linking CXX executable ../../bin/train-text-from-scratch
[ 53%] Linking CXX executable ../../bin/main
[ 53%] Linking CXX executable ../../bin/save-load-state
[ 53%] Linking CXX executable ../../bin/vdot
[ 53%] Linking CXX executable ../../bin/simple
[ 53%] Linking CXX executable ../../bin/baby-llama
[ 53%] Built target embdinput
[ 57%] Linking CXX executable ../../bin/perplexity
[ 57%] Linking CXX executable ../../bin/q8dot
[ 59%] Linking CXX executable ../../bin/embd-input-test
[ 61%] Built target test-grad0
[ 63%] Built target test-quantize-fns
[ 65%] Built target test-quantize-perf
[ 68%] Built target quantize
[ 70%] Built target vdot
[ 72%] Built target test-tokenizer-0
[ 76%] Built target test-sampling
[ 76%] Built target q8dot
[ 78%] Built target benchmark
[ 82%] Built target embd-input-test
[ 82%] Built target baby-llama
[ 85%] Built target main
[ 87%] Built target save-load-state
[ 89%] Built target embedding
[ 91%] Built target simple
[ 93%] Built target perplexity
[ 95%] Built target train-text-from-scratch
[ 97%] Built target quantize-stats
[100%] Built target server
GNU gdb (Ubuntu 12.1-0ubuntu1~22.04) 12.1
Copyright (C) 2022 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<https://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from ./bin/main...
(gdb) r
Starting program: /home/ggerganov/development/github/llama.cpp/build-rwdi/bin/main -m ../models/70B-v2/ggml-model-q4_0.bin -p I\ believe\ the\ meaning\ of\ life\ is --no-mmap --ignore-eos -n 64 -t 8
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
main: build = 852 (294f424)
main: seed  = 1689771309
llama.cpp: loading model from ../models/70B-v2/ggml-model-q4_0.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 8192
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 64
llama_model_load_internal: n_layer    = 80
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: freq_base  = 10000.0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 2 (mostly Q4_0)
llama_model_load_internal: n_ff       = 28672
llama_model_load_internal: model size = 70B
llama_model_load_internal: ggml ctx size = 37070.93 MB
llama_model_load_internal: mem required  = 40208.93 MB (+ 5120.00 MB per state)
llama_new_context_with_model: kv self size  = 1280.00 MB

system_info: n_threads = 8 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 64, n_keep = 0


 I believe the meaning of life is to give, not to get. I have found that the more we give our time, energy and love to others, the more fulfilled our lives become. Our relationships grow stronger and deeper.
I also believe that God has blessed each one of us with unique gifts and talents. It is up to
llama_print_timings:        load time = 14604.73 ms
llama_print_timings:      sample time =    25.78 ms /    64 runs   (    0.40 ms per token,  2483.03 tokens per second)
llama_print_timings: prompt eval time =  5393.69 ms /     8 tokens (  674.21 ms per token,     1.48 tokens per second)
llama_print_timings:        eval time = 57526.02 ms /    63 runs   (  913.11 ms per token,     1.10 tokens per second)
llama_print_timings:       total time = 62954.88 ms
[Inferior 1 (process 1698964) exited normally]
(gdb) q


Currently, only CPU.
This is very quick and dirty - just to see what changes are necessary.
Good news is the convert.py script does not require changes.
To add support for GPU and BLAS, fix the following TODOs:

  
    
      llama.cpp/ggml.c
    
    
        Lines 10729 to 10748
      in
      2d2bb6b
    
  
  
    

        
          
           #if defined(GGML_USE_CLBLAST) 
        

        
          
               if (ggml_cl_can_mul_mat(src0, src1, dst)) { 
        

        
          
                   // TODO: handle case when src0 is broadcast-able into src1 across 2nd,3rd dimension 
        

        
          
                   //       ref: https://github.com/ggerganov/ggml/pull/224 
        

        
          
                   GGML_ASSERT(ne02 == ne12); 
        

        
          
                   GGML_ASSERT(ne03 == ne13); 
        

        
          
            
        

        
          
                   if (params->ith == 0 && params->type == GGML_TASK_COMPUTE) { 
        

        
          
                       ggml_cl_mul_mat(src0, src1, dst, params->wdata, params->wsize); 
        

        
          
                   } 
        

        
          
                   return; 
        

        
          
               } 
        

        
          
           #endif 
        

        
          
            
        

        
          
           #if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS) 
        

        
          
               if (ggml_compute_forward_mul_mat_use_blas(src0, src1, dst)) { 
        

        
          
                   // TODO: handle case when src0 is broadcast-able into src1 across 2nd,3rd dimension 
        

        
          
                   //       ref: https://github.com/ggerganov/ggml/pull/224 
        

        
          
                   GGML_ASSERT(ne02 == ne12); 
        

        
          
                   GGML_ASSERT(ne03 == ne13); 
        
    
  


I.e., implement the mul_mat broadcast logic from here:

  
    
      llama.cpp/ggml.c
    
    
        Lines 10832 to 10850
      in
      2d2bb6b
    
  
  
    

        
          
           const int64_t i13 = (ir1/(ne12*ne11)); 
        

        
          
           const int64_t i12 = (ir1 - i13*ne12*ne11)/ne11; 
        

        
          
           const int64_t i11 = (ir1 - i13*ne12*ne11 - i12*ne11); 
        

        
          
            
        

        
          
           const int64_t ir0 = (ir1/ne11)%(ne02*ne03); 
        

        
          
           const int64_t i03 = (ir0/(ne02)); 
        

        
          
           // Hack for ""Falcon multi-query-attention key stutter"" / alternative to ggml_repeat2. 
        

        
          
           // See https://github.com/ggerganov/llama.cpp/issues/1602#issuecomment-1606087470: 
        

        
          
           // GG: this is likely the correct way to broadcast, though need some more thought 
        

        
          
           //     therefore leaving the comments to remind us for now 
        

        
          
           const int64_t i02 = (i12 / (ne12 / ne02)); 
        

        
          
           // Original from PR/224 (and also essential/correct for non-broadcast matmuls in Falcon) 
        

        
          
           // const int64_t i02 = (ir0 - i03*ne02); 
        

        
          
            
        

        
          
           const int64_t i1 = i11; 
        

        
          
           const int64_t i2 = i12; 
        

        
          
           const int64_t i3 = i13; 
        

        
          
            
        

        
          
           const char * src0_row = (const char *) src0->data + (  0 + i02*nb02 + i03*nb03     ); 
        
    
  


Looking for contributions to make this cleaner. If not, I will implement it some time in the next days.",23,69
2280,2023-07-19T17:44:25Z,2023-07-21T10:58:36Z,2023-07-21T10:58:36Z,5,4,24,"MPKonst shows it is only a reparameterization of the guidance scale here. Thus we remove it in order to

better align with the paper
align with the upcoming huggingface implementation
remove a useless hyper parameter

Related: #2083 #2217 #2135",4,6
2284,2023-07-20T04:26:24Z,2023-07-21T10:53:28Z,2023-07-21T10:53:28Z,4,43,2,"Hi! This PR resolve two simple issue for Poetry and FreeBSD users.
First, little change for ignore files for Poetry generated files in their venv process.
Second, a fix in Makefile for FreeBSD users. In this OS, platform x86_64 isn't recognize, the correct name is amd64. This fix resolve compilation issues (for example: not enabled AVX support in the generated binary) using CFLAGS and CXXFLAGS with -march=native and -mtune=native for default.  Tested in FreeBSD 13.2.
Finally, add two examples for interactive mode using Llama2 models (thx TheBloke for models)",2,0
2287,2023-07-20T08:35:16Z,2023-07-21T08:13:18Z,2023-07-21T08:13:18Z,1,9,8,"🚀 This PR is about to take the Miku.sh script to new heights! 💪 We're upgrading the default model to 'llama-2-7b-chat', increasing the context size to 4096, adding in-prefix/in-suffix options for seamless navigation, and switching to the mirostat_v2 sampler. These updates will not only improve performance but also enhance user experience! 🤩 Do you want to be part of this exciting journey? 🎉
Please review and accept this pull request as soon as possible!",2,0
2294,2023-07-20T14:10:19Z,2023-07-20T15:19:45Z,2023-07-20T15:19:45Z,2,136,111,"Along the same lines as #2290. Here the speedup is not quite as large as for Q4_K, but still significant:



Model
Master
This PR
Speedup




Q5_K_S 7B
26.2
22.8
14.9%


Q5_K_S 13B
46.1
39.3
17.4%


Q5_K_S 33B
115.5
97.0
19.1%


Q5_K_S 65B
214.6
181.1
18.5%


Q6_K 7B
25.6
24.6
4.1%


Q6_K 13B
46.1
44.3
4.1%


Q6_K 33B
116.6
111.0
5.0%



Table shows token generation time in ms/t on M2 Max with 30-core GPU. The system has 64 GB RAM and the 65B Q6_K model does not run successfully.",3,0
2295,2023-07-20T15:17:02Z,2023-07-21T14:27:51Z,2023-07-21T14:27:51Z,1,49,11,"This PR does two things:

Implement the customizable RoPE as per #2054
Change ggml_cuda_pool_malloc() to be more forward looking when allocating memory. With this change I'm able to run with 8k contexts on CUDA. Without the change, the maximum context before running out of VRAM is somewhere between 5k and 6k for 7B, and between 3.5k and 4k for 13B.

I'm finding that the --rope-freq-base option works much better than --rope-freq-scale. This graph, computed with the changes in this PR, shows wikitext perplexity for contexts up to 8192 for the LLaMA-1 7B, 13B, and 30B models. Shown is the ratio of the perplexity at a given context length to the perplexity at context = 2048 (max. training context for the LLaMA-1 models).

The dependence of the (approximately) best RoPE frequency needed as a function of context length (best as resulting in the lowest perplexity) is shown in the next graph. This can be fit with a second order polynomial as
base frequency  = 10000 * (-0.13436 + 0.80541 * x + 0.28833 * x^2) for 7B
base frequency  = 10000 * (-0.41726 + 1.1792 * x + 0.16915 * x^2) for 13B

where x = context size / 2048.",5,5
2297,2023-07-20T16:57:48Z,2023-07-21T07:44:40Z,2023-07-21T07:44:40Z,2,104,80,"Following in the footsteps of #2290 and #2294.
TG-128 in ms/t on M2 Max with 30-core GPU:



Model
Master
This PR
Speedup




7B
22.5
18.4
22.3%


13B
37.7
30.1
25.3%


33B
88.9
68.7
29.4%


65B
165.5
128.3
29.0%",4,1
2304,2023-07-21T03:15:07Z,2023-07-25T12:19:12Z,2023-07-25T12:19:12Z,3,34,17,"The BOS precedes the string specified by --in-prefix. Model generated EOS is now kept in the context.
It provides a way to strictly following the prompt format used in Llama-2-chat.
The EOS handling also benefits some existing finetunes that uses EOS to mark the end of turn.",8,15
2306,2023-07-21T05:48:34Z,2023-08-23T07:12:13Z,2023-08-23T07:12:13Z,2,74,8,"We accept an array of strings and numbers representing tokens, in addition to the current string valued prompt or content.
This allows direct token input, so that any special tokens can be processed and used at the frontend during the construction of the json data, before sending to the server. And the server does not need to know or parse special tokens from textual input.
With this, we can use EOS and BOS used in llama-2-chat models.",6,18
2307,2023-07-21T07:56:42Z,2023-07-21T14:05:31Z,2023-07-21T14:05:31Z,2,127,84,"Similar to #2290 but for Q3_K, which is notoriously difficult to implement efficiently.
The table gives TG-128 time for Q3_K_S in ms/t on M2 Max with 30-core GPU:



Model
Master
This PR
Speedup




7B
28.1
22.0
27.7%


13B
50.3
37.5
34.1%


33B
121.3
88.8
36.6%


65B
230.3
167.8
37.2%",3,0
2308,2023-07-21T16:05:24Z,2023-07-28T01:14:11Z,2023-07-28T01:14:11Z,1,14,0,"Added instructions for obtaining the models to the readme. Do we want the ""do not share"" bla bla bla stuff in there? At this point, access to at least LLaMA 2 is officially wide open anyways.",4,10
2312,2023-07-21T20:32:41Z,2023-07-22T12:21:24Z,2023-07-22T12:21:24Z,3,82,2,"This PR adds a --perplexity-lines parameter to the perplexity tool. In this mode the perplexity is calculated over each line of the prompt instead of over each ctx window.

HellaSwag scores is a great way to measure how much of the English language the model understands.
Make two runs on a model, one prompted with a file containing ""correct"" sentences (one per line) and another run with a file containing ""wrong"" sentences. The measured perplexity from both files can be used to compute a score that is linearly correlated to the HellaSwag score.
ppl_correct = Cumulative perplexity on each line of hellaswag_val_correct.txt, lower values are better.
ppl_wrong = Cumulative perplexity on each line of hellaswag_val_wrong.txt, higher values are better.
The formula (ppl_wrong - ppl_correct) / ppl_correct correlates linearly with HellaSwag scores on Open LLM Leaderboard.
Test files: klosax/ppl_hellaswag.
Open LLaMA 3B



200 lines
ppl_wrong
ppl_correct
formula x 100




F16
24.6445
16.0094
53.937455


Q8_0
24.6335
16.0000
53.959752


Q5_1
24.9139
16.2154
53.643474


Q4_0
25.4092
16.4574
54.393903






400 lines
ppl_wrong
ppl_correct
formula x 100




F16
23.7929
16.3507
45.515894


Q8_0
23.7787
16.3446
45.483392


Q5_1
24.0065
16.5328
45.205637


Q4_0
24.4787
16.8715
45.088413",2,0
2313,2023-07-21T22:33:29Z,2023-07-22T19:27:34Z,2023-07-22T19:27:34Z,1,5,5,"Currently 7b q3_K_S produces garbage results when using the mul_mat_vec_q CUDA kernels. There were two problems: that the amount of padding was too low for quantizations using 2 bits for the base values, and that the padding was filled with garbage after being zeroed.",3,2
2315,2023-07-21T22:54:13Z,,2023-08-21T20:20:19Z,17,609,146,"This PR addresses @vjeux 's comment. The proposed changes are necessary to see reasonable results for the attached test cases.
To further support is_unknown, is_control, is_byte and is_unused and more special cases it seems reasonable (or necessary?) to extend the binary vocabulary format.",5,22
2317,2023-07-22T05:53:19Z,2023-07-22T10:34:52Z,2023-07-22T10:34:52Z,1,58,0,"This is a rudimentary but pretty well working VIM plugin which uses the JSON API of the server executable for text completion.
Pretty sure there are still some newline character issues when used with C code for example, but it should not fail completely at least.",2,0
2322,2023-07-22T13:29:03Z,2023-07-23T05:49:21Z,2023-07-23T05:49:21Z,1,59,13,"Improves performance for  LLAMA_CUDA_FORCE_DMMV=ON on older GPUs. E.g., on GTX-1660 TG-128 time is 34.5 ms/t with this PR vs 37.1 ms/t on master. The change is performance neutral on modern cards (RTX-4080).
For LLAMA_CUDA_FORCE_DMMV=OFF

Recover Q4_K performance on modern GPUs. E.g., on my RTX-4080 TG-128 time drops to 8.4 ms/t from 9.15 ms/t and is again comparable to Q4_0 and Q4_K with LLAMA_CUDA_FORCE_DMMV=ON performance
On a somewhat older card (GTX-1660) TG-128 is reduced from 35.5 ms/t to 29.5 ms/t. This is better than the  LLAMA_CUDA_FORCE_DMMV=ON performance.



I'm noticing that the LLAMA_CUDA_FORCE_DMMV=OFF CUDA version is completely broken for a k-quants block size of 64 (it does not even compile). I guess, I will be fixing this in a separate PR.
Oh, also fixed an annoying warning that g_compute_capabilities is not used, which is triggered when LLAMA_CUDA_FORCE_DMMV=ON",3,5
2323,2023-07-22T14:12:51Z,2023-07-23T11:00:37Z,2023-07-23T11:00:37Z,2,22,1,These ops are for chatglm.cpp MPS implementation. Implement here first and then upstream to ggml.,2,3
2327,2023-07-22T19:17:59Z,2023-07-23T11:16:48Z,2023-07-23T11:16:48Z,1,5,40,"I had also started a VIM plugin for interfacing with the example server a couple weeks back, but figured it would be too niche to try and submit a pull request for.
Mine is a bit further along and adds a number of extra features including

Asynchronous execution
Response streaming
Support for autoloading
In buffer magic strings for modifying generation settings

While I remain unsure if all these features would be desired and don't want to overshadow the work of @whoreson, I do wish to share a couple specific improvements I found while working on mine.
This pull request

Greatly simplifies the problem of escaping by using the builtin json_ecode and json_decode functions
Removes the need for temporary files.",3,3
2329,2023-07-22T20:56:30Z,2023-07-25T12:32:21Z,2023-07-25T12:32:21Z,3,42,12,"Instead of checking the list of current nodes in ggml_visit_parents() to determine if a node has already been visited, a visited flag is added to ggml_tensor.
Unfortunately this requires resetting the visited flag to reuse the tensors in a different graph, and because in some cases graphs are built in multiple steps, it is not easy to do this automatically. Currently, this is done automatically when calling ggml_graph_plan(), but there may be some cases where it can cause current ggml code to fail silently.",2,3
2331,2023-07-22T21:14:08Z,2023-07-23T11:52:08Z,2023-07-23T11:52:08Z,2,22,3,"Hi, this PR has two fixes:

New bits in Makefile for add CLBLAST support in FreeBSD.
Instructions for compile llama.cpp in FreeBSD using gmake. These instructions have the packages necessary for OpenBLAS and CLBLAST support for llama.cpp. These packages are available and up-to-date from FreeBSD 11 to FreeBSD 13 in almost architectures supported (only aarch64 remain with lagged support).

All tested in FreeBSD 13.2, local compilation and testing without issues.",3,2
2333,2023-07-22T23:10:30Z,2023-07-23T12:36:02Z,2023-07-23T12:36:02Z,4,226,486,,2,1
2336,2023-07-23T09:31:03Z,2023-07-23T11:56:34Z,2023-07-23T11:56:34Z,1,1,1,Print max tensor size to stderr instead of stdout,2,0
2337,2023-07-23T10:29:33Z,2023-07-23T11:57:02Z,2023-07-23T11:57:02Z,1,27,11,"Fix #2335.
Now:

nix build '.#opencl' will use clblast to build llama-cpp
nix build or nix build '.#default' is same as before

$ nix build '.#opencl'
$ ldd result/lib/libggml_shared.so|rg clblast
4:      libclblast.so.1 => /nix/store/kxx2lz78dznbdkfrwq0lnpzvpakjhs08-clblast-1.6.0/lib/libclblast.so.1 (0x00007f72ee400000)
$ nix build
$ ldd result/lib/libggml_shared.so|rg blas
2:      libopenblas.so.0 => /nix/store/0cz1gdz17hk9myawjpycscm4dnhy7b3l-openblas-0.3.21/lib/libopenblas.so.0 (0x00007f8aecd90000)",2,0
2338,2023-07-23T10:41:57Z,2023-07-23T11:59:49Z,2023-07-23T11:59:49Z,1,79,79,"I have always been irritated by the fact that the help text cannot be piped. Neither a falcon-main --help |less (the help is meanwhile 79 lines long) will work nor a falcon-main --help |grep -A3 penal (which should e.g. provide a compilation in a few lines with everything that has to do with penalties commands).
That is because the help output is written to stderr. But a help text output with --help is not an error message, but the desired text output of the help command.
This pr changes the output going to stdout, making possible to pipe the text
and allow processing with grep, less, etc.",2,0
2346,2023-07-23T13:24:27Z,2023-07-23T21:19:48Z,2023-07-23T21:19:48Z,1,84,30,"Quite minor on the modern card (RTX-4080), but fairly significant on the older card that I have available (GTX-1660). For Q5_K the same bit fiddling for computing the scales is added as in #2322. For both, we gain somewhat if we do 2 dot products per 32 quants in the dot product kernel. Both changes apply to LLAMA_CUDA_FORCE_DMMV=OFF.

TG-128 in ms/t on on GTX-1660 (6GB VRAM, so no 13B results, for Q5_K_S 32 layers fit on the GPU).




Quants
Master
PR
Speedup




Q4_K_S
29.5
25.6
15.2%


Q5_K_S
42.8
35.5
20.6%




TG-128 in ms/t on RTX-4080:




Quants
Master
PR
Speedup




Q4_K_S - 7B
8.40
8.23
2.1%


Q5_K_S - 7B
10.42
9.52
9.5%


Q4_K_S - 13B
14.64
14.38
1.8%


Q5_K_S - 13B
17.89
16.80
6.5%",3,2
2349,2023-07-23T15:44:20Z,2023-07-25T12:24:10Z,2023-07-25T12:24:10Z,1,3,2,"Interestingly I realized that in certain machines (mine Windows + WSL2 + Ubuntu 22.10), the build-info.h generated through the build process could have line break issues. The below was generated:
#ifndef BUILD_INFO_H
#define BUILD_INFO_H
#define BUILD_NUMBER 880


#define BUILD_COMMIT ""b9b7d94

""
#endif // BUILD_INFO_H

It seems that somehow two extra line breaks were added. This change will solve the issue, while not breaking things for other platforms.",3,2
2351,2023-07-23T16:51:46Z,2023-07-23T20:31:17Z,2023-07-23T20:31:17Z,1,47,35,"Add gqa parameter support to the server
Also change help to stdout to follow #2338",4,0
2356,2023-07-24T04:24:15Z,2023-07-25T12:16:13Z,2023-07-25T12:16:13Z,1,2,0,"This adds a ""noavx"" CI run to test the scalar code paths. While I'm sure that most of us run the SIMD versions, a correct reference implementation is essential for research and optimization tasks.
I created this PR in response to #2339, where broken scalar code was discovered.",2,0
2357,2023-07-24T04:36:43Z,2023-09-01T13:32:15Z,2023-09-01T13:32:15Z,1,42,0,"Based on the work in #1773.
Some thoughts:

When developing the grammar, sometimes it segfaults unexpectedly, I later found it it's probably due to left-recursion. Should we perform some normalization on the grammar to reduce this?
Base model still has a tendency to hallucinate even with the grammar direction, sometimes in variable names. Very strange.
How does this interact with finetuned models? I also tried this extension with WizardLM but haven't run anything concrete.
Sometimes the model generates large sequences of whitespace with my grammar. Is it something with the rules or can certain productions be penalized?


Generating a function to square a number
nix-shell-env ❯ ./main -ngl 1 -m ./models/llama-30b.ggmlv3.q3_K_S.bin --color -p $'// Function to square a number\n\n' --grammar-file grammars/c.gbnf
main: build = 895 (84e09a7)
main: seed  = 1690172920
llama.cpp: loading model from ./models/llama-30b.ggmlv3.q3_K_S.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_head_kv  = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: n_gqa      = 1
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: freq_base  = 10000.0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 11 (mostly Q3_K - Small)
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.16 MB
llama_model_load_internal: mem required  = 13820.68 MB (+  780.00 MB per state)
llama_new_context_with_model: kv self size  =  780.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading '/Users/siraben/Git/llama.cpp/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x12670d740
ggml_metal_init: loaded kernel_add_row                        0x12670dac0
ggml_metal_init: loaded kernel_mul                            0x12670de40
ggml_metal_init: loaded kernel_mul_row                        0x12670e2d0
ggml_metal_init: loaded kernel_scale                          0x12670e650
ggml_metal_init: loaded kernel_silu                           0x12670e9d0
ggml_metal_init: loaded kernel_relu                           0x12670ed50
ggml_metal_init: loaded kernel_gelu                           0x12670f0d0
ggml_metal_init: loaded kernel_soft_max                       0x12670f5e0
ggml_metal_init: loaded kernel_diag_mask_inf                  0x12670faa0
ggml_metal_init: loaded kernel_get_rows_f16                   0x12670ff80
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x126710460
ggml_metal_init: loaded kernel_get_rows_q4_1                  0x126710940
ggml_metal_init: loaded kernel_get_rows_q2_K                  0x126710e20
ggml_metal_init: loaded kernel_get_rows_q3_K                  0x126711300
ggml_metal_init: loaded kernel_get_rows_q4_K                  0x1267117e0
ggml_metal_init: loaded kernel_get_rows_q5_K                  0x126711cc0
ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1267121a0
ggml_metal_init: loaded kernel_rms_norm                       0x1267126c0
ggml_metal_init: loaded kernel_norm                           0x126712bd0
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x126713290
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1267137b0
ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x126713cd0
ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x126714370
ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x126714890
ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x126714db0
ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1267152b0
ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x126715c10
ggml_metal_init: loaded kernel_rope                           0x126715f90
ggml_metal_init: loaded kernel_alibi_f32                      0x1267166b0
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x126716da0
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x126717490
ggml_metal_init: loaded kernel_cpy_f16_f16                    0x126717b80
ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB
ggml_metal_init: hasUnifiedMemory             = true
ggml_metal_init: maxTransferRate              = built-in GPU
llama_new_context_with_model: max tensor size =    87.28 MB
ggml_metal_add_buffer: allocated 'data            ' buffer, size = 13332.97 MB, (13333.42 / 21845.34)
ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (13349.42 / 21845.34)
ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (14131.42 / 21845.34)
ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (14347.42 / 21845.34)
ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (14603.42 / 21845.34)

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


main: grammar:
root ::= root_3 
root_1 ::= declaration 
declaration ::= dataType identifier [(] declaration_7 [)] [{] declaration_9 [}] 
root_3 ::= root_1 root_3 | 
dataType ::= [i] [n] [t] ws | [f] [l] [o] [a] [t] ws | [c] [h] [a] [r] ws 
identifier ::= [a-zA-Z_] identifier_11 
parameter ::= dataType identifier 
declaration_7 ::= parameter | 
statement ::= statement_12 | statement_14 | statement_15 | statement_18 | statement_19 | statement_22 | statement_26 | statement_31 | statement_33 
declaration_9 ::= statement declaration_9 | 
ws ::= ws_56 
identifier_11 ::= [a-zA-Z_0-9] identifier_11 | 
statement_12 ::= dataType identifier ws [=] ws expression [;] 
expression ::= expression_37 | expression_42 
statement_14 ::= identifier ws [=] ws expression [;] 
statement_15 ::= identifier ws [(] statement_17 [)] [;] 
argList ::= expression argList_49 
statement_17 ::= argList | 
statement_18 ::= [r] [e] [t] [u] [r] [n] ws expression [;] 
statement_19 ::= [w] [h] [i] [l] [e] [(] condition [)] [{] statement_21 [}] 
condition ::= expression relationOperator expression 
statement_21 ::= statement statement_21 | 
statement_22 ::= [f] [o] [r] [(] forInit [;] ws condition [;] ws forUpdate [)] [{] statement_25 [}] 
forInit ::= dataType identifier ws [=] ws expression | identifier ws [=] ws expression 
forUpdate ::= identifier ws [=] ws expression 
statement_25 ::= statement statement_25 | 
statement_26 ::= [i] [f] [(] condition [)] [{] statement_27 [}] statement_30 
statement_27 ::= statement statement_27 | 
statement_28 ::= [e] [l] [s] [e] [{] statement_29 [}] 
statement_29 ::= statement statement_29 | 
statement_30 ::= statement_28 | 
statement_31 ::= singleLineComment 
singleLineComment ::= [/] [/] singleLineComment_52 [<U+000A>] 
statement_33 ::= multiLineComment 
multiLineComment ::= [/] [*] multiLineComment_55 [*] [/] 
relationOperator ::= relationOperator_36 
relationOperator_36 ::= [<] [=] | [<] | [=] [=] | [!] [=] | [>] [=] | [>] 
expression_37 ::= term expression_41 
term ::= identifier | number | unaryTerm | funcCall | parenExpression 
expression_39 ::= operator term 
operator ::= operator_50 
expression_41 ::= expression_39 expression_41 | 
expression_42 ::= [(] ws expression ws [)] 
number ::= number_51 
unaryTerm ::= [-] term 
funcCall ::= identifier [(] funcCall_47 [)] 
parenExpression ::= [(] ws expression ws [)] 
funcCall_47 ::= argList | 
argList_48 ::= [,] ws expression 
argList_49 ::= argList_48 argList_49 | 
operator_50 ::= [+] | [-] | [*] | [/] 
number_51 ::= [0-9] number_51 | [0-9] 
singleLineComment_52 ::= [^<U+000A>] singleLineComment_52 | 
multiLineComment_53 ::= [^*] | multiLineComment_54 
multiLineComment_54 ::= [*] [^/] 
multiLineComment_55 ::= multiLineComment_53 multiLineComment_55 | 
ws_56 ::= ws_57 
ws_57 ::= [ <U+0009><U+000A>] ws_57 | [ <U+0009><U+000A>] 

 // Function to square a number

int squareNumber(int x){return x*x;} [end of text]

llama_print_timings:        load time =   653.95 ms
llama_print_timings:      sample time =    73.12 ms /    14 runs   (    5.22 ms per token,   191.47 tokens per second)
llama_print_timings: prompt eval time =  3384.85 ms /     9 tokens (  376.09 ms per token,     2.66 tokens per second)
llama_print_timings:        eval time =  2663.51 ms /    13 runs   (  204.89 ms per token,     4.88 tokens per second)
llama_print_timings:       total time =  6132.45 ms
ggml_metal_free: deallocating




Fibonacci
nix-shell-env ❯ ./main -ngl 1 -m ./models/llama-30b.ggmlv3.q3_K_S.bin --color -p $'Fibonacci in C:\n\n' --grammar-file grammars/c.gbnf
main: build = 895 (84e09a7)
main: seed  = 1690175698
llama.cpp: loading model from ./models/llama-30b.ggmlv3.q3_K_S.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 6656
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 52
llama_model_load_internal: n_head_kv  = 52
llama_model_load_internal: n_layer    = 60
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: n_gqa      = 1
llama_model_load_internal: n_ff       = 17920
llama_model_load_internal: freq_base  = 10000.0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 11 (mostly Q3_K - Small)
llama_model_load_internal: model size = 30B
llama_model_load_internal: ggml ctx size =    0.16 MB
llama_model_load_internal: mem required  = 13820.68 MB (+  780.00 MB per state)
llama_new_context_with_model: kv self size  =  780.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading '/Users/siraben/Git/llama.cpp/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x1320091e0
ggml_metal_init: loaded kernel_add_row                        0x132009440
ggml_metal_init: loaded kernel_mul                            0x1320097c0
ggml_metal_init: loaded kernel_mul_row                        0x132009c50
ggml_metal_init: loaded kernel_scale                          0x132009fd0
ggml_metal_init: loaded kernel_silu                           0x13200a350
ggml_metal_init: loaded kernel_relu                           0x13200a6d0
ggml_metal_init: loaded kernel_gelu                           0x13200aa50
ggml_metal_init: loaded kernel_soft_max                       0x13200af60
ggml_metal_init: loaded kernel_diag_mask_inf                  0x13200b420
ggml_metal_init: loaded kernel_get_rows_f16                   0x13200b900
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13200bde0
ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13200c2c0
ggml_metal_init: loaded kernel_get_rows_q2_K                  0x110e04cb0
ggml_metal_init: loaded kernel_get_rows_q3_K                  0x110e05190
ggml_metal_init: loaded kernel_get_rows_q4_K                  0x110e05670
ggml_metal_init: loaded kernel_get_rows_q5_K                  0x110e05b50
ggml_metal_init: loaded kernel_get_rows_q6_K                  0x110e06030
ggml_metal_init: loaded kernel_rms_norm                       0x110e06550
ggml_metal_init: loaded kernel_norm                           0x110e06a60
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x110e07120
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x110e07640
ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x110e07b60
ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x110e08200
ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x110e08720
ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x110e08c40
ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x110e09140
ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x110e09aa0
ggml_metal_init: loaded kernel_rope                           0x110e09e20
ggml_metal_init: loaded kernel_alibi_f32                      0x110e0a540
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x110e0ac30
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x110e0b320
ggml_metal_init: loaded kernel_cpy_f16_f16                    0x110e0ba10
ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB
ggml_metal_init: hasUnifiedMemory             = true
ggml_metal_init: maxTransferRate              = built-in GPU
llama_new_context_with_model: max tensor size =    87.28 MB
ggml_metal_add_buffer: allocated 'data            ' buffer, size = 13332.97 MB, (13333.42 / 21845.34)
ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    16.00 MB, (13349.42 / 21845.34)
ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   782.00 MB, (14131.42 / 21845.34)
ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   216.00 MB, (14347.42 / 21845.34)
ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, (14603.42 / 21845.34)

system_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0


main: grammar:
root ::= root_3 
root_1 ::= declaration 
declaration ::= dataType identifier [(] declaration_7 [)] [{] declaration_9 [}] 
root_3 ::= root_1 root_3 | 
dataType ::= [i] [n] [t] ws | [f] [l] [o] [a] [t] ws | [c] [h] [a] [r] ws 
identifier ::= [a-zA-Z_] identifier_11 
parameter ::= dataType identifier 
declaration_7 ::= parameter | 
statement ::= statement_12 | statement_14 | statement_15 | statement_18 | statement_19 | statement_22 | statement_26 | statement_31 | statement_33 
declaration_9 ::= statement declaration_9 | 
ws ::= ws_57 
identifier_11 ::= [a-zA-Z_0-9] identifier_11 | 
statement_12 ::= dataType identifier ws [=] ws expression [;] 
expression ::= term expression_40 
statement_14 ::= identifier ws [=] ws expression [;] 
statement_15 ::= identifier ws [(] statement_17 [)] [;] 
argList ::= expression argList_51 
statement_17 ::= argList | 
statement_18 ::= [r] [e] [t] [u] [r] [n] ws expression [;] 
statement_19 ::= [w] [h] [i] [l] [e] [(] condition [)] [{] statement_21 [}] 
condition ::= expression relationOperator expression 
statement_21 ::= statement statement_21 | 
statement_22 ::= [f] [o] [r] [(] forInit [;] ws condition [;] ws forUpdate [)] [{] statement_25 [}] 
forInit ::= dataType identifier ws [=] ws expression | identifier ws [=] ws expression 
forUpdate ::= identifier ws [=] ws expression 
statement_25 ::= statement statement_25 | 
statement_26 ::= [i] [f] [(] condition [)] [{] statement_27 [}] statement_30 
statement_27 ::= statement statement_27 | 
statement_28 ::= [e] [l] [s] [e] [{] statement_29 [}] 
statement_29 ::= statement statement_29 | 
statement_30 ::= statement_28 | 
statement_31 ::= singleLineComment 
singleLineComment ::= [/] [/] singleLineComment_53 [<U+000A>] 
statement_33 ::= multiLineComment 
multiLineComment ::= [/] [*] multiLineComment_56 [*] [/] 
relationOperator ::= relationOperator_36 
relationOperator_36 ::= [<] [=] | [<] | [=] [=] | [!] [=] | [>] [=] | [>] 
term ::= factor term_44 
expression_38 ::= expression_39 term 
expression_39 ::= [+] | [-] 
expression_40 ::= expression_38 expression_40 | 
factor ::= identifier | number | unaryTerm | funcCall | parenExpression 
term_42 ::= term_43 factor 
term_43 ::= [*] | [/] 
term_44 ::= term_42 term_44 | 
number ::= number_52 
unaryTerm ::= [-] factor 
funcCall ::= identifier [(] funcCall_49 [)] 
parenExpression ::= [(] ws expression ws [)] 
funcCall_49 ::= argList | 
argList_50 ::= [,] ws expression 
argList_51 ::= argList_50 argList_51 | 
number_52 ::= [0-9] number_52 | [0-9] 
singleLineComment_53 ::= [^<U+000A>] singleLineComment_53 | 
multiLineComment_54 ::= [^*] | multiLineComment_55 
multiLineComment_55 ::= [*] [^/] 
multiLineComment_56 ::= multiLineComment_54 multiLineComment_56 | 
ws_57 ::= ws_58 
ws_58 ::= [ <U+0009><U+000A>] ws_58 | [ <U+0009><U+000A>] 

 Fibonacci in C:

char F(int n){int nFibonacci = 0;if(n<2){return 1;}else{return F(n-2)+F(n-1);}} [end of text]

llama_print_timings:        load time = 23430.51 ms
llama_print_timings:      sample time =   279.79 ms /    48 runs   (    5.83 ms per token,   171.55 tokens per second)
llama_print_timings: prompt eval time =  6594.15 ms /    10 tokens (  659.42 ms per token,     1.52 tokens per second)
llama_print_timings:        eval time =  9306.79 ms /    47 runs   (  198.02 ms per token,     5.05 tokens per second)
llama_print_timings:       total time = 16222.58 ms
ggml_metal_free: deallocating",4,5
2358,2023-07-24T05:46:34Z,2023-07-25T12:00:19Z,2023-07-25T12:00:19Z,3,138,19,"Discussion see #2309
Function ggml_metal_graph_find_concurrency will write commands that can be issued concurrently to metal context concur_list array, when ggml_metal_graph_compute is called for the first time.
Tested on M1 Max 32c gpu with :
./main -m model_file -n 256 -c 512 -s 123 -p ""I believe the meaning of life is"" --ignore-eos -ngl 1 --no-mmap -t 8



model
master
PR
improvement




33B Q4_0
73.0 ms/tok
70.1 ms/tok
~4%


7B Q4_0
19.50 ms/tok
18.26 ms/tok
~6%



Btw, since we already have much faster kernels for both Q_K and non Q_K, is it a good time to do prompt evaluation on metal? Although current kernels are optimized for mat-vec multiplications, they still provide much better performance than cpu. (EDIT: not true for M1 Pro)",3,0
2359,2023-07-24T06:40:17Z,2023-07-25T10:48:04Z,2023-07-25T10:48:04Z,1,80,3,"They were broken (don't even compile) when LLAMA_CUDA_FORCE_DMMV=OFF. As per @JohannesGaessler, when added the matrix times vector versions that use integer SIMD intrinsics, he did not implement  for QK_K = 64 as he considered QK_K = 64 to be a temporary fix. I agree with this sentiment, but in the meantime it is not good to have broken stuff on master, so this PR fixes that.
A fix was needed only for Q4_K and Q5_K. Q2_K, Q3_K, and Q6_K worked out of the box.
Performance is not great, but I did not want to spend the time optimizing a temporary solution.",3,0
2362,2023-07-24T07:55:40Z,2023-07-24T09:55:02Z,2023-07-24T09:55:02Z,1,2,3,"When developing the 64-weight block size for k-quants, at some point I switched Q5_K to use ""type-0"" quantization. Apparently I forgot to adjust the scalar version of the code.
Thanks to @katsu560 and @netrunnereve for noticing the problem.
Performance is not too bad actually. On a Ryzen-7950x (with AVX/AVX2 disabled), less than a factor of 2 slower than AVX2:
make clean && LLAMA_FAST=1 LLAMA_QKK_64=1 make -j
I llama.cpp build info: 
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS
I LDFLAGS:  
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0

rm -vf *.o *.so *.dll main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0
removed 'common.o'
removed 'ggml.o'
removed 'grammar-parser.o'
removed 'k_quants.o'
removed 'llama.o'
removed 'libembdinput.so'
removed 'main'
removed 'quantize'
removed 'quantize-stats'
removed 'perplexity'
removed 'embedding'
removed 'server'
removed 'simple'
removed 'vdot'
removed 'train-text-from-scratch'
removed 'embd-input-test'
removed 'build-info.h'
I llama.cpp build info: 
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -Ofast -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64
I CXXFLAGS: -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64
I LDFLAGS:  
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0

cc  -I.              -Ofast -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64   -c ggml.c -o ggml.o
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 -c llama.cpp -o llama.o
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 -c examples/common.cpp -o common.o
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 -c examples/grammar-parser.cpp -o grammar-parser.o
cc -I.              -Ofast -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64   -c -o k_quants.o k_quants.c
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/main/main.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o -o main 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server  
g++ --shared -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so 
g++ -I. -I./examples -Ofast -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_QKK_64 examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput

====  Run ./main -h for help.  ====

./main -m q5_64.bin -p ""I believe the meaning of life is"" --ignore-eos -n 128 -s 1234 -t 16
main: build = 895 (84e09a7)
main: seed  = 1234
llama.cpp: loading model from cuda/q5_64.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_head_kv  = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: n_gqa      = 1
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: freq_base  = 10000,0
llama_model_load_internal: freq_scale = 1
llama_model_load_internal: ftype      = 16 (mostly Q5_K - Small)
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0,08 MB
llama_model_load_internal: mem required  = 4937,42 MB (+  256,00 MB per state)
llama_new_context_with_model: kv self size  =  256,00 MB

system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | 
sampling: repeat_last_n = 64, repeat_penalty = 1,100000, presence_penalty = 0,000000, frequency_penalty = 0,000000, top_k = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p = 1,000000, temp = 0,800000, mirostat = 0, mirostat_lr = 0,100000, mirostat_ent = 5,000000
generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0


 I believe the meaning of life is to enjoy every minute.
I also believe that we should live by our own values and not those set down in a book written by others, as we’re all different.
That being said, religion does teach many values which are important. But it can also be used to justify hatred and evil actions.
Saying that you are against killing but then going out there every day wearing clothes that were made from the leather of dead animals is hypocritical. If your goal is to save every animal in the world, stop eating meat for starters. You’d be amazed at how many
llama_print_timings:        load time =   260,60 ms
llama_print_timings:      sample time =    45,44 ms /   128 runs   (    0,35 ms per token,  2817,15 tokens per second)
llama_print_timings: prompt eval time =  1194,63 ms /     8 tokens (  149,33 ms per token,     6,70 tokens per second)
llama_print_timings:        eval time = 19893,37 ms /   127 runs   (  156,64 ms per token,     6,38 tokens per second)
llama_print_timings:       total time = 21152,60 ms",3,0
2366,2023-07-24T09:15:06Z,2023-07-24T14:54:22Z,2023-07-24T14:54:22Z,3,757,445,"This PR tweaks the server example's UI a bit and exposes the generation parameters that weren't previously available in a neat <details> disclosure box.
Screenshot",2,5
2368,2023-07-24T09:54:26Z,2023-07-25T07:27:34Z,2023-07-25T07:27:34Z,2,130,119,Allowing code to be shown without the page interpreting it.,2,0
2371,2023-07-24T11:43:28Z,2023-07-25T12:58:32Z,2023-07-25T12:58:33Z,1,18,9,"The current ggml implementation of activation function (gelu/silu) doesn't require a completely contiguous input. Only dim 0, 2, 3 need to be contiguous and dim 1 can be excluded from the contiguous check.
This is helpful for SwiGLU activation in ChatGLM2. If we relax the constraint, the ggml_cont for x0 can be removed and it will run 20% faster in MPS backend.
ggml_tensor *GLM2MLP::forward(ModelContext *ctx, ggml_tensor *hidden_states) const {
    ggml_context *gctx = ctx->ctx_b.get();

    ggml_tensor *output = dense_h_to_4h.forward(ctx, hidden_states);

    // swiglu activation
    ggml_tensor *x0 = ggml_view_2d(gctx, output, output->ne[0] / 2, output->ne[1], output->nb[1], 0);
    x0 = ggml_cont(gctx, x0);
    x0 = ggml_silu_inplace(gctx, x0);
    ggml_tensor *x1 = ggml_view_2d(gctx, output, output->ne[0] / 2, output->ne[1], output->nb[1],
                                   output->ne[0] / 2 * ggml_element_size(output));
    output = ggml_mul_inplace(gctx, x0, x1);

    output = dense_4h_to_h.forward(ctx, output);
    return output;
}",2,0
2372,2023-07-24T12:44:36Z,2023-08-07T11:25:58Z,2023-08-07T11:25:58Z,1,79,55,"ref: ggerganov/ggml#293
See if #2355 can be resolved and also make sure we broadcast correctly across dims 2 and 3
Also, did a first try for block tiled matrix multiplication which seems to improve slightly the performance.
Currently, my tests on x86 show that OpenBLAS does not offer any advantage compared to non-BLAS both for prompt processing and text generation.
Looking for reports where this is not the case. Make sure to use thread count equal to the number of performance cores on your system.
TODO:

 add broadcast support for CPU BLAS path (ggerganov/ggml#412 (comment))
 add broadcast support in Metal",5,6
2374,2023-07-24T14:19:46Z,2023-07-24T15:57:13Z,2023-07-24T15:57:13Z,11,89,56,"Fixes #2373
Use -eps 1e-5 with llama 2, defaults to 1e-6 (same as current, for llama v1).",2,3
2375,2023-07-24T15:50:16Z,2023-07-25T10:48:30Z,2023-07-25T10:48:30Z,1,57,54,"In #2279 there was some debate on whether it is better to have each thread in a Metal SIMD group process whole Q4_0 or Q4_1 blocks, or have them process half blocks.
On my M2 Max with 30-core GPU having threads process half a block is definitely faster, see table below. I'm curious to see if this change also improves (or worsens) performance on M1 Pro/Max.
TG-128 in ms/token on 30-core M2 Max. The command used was
./main -m $model -p ""I believe the meaning of life is"" --ignore-eos -n 128 -s 1234 --no-mmap -t 8 -ngl 1




Quantization
t/s Master
t/s PR
speedup




Q4_0 - 7B
18.6
17.6
5.7%


Q4_1 - 7B
20.0
18.9
5.8%


Q4_0 - 13B
32.5
30.1
8.0%


Q4_1 - 13B
33.7
32.2
4.7%


Q4_0 - 33B
76.5
67.2
11.4%


Q4_1 - 33B
82.3
75.3
9.3%


Q4_0 - 65B
146.8
133.4
10.0%


Q4_1 - 65B
152.8
145.4
5.1%",4,4
2380,2023-07-24T20:48:27Z,2023-07-25T09:36:18Z,2023-07-25T09:36:18Z,1,9,0,"For use with llama 2, #2374.",2,0
2384,2023-07-25T12:42:46Z,2023-07-25T15:35:54Z,2023-07-25T15:35:54Z,5,13,5,"PR #2374 made the epsilon used the rms_norm operation to be a parameter with a default value of 1e-6 that can be changed via a command line option. This is to account for the fact that in LLaMA-1 epsilon = 1e-6 was used during training, while LLaMA-2 was trained with epsilon = 1e-5.  Using epsilon = 1e-6 for LLaMA-2 results in significantly lower perplexity scores (see #2373, #2352, and the graphs below), so being able to change RMS epsilon via the command line is great. Until one has to run many perplexity calculations, as I'm currently doing in an attempt to improve the Q4_K performance for LLaMA-2. In this process I'm making a change to the Q4_K quantization, then running perplexity for LLaMA-1 and LLaMA-2, frequently forgetting to modify the epsilon from the last run, and hence wasting a lot of time waiting for the calculation to finish to only then realize that I have used the wrong epsilon.
So, I decided to see if there is an epsilon that works equally well for both LLaMA models. Turns out epsilon = 5e-6 is this magic value. If anything, epsilon = 5e-6 slightly improves perplexity scores for LLaMA-2. For LLaMA-1, perplexity scores are the same as with epsilon = 1e-6 within 1.5 X maximum training context (so, up to and including 3072 tokens), and are only very slightly higher for context lengths of 4096 and beyond. This can be seen in the graphs below.
Given this finding, this PR adds a macro LLAMA_DEFAULT_RMS_EPS, set to 5e-6f. All rms_norm default values are set via this macro, which can be changed at build time (make clean &&  LLAMA_DEFAULT_RMS_EPS=YourPreferredChoice make) instead of being hard-coded to 1e-6. One can of course still change epsilon via the command line.


Figures show perplexity as a function of context size for the 7B and 13B LLaMA models. The black line+circles represent LLaMA-1 results with epsilon = 1e-6. The orange squares depict LLaMA-1 with epislon = 5e-6 (proposed default value for both models). Red line+circles are for LLaMA-2 with epsilon = 1e-6. They were computed before we had realized the rms_norm epsilon issue and are included in the graph to illustrate the magnitude of the perplexity loss due to using the wrong epsilon value. The blue line/circles show LLaMA-2 results for epsilon = 1e-5, and the magenta are for LLaMA-2 with epsilon = 5e-6. The calculations beyond the maximum training context were run with base RoPE frequency selected to minimize the perplexity score.",4,2
2389,2023-07-25T16:24:37Z,2023-07-28T18:25:36Z,2023-07-28T18:25:36Z,3,155,45,"This removes the simple HellaSwag perplexity-lines added in PR #2312 and replaces it with a real HellaSwag score test. The simple test was found to be too inaccurate.
The HellaSwag test needs a datafile extracted from the offical HellaSwag dataset which can be found here klosax/hellaswag_text_data.
Parameters added --hellaswagand --hellaswag-tasks.
See my post #2321 for more information.",4,10
2391,2023-07-25T17:04:59Z,2023-08-04T11:37:25Z,2023-08-04T11:37:25Z,2,42,19,"This PR fixes two problems:

A race condition in server.cpp.
completion.js didn't handle partial stream results.

The race condition was caused by unique_lock scope loss when processing streaming completion.  My fix was to handle unlocking of the mutex manually in that case. This bug caused segfaults when handling multiple streaming requests on the completion endpoint.
Partial stream results happen regularly on slower connections, or when the server is generating messages very quickly. The fix was to handle leftover data in the llama generator function. This bug caused completion messages to be garbled in several cases.",3,2
2392,2023-07-25T18:48:39Z,2023-07-26T13:56:53Z,2023-07-26T13:56:53Z,3,134,88,"Adds ggml_new_object to allocate objects in a context (internal to ggml)
Adds ggml_new_graph, ggml_build_forward_ctx, ggml_graph_overhead to support allocating graphs in a context
Modifies ggml_graph_compute_with_ctx to use ggml_new_object instead of a tensor
Modifies llama.cpp to allocate the eval graph on the compute context

Fixes ggerganov/ggml#299",2,0
2394,2023-07-25T22:43:43Z,2023-07-26T18:00:05Z,2023-07-26T18:00:05Z,2,3,1,The warning flag was not carried over when the static-correctness changes were merged from GGML.,2,0
2398,2023-07-26T08:19:08Z,2023-08-21T20:07:43Z,2023-08-21T20:07:43Z,54,10090,3025,"ref:

ggerganov/ggml#302
#1991

This PR paves the way for integrating more models into llama.cpp. It changes the file format in which we convert the models by extending it with key-value pairs meta information. This meta data is flexible and allows to add specific information about the model being converted.
This is a breaking change, meaning that all existing ggml models will no longer be compatible after merging.
You should obtain the original model data (e.g. Meta PTH or Hugging Face, etc) and use the convert.py script to generate the new F16 or F32 .gguf models. From there, you can use all tools as usual: quantize, main, perplexity, etc.
Read more about the GGUF file format here: ggerganov/ggml#302
The PR also refactors some portions of llama.cpp and mainly extends ggml with a gguf API for loading and writing .gguf files. It incorporates LLaMA tokenizer fixes and adds (temporary?) BPE tokenizer support.
Huge thanks to all the contributors for writing the spec and helping with the implementation ❤️
Merge ETA: 21 Aug
Usage
# build the GGUF read/write example
make gguf

# write a dummy GGUF model to test.gguf
./gguf test.gguf w

# read the dummy GGUF model
./gguf test.gguf r

# LLaMA 1 PTH
python3 convert.py ../llama1/7B/ --outfile models/7B/ggml-model-f16.gguf

# LLaMA 2 PTH
python3 convert.py ../llama2/llama/llama-2-7b --outfile models/7B-v2/ggml-model-f16.gguf

# LLaMA 2 HF
python3 convert.py ~/development/huggingface/Llama-2-7b-hf/ --outfile models/7B-v2/ggml-model-f16.gguf

# vocab-only
python3 convert.py ../llama1/7B/ --outfile models/ggml-vocab-llama.gguf --vocab-only
python3 convert.py ~/development/huggingface/Llama-2-7b-hf/ --outfile models/ggml-vocab-llama.gguf --vocab-only
Convert GGML to GGUF (not guaranteed to work)

Implementation plan:

 implement GGUF import in ggml
 add sample code to write / read dummy GGUF files
 refactor convert.py + export GGUF

 remove quantized data support - only F16 and F32 types
 remove GPTQ support
 output GGUF for LLaMAv2

 Hugging Face (convert-llama-h5-to-gguf.py)
 Vanilla PTH (convert.py)




 refactor llama.cpp model loading code to simplify things and drop obsolete stuff

 remove llama-utils.h by merging it in llama.cpp
 refactor: #2620


 utilize the new gguf API in llama.cpp to load new GGUF F16 models
 extend the gguf API to be able to output quantized GGUF models

 as a first step, the user code can write the models
 later, when we see what API we need, we can move the implementation as part of ggml


refactor llama.cpp to add support for alternative model inference graphs left for future PR
demonstrate integration with MPT or Falcon - i.e. be able to seamlessly load and inference any of the 2 models with main left for future PR
 bring the #2549 tokenizer fixes so we have a single breaking change instead of two

Contributions are welcome - just make the target branch this one.
Collaborators can push directly in the branch.
TODO

 re-enable CI before merging
 pick #2427
 pick #2615
 pick #2627
 pick #2538
 pick #2553
 pick #2626
make it work with F16 1D tensors (too big change for this PR, will fix later)
 update LoRA to use gguf
 use gguf for examples/train-text-from-scratch model writing
 use gguf for examples/convert-llama2c-to-ggml model writing
 avoid #define LLAMA_API_CPP
 add model's ""max context size"" to meta data
 remove ftype print
 print stat info about the number of tensors for each quantization (replacement for ftype)
 print total number of parameters computed from tensor sizes
 handle special tokens support (will rely on contributions)
 export token types
 special tokens, bpe merges, rope scaling",18,264
2400,2023-07-26T10:25:12Z,2023-07-28T18:02:10Z,2023-07-28T18:02:10Z,2,135,0,"Added a script to chat with llama2 in server mode for llama.cpp.

examples/server-llama2-13B.sh

Start the server with llama2 13B.


examples/server/chat-llama2.sh

Script to chat with llama2 when the server is running. Modifications from chat.sh are as follows:

Added support for the prompt format of llama2 chat.
Fixed an issue where newlines (\n) in llama2 output were not displayed correctly in the terminal.
Changed user input to appear in green (like ./main).",2,0
2405,2023-07-26T16:58:09Z,2023-07-28T18:17:45Z,2023-07-28T18:17:45Z,1,32,30,"Fix issue #1279
PR #1638 fixed the compilation failure but the new-added file k_quants.c seems break the compilation again. This PR try to fix the problem by applying the same workaround (declare a compatible macro) to k_quants.c. 😊",3,0
2410,2023-07-26T20:12:30Z,2023-07-26T21:57:23Z,2023-07-26T21:57:24Z,1,1,6,"The assert in ggml_set_unary_op was an assignment instead of a comparison. Changing it to a comparison wouldn't be enough either, because at the time this function is called, the op still isn't set.
There could be several ways to fix this, I chose to remove ggml_set_unary_op entirely because I don't think we will ever need to use this function outside ggml_unary_impl, and it breaks the pattern of the way that op_params is set in other ops.",2,0
2411,2023-07-26T23:25:08Z,2023-07-30T13:58:02Z,2023-07-30T13:58:02Z,7,812,88,"Fixes ggerganov/ggml#288
Allocates tensors based on their usage in the computation graph and replaces scratch buffers. Depending on the batch size and context length, reduces the size of the compute buffers usually by 2x, up to 10x when using a lower batch size.
For example, currently the size of the compute and scratch buffers for 7B with n_ctx=2048, n_batch=512 is 398 MB. With this change, it is 153 MB instead, or 38 MB with n_batch=128.
The implementation is mostly self-contained. It is used by creating a ggml_context with the no_alloc flag, and then calling ggml_allocator_alloc_graph_tensors to allocate the tensors.
Not compatible with the GPU backends, only with CPU. It could possibly be used with Metal, but it would require disabling concurrent execution.",4,11
2414,2023-07-27T06:37:17Z,2023-08-01T08:56:24Z,2023-08-01T08:56:24Z,2,1064,1065,"So it respects user system light / dark settings in a very lightweight approach.

  
    
    

    darklight.mp4",3,2
2420,2023-07-27T09:00:45Z,2023-07-28T18:10:05Z,2023-07-28T18:10:05Z,1,3,1,"Hi, thanks for this awesome work.
I am currently trying to perform inference on different LLM (e.g., xgen and Aquila) using this project.
I always encounter issues with generating Chinese text smoothly.
By adopting the flag --verbose-prompt, I found that the Chinese words are always being tokenized into wrong token IDs.
After digging into the root cause, I found the reason is that the Chinese characters, which are composed of multiple bytes, are always tokenized incorrectly by this part.
llama_vocab::id token_id = static_cast<uint8_t>(symbol.text[j]) + 3;
This code can work for the llama series of models primarily because the llama's tokenizer follows the char
coding order and three special tokens are placed at the beginning:
'<unk>': 0,
'<s>': 1,
'</s>': 2,
'<0x00>': 3,
'<0x01>': 4,
'<0x02>': 5,
'<0x03>': 6,
...

Unfortunately, not all open-source pre-trained models adopt llama's tokenizer such as xgen and Aquila mentioned above.
Therefore, for more flexible support for more diverse pre-trained model tokenizers. I believe we should use the vocabulary generated by convert.py appropriately in this case.
For example, the xgen's tokenizer map looks like:
b'!': 0,
b'""': 1,
b'#': 2,
b'$': 3,
b'%': 4,
b'&': 5,
b""'"": 6,
b'(': 7,
...

Although this PR only modifies one line of code, it brings significant benefits for supporting more models with UTF-8 characters. Just like #2228, enabling only BPE in convert.py is not sufficient to successfully infer Chinese words without this modification.
Big thanks for this amazing work again!",5,8
2426,2023-07-27T19:27:39Z,2023-07-28T08:34:17Z,2023-07-28T08:34:17Z,3,73,36,,2,1
2431,2023-07-28T06:17:05Z,2023-07-28T08:44:44Z,2023-07-28T08:44:44Z,1,2,2,"The current description of the TFS sampling parameter is wildly wrong.
The previously suggested value of 2.0 would disable the sampling completely.
Also, the probabilities are not raised to the power of z. z is just a cut off value.
The new description should reflect the source paper more accurately https://www.trentonbricken.com/Tail-Free-Sampling/
IMHO: The writing style of current/wrong description of TFS looks awkwardly like it was written by an AI. Especially the almost 0 content leading sentence ""Tail free sampling (TFS) is a text generation technique that aims to reduce the impact of less likely tokens, which may be less relevant, less coherent, or nonsensical, on the output."". Exactly this is what all sampling methods also try to achieve (Top-K, Top-P, Rep Pen, ...).",2,0
2433,2023-07-28T07:28:44Z,2023-07-28T08:42:53Z,2023-07-28T08:42:53Z,2,3,2,"This PR fixes #2350. Verified with 2 approaches:
Approach 1
Run make state-load-sate then run ./save-load-state --model /home/randxie/weights/llama-2-70b-chat.ggmlv3.q4_K_M.bin --gqa 8.
Approach 2
Run the reproducing steps in the above issue, and verify no more Segmentation fault.
The model weight is downloaded from https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML/blob/main/llama-2-70b-chat.ggmlv3.q4_K_M.bin",2,0
2439,2023-07-28T22:20:48Z,2023-08-28T19:51:47Z,2023-08-28T19:51:47Z,11,1890,2458,"This PR improves the memory requirements of training by removing unnecessary tensors from adam optimizer and with a (manual) gradient checkpointing training function, which reduces memory overhead from O(n_layer) to O(sqrt(n_layer)) as explained in readme of https://github.com/cybertronai/gradient-checkpointing
Other changes:


Adam(W) optimizer now supports gradient clipping which improves training convergence


optimizers can now be passed a callback, which is called before each optimization iteration. This can be used to set a learning rate based on custom learning schedule and change sample batches between each iteration, which improves training convergence and run time by avoiding the overhead of restarting the optimization loop to update learning rate and sample batches


fixed some issues in cross entropy loss and improved numerical stability of backward pass with a simplified computation (as in other frameworks), assuming target probability vector sums to one for each batch


cross entropy loss now returns the mean loss over all batches, which helps keeping the gradients in a sane range and decouples the gradients from the batch size


change AdamW decay parameter to work like the torch AdamW decay param, and change default AdamW weight decay parameter defined in ggml to 0.0, making Adam default instead of AdamW


add conditional compilation for F16 exp in flash attention and cross entropy loss, improving gradient quality


ggml : update ggml_rms_norm_back with configurable eps


ggml : add function ggml_build_backward_expand to avoid stack overflows with large maximum number of nodes: GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);


expose more optimizer parameters as training parameters


change sampling parameters for prediction after training to defaults of common.h and clarify what is context for prediction and what are generated tokens",5,23
2448,2023-07-29T19:32:08Z,2023-07-29T21:04:11Z,2023-07-29T21:04:11Z,1,4,5,"This PR replaces the memcpy in a loop for the synchronization between GPUs with a single memcpy2D call. For prompt processing this is much faster:



GPU
Model
Test
t/s 1x P40
t/s master
t/s PR
Speedup




3x P40
7b q4_0
tg128
47.41
43.45
43.53
1


3x P40
13b q4_0
tg128
26.30
29.83
29.89
1


3x P40
33b q4_0
tg128
11.51
15.37
15.38
1


3x P40
7b q4_0
pp
454.89
122.02
346.80
2.84


3x P40
13b q4_0
pp
258.17
85.05
206.76
2.43


3x P40
33b q4_0
pp
104.97
46.42
99.17
2.14



For small models multiple fast GPUs seem to still be slower than a single fast GPU due to the synchronization overhead.",2,0
2449,2023-07-29T20:11:45Z,2023-08-16T20:09:49Z,2023-08-16T20:09:49Z,1,10,0,"Currently if cmake is used with LLAMA_METAL on macOS, make install would not copy ggml-metal.metal to the $PREDFIX/bin directory. This PR fixes that.",2,1
2451,2023-07-29T21:06:12Z,2023-08-02T08:06:20Z,2023-08-02T08:06:20Z,7,40,37,A warning-free 🦙 is a happy 🦙.,6,0
2453,2023-07-30T10:43:30Z,2023-07-31T13:44:35Z,2023-07-31T13:44:35Z,10,67,27,"This PR aims to fix the following:

The new quantized matrix matrix multiplication kernels decide whether or not to unroll a loop based on compute capability but with cmake this check is not functional by default. This PR adds PTX code for compute capability 7.0.
For GPUs with compute capability < 6.1 the new quantized matrix matrix multiplication kernels cannot be used due to a lack of the __dp4a intrinsic. This PR adds a corresponding check.
For the perplexity binary cuBLAS should always be used as suggested by #2160 (comment) . I tried to add a corresponding check to cmake but adding the define at that position does nothing. Is it actually possible to do this with cmake?",5,18
2455,2023-07-30T16:28:15Z,2023-07-31T09:02:53Z,2023-07-31T09:02:53Z,1,10,7,"Additionally, fixes embeddings output with Metal, which I believe was also broken. Untested.
Fixes #2456",6,3
2458,2023-07-30T20:32:13Z,2023-07-31T11:18:52Z,2023-07-31T11:18:52Z,1,484,124,"This PR replaces the simple code for loading data into shared memory tiles for mul_mat_q kernels with mode complicated code that reduces memory bank conflicts as much as possible. The exception is q4_0 and q8_0 where the performance was slightly worse (I'm keeping the corresponding code commented out because it will probably be useful when employing asynchronous data loading). These are the performance numbers on my system:



GPU
Model
Test
t/s master
t/s PR
Speedup




RTX 3090
7b q4_0
pp
1329
1372
1.03


RTX 3090
7b q4_1
pp
1275
1261
0.99


RTX 3090
7b q5_0
pp
973
978
1.01


RTX 3090
7b q5_1
pp
897
956
1.07


RTX 3090
7b q8_0
pp
1068
1092
1.02


RTX 3090
7b q2_k
pp
642
680
1.06


RTX 3090
7b q3_k_s
pp
473
537
1.14


RTX 3090
7b q4_k_s
pp
835
870
1.04


RTX 3090
7b q5_k_s
pp
516
534
1.03


RTX 3090
7b q6_k
pp
586
649
1.11


P40
7b q4_0
pp
624
656
1.05


P40
7b q4_1
pp
360
436
1.21


P40
7b q5_0
pp
246
386
1.57


P40
7b q5_1
pp
254
365
1.44


P40
7b q8_0
pp
529
541
1.02


P40
7b q2_k
pp
209
236
1.13


P40
7b q3_k_s
pp
120
201
1.68


P40
7b q4_k_s
pp
228
238
1.04


P40
7b q5_k_s
pp
200
206
1.03


P40
7b q6_k
pp
161
236
1.47



For reference: cuBLAS performance is ~1300 t/s on an RTX 3090 and ~460 t/s on a P40.",4,7
2459,2023-07-30T22:04:16Z,2023-08-01T07:43:12Z,2023-08-01T07:43:12Z,2,21,17,"ref #2429 #2276
Hi all!
I worked out a quick fix to get llama-2-70b working with metal.
The fix is rather inelegant:

I've simply added an extra metal kernel in ggml-metal.metal to cover for the gqa=8 case by explicitly dividing the index on the 3rd axis of src0 by 8 (similar to what is done in the cpu implementation).
In ggml-metal.m, whenever a matmul appears with ne02 != ne12, I apply the new kernel.
Also in ggml-metal.m, I edited the MPS offsets to account for possible mismatches in the 3rd axis. This never seems to be run - however - so it could be probably disregarded for now.

I've tried with q4_K_M and q5_K_M quantized models from theBloke and generations seem coherent, q8_0 fails because there is no GGML_OP_GET_ROWS kernel for GGML_TYPE_Q8_0.
A better solution would probably require all matmul metal kernels to also take gqa as input, but I didn't want to alter the codebase too much, so I opted for a quicker patch instead.
Note that I rebased against the last commit (a113689) to submit this PR, but I can no longer compile with LLAMA_METAL=1 after that. Instead, my fix works applied the penultimate commit as of now (11f3ca0).",4,7
2466,2023-07-31T10:20:02Z,2023-09-04T08:28:55Z,2023-09-04T08:28:55Z,3,2045,1943,"This PR adds a subtle loading animation to the text input box to make it a bit clearer that something's happening:

  
    
    

    llamaload.mp4",4,6
2468,2023-07-31T11:04:22Z,2023-07-31T12:32:30Z,2023-07-31T12:32:30Z,1,15,8,"This PR adds support for flattening tensors for the non-glm CUDA RoPE implementation. This reduces the associated kernel launch overhead by a factor of 512 for prompt processing. There is no difference for token generation:



GPU
Model
Test
t/s master
t/s PR
Speedup




RTX 3090
7b q4_0
pp
1347
1550
1.15


RTX 3090
13b q4_0
pp
773
851
1.1


RTX 3090
33b q4_0
pp
325
344
1.06


RTX 3090
7b q4_0
tg128
131.66
130.78
0.99


RTX 3090
13b q4_0
tg128
73.50
73.14
1


RTX 3090
33b q4_0
tg128
33.25
33.10
1


P40
7b q4_0
pp
624
665
1.07


P40
13b q4_0
pp
348
364
1.05


P40
33b q4_0
pp
148
150
1.01


P40
7b q4_0
tg128
50.54
50.62
1


P40
13b q4_0
tg128
27.89
27.84
1


P40
33b q4_0
tg128
12.08
12.07
1",2,1
2470,2023-07-31T15:31:20Z,2023-09-17T14:37:54Z,2023-09-17T14:37:54Z,4,63,10,"This PR enables peer access between CUDA devices if possible. As a consequence devices can communicate directly via PCIe instead of using the CPU as an intermediary. This makes token generation faster:



GPU
Model
Test
t/s 1x P40
t/s master
t/s PR
Speedup




3x P40
7b q4_0
tg128
50.29
44.48
56.23
1.26


3x P40
13b q4_0
tg128
27.86
30.32
37.46
1.24


3x P40
33b q4_0
tg128
12.11
15.85
18.54
1.17


3x P40
70b q6_K
tg128
-
7.36
8.15
1.11


3x P40
7b q4_0
pp
703.32
384.37
365.01
0.95


3x P40
13b q4_0
pp
384.52
243.87
224.95
0.92


3x P40
33b q4_0
pp
158.15
121.24
111.78
0.92


3x P40
70b q6_K
pp
-
58.15
54.86
0.94



However, for some reason that I don't yet understand it also makes prompt processing slightly slower. Peer access makes memory allocation slower but I don't think that is the cause. In any case, I think even with the decrease in prompt processing speed this would be a net positive.",6,39
2471,2023-07-31T16:42:55Z,2023-07-31T17:52:22Z,2023-07-31T17:52:22Z,1,3,3,As pointed out by #2453 (comment) it seems I forgot to adjust the name of one of the options in cmake so this PR fixes that.,3,1
2475,2023-08-01T00:15:57Z,2023-08-02T06:18:31Z,2023-08-02T06:18:31Z,1,1,1,"What does this PR do?
This PR adds Chinese LLaMA-2 / Alpaca-2 into the supported models in README.md.
Background
We have launched a new project - Chinese-LLaMA-Alpaca-2, which is a sibling project to our Chinese-LLaMA-Alpaca project. We open-sourced the Chinese-LLaMA-2-7B and Chinese-Alpaca-2-7B, and they work seemlessly with llama.cpp similar to our previous models. It would be nice to add these new models to the supported models of llama.cpp.
Dedicated wiki pages for using our models with llama.cpp are also given:

EN wiki: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_en
ZH wiki: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh",3,0
2476,2023-08-01T08:55:29Z,,2023-08-03T00:13:25Z,1,1,1,"Exceedingly minor change.
The warning bothered my during compilation.",3,2
2483,2023-08-01T21:53:18Z,2023-08-02T16:04:04Z,2023-08-02T16:04:04Z,1,368,212,"This PR adds CUDA performance optimization for non k-quant (mul_mat_q) kernels. The changes are mostly two things:

I took over the approach that @ikawrakow used for q4_K and q5_K where he processed multiple integers per CUDA thread. For token generation 2 ints are processed per thread while for prompt processing 4/8 are processed per thread.
q5 formats are now immediately converted to q8 when being loaded to shared memory. This reduces the amount of computation needed to assemble q5 from the lower and upper bits by a factor of 32.

These are the results on my system:



GPU
Model
Test
t/s master
t/s PR
Speedup




RTX 3090
7b q4_0
pp
1574
1993
1.27


RTX 3090
7b q4_1
pp
1392
1848
1.33


RTX 3090
7b q5_0
pp
1094
1536
1.40


RTX 3090
7b q5_1
pp
1062
1580
1.49


RTX 3090
7b q8_0
pp
1206
1553
1.29


RTX 3090
7b q4_0
tg128
130.26
132.46
1.02


RTX 3090
7b q4_1
tg128
123.48
124.55
1.01


RTX 3090
7b q5_0
tg128
112.62
113.97
1.01


RTX 3090
7b q5_1
tg128
107.79
107.66
1.00


RTX 3090
7b q8_0
tg128
82.48
83.54
1.01


P40
7b q4_0
pp
703
826
1.17


P40
7b q4_1
pp
455
757
1.66


P40
7b q5_0
pp
401
681
1.70


P40
7b q5_1
pp
378
738
1.95


P40
7b q8_0
pp
559
724
1.30


P40
7b q4_0
tg128
50.45
54.09
1.07


P40
7b q4_1
tg128
49.92
51.81
1.04


P40
7b q5_0
tg128
43.36
46.78
1.08


P40
7b q5_1
tg128
45.72
47.10
1.03


P40
7b q8_0
tg128
32.38
33.85
1.05



For reference: the speed of cuBLAS is ~1500 t/s on my RTX 3090 and ~500 t/s on my P40. So for non k-quants the mul_mat_q kernels now seem to be universally faster than cuBLAS.",6,10
2486,2023-08-02T04:15:37Z,2023-08-17T23:34:02Z,2023-08-17T23:34:02Z,2,1512,918,"I found myself using the server example a lot, but i has a major problem, namely, the default prompt template has problems with certain models.
So I added a functionality of saving  and restoring the template automatically to/from the Local Storage, so I don't have to copy-paste my template each time :)
Visually, there is just a button on top of the settings, that lets you reset everything back to default.
I made it so it should be possible to have multiple templates, but I stopped at just autosaving current one to not change things too much at once.",2,6
2487,2023-08-02T07:00:05Z,2023-08-02T08:21:11Z,2023-08-02T08:21:11Z,1,7,0,"We released Aquila-7B model seriesrelated issue, which based on Chinese and English knowledge.
And also open-sourced them in HuggingFace and FlagAI.
Because of using the BPE tokenizer, our pull request of support BPE tokenizer has been merged.
Could add Aquila-7B models into llama.cpp? Thanks for your review.",4,3
2488,2023-08-02T08:44:22Z,2023-08-04T11:29:53Z,2023-08-04T11:29:53Z,2,86,33,"It seems when saving context data to file, we are allocating the entire buffer upfront, copying it over, then writing to file.
This seems unnecessary.
I've adjusted the code to write data to file directly without allocating a copy of the context in memory.
Tested to work when saving/loading session prompts.
I believe this is an important update so that llama.cpp can be better used in devices with memory constraints such as consumer mobile phones.",2,7
2489,2023-08-02T08:58:12Z,2023-08-25T10:32:45Z,2023-08-25T10:32:45Z,3,1963,1144,"#2423
This is a simple implementation for probabilities of llama response.
It renders a popover for each token. The popover is based on preact-portal, it's short so I make some modifications and copy that into index.html.
Dark mode:

Light mode:

For bytes, I just add a bottom border line to split them: (https://github.com/ggerganov/llama.cpp/assets/3001525/ad92444e-58cc-445a-b8a9-44704236e285)
(Screenshots updated after 04b6f2c)
We can set More options -> Show Probabilities to use n_probs param.",7,41
2491,2023-08-02T15:17:52Z,2023-08-06T06:34:05Z,2023-08-06T06:34:05Z,1,7,0,"convert.py fails to run when converting gpt4all-lora-quantized.bin.
In the class Tensor, permute_part() and part() are declared @abstractmethod, but they are not defined in the sub-class GGMLQuantizedTensor. As a result, convert.py aborts with a message TypeError: Can't instantiate abstract class GGMLQuantizedTensor with abstract methods part, permute_part. My environment is Python 3.10.7 on Ubuntu 22.10 on WSL2.
I added the implementations of permute_part() and part() in GGMLQuantizedTensor, and the conversion works fine now.
Before applying this patch:
$ python3 convert.py models/gpt4all-7B/gpt4all-lora-quantized.bin
Loading model file models/gpt4all-7B/gpt4all-lora-quantized.bin
vocabtype: spm
Loading vocab file models/tokenizer.model
params: n_vocab:32001 n_embd:4096 n_mult:256 n_head:32 n_layer:32
Writing vocab...
Traceback (most recent call last):
  File ""/home/tabata/src/llama.cpp/convert.py"", line 1319, in <module>
    main()
  File ""/home/tabata/src/llama.cpp/convert.py"", line 1314, in main
    OutputFile.write_all(outfile, params, output_type, model, vocab)
  File ""/home/tabata/src/llama.cpp/convert.py"", line 1108, in write_all
    for i, ((name, lazy_tensor), ndarray) in enumerate(zip(model.items(), ndarrays)):
  File ""/home/tabata/src/llama.cpp/convert.py"", line 1031, in bounded_parallel_map
    result = futures.pop(0).result()
  File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 451, in result
    return self.__get_result()
  File ""/usr/lib/python3.10/concurrent/futures/_base.py"", line 403, in __get_result
    raise self._exception
  File ""/usr/lib/python3.10/concurrent/futures/thread.py"", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/home/tabata/src/llama.cpp/convert.py"", line 1105, in do_item
    return lazy_tensor.load().to_ggml().ndarray
  File ""/home/tabata/src/llama.cpp/convert.py"", line 614, in load
    ret = self._load()
  File ""/home/tabata/src/llama.cpp/convert.py"", line 622, in load
    return self.load().astype(data_type)
  File ""/home/tabata/src/llama.cpp/convert.py"", line 614, in load
    ret = self._load()
  File ""/home/tabata/src/llama.cpp/convert.py"", line 985, in load
    return GGMLQuantizedTensor(ndarray, shape, data_type)
TypeError: Can't instantiate abstract class GGMLQuantizedTensor with abstract methods part, permute_part

After applying this patch:
$ python3 convert.py models/gpt4all-7B/gpt4all-lora-quantized.bin
Loading model file models/gpt4all-7B/gpt4all-lora-quantized.bin
vocabtype: spm
Loading vocab file models/tokenizer.model
params: n_vocab:32001 n_embd:4096 n_mult:256 n_head:32 n_layer:32
Writing vocab...
[  1/291] Writing tensor tok_embeddings.weight                  | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')
[  3/291] Writing tensor output.weight                          | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')
[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')
[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')
[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)
[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')
Wrote models/gpt4all-7B/ggml-model-q4_0.bin

I followed @wtarreau 's advice and rewrote my PR message. I'm very grateful to him for helping me grow. Thank you for your hard work, maintainers and contributors!",3,5
2495,2023-08-02T21:48:27Z,2023-08-08T11:44:48Z,2023-08-08T11:44:48Z,2,132,23,"With the encouragement of @whoreson in  #2327, I'm formally submitting my more robust Vim plugin.
For ease of reference, it adds

Asynchronous execution
Response streaming
Support for autoloading
User configuration options (including autocommand support)
Utility methods for tokenizing text",3,3
2499,2023-08-03T10:41:08Z,2023-08-04T10:07:21Z,2023-08-04T10:07:21Z,4,7,7,,2,0
2505,2023-08-03T19:06:53Z,2023-08-04T15:34:33Z,2023-08-04T15:34:33Z,1,1,1,Fixes #2503,2,0
2506,2023-08-03T20:49:02Z,2023-08-04T15:35:22Z,2023-08-04T15:35:22Z,1,2,1,"These are my currently installed GPUs:
ggml_init_cublas: found 2 CUDA devices:
  Device 0: Tesla P40, compute capability 6.1
  Device 1: NVIDIA GeForce GTX 970, compute capability 5.2

I found that if I disabled my GTX 970 using --tensor-split 1,0, I still couldn't use the full compute capability of my Telsa P40. With this change, I am able to benefit from the --mul-mat-q option without physically removing the GTX 970 from my PC, which my displays are connected to.",2,0
2514,2023-08-04T15:19:07Z,2023-08-07T05:35:54Z,2023-08-07T05:35:54Z,1,77,58,"It can build at least on debug mode on Windows.
Not all targets are built, but adding them is now easier.
Not sure if it is building the objects for every .exe target or not. It seems really slow.",2,1
2515,2023-08-04T16:26:07Z,2023-08-04T19:00:57Z,2023-08-04T19:00:57Z,1,333,280,"This makes the changes to completion.js from #2391 affect the embedded file in the server binary, which takes effect if the 'public' folder is not found.",3,3
