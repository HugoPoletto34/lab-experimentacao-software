number,created_at,merged_at,closed_at,files,additions,deletions,body_text,participants,comments
3,2023-03-28T18:49:03Z,2023-03-29T17:50:27Z,2023-03-29T17:50:27Z,1,4,2,,2,0
96,2023-03-30T15:55:02Z,2023-04-03T21:18:11Z,2023-04-03T21:18:11Z,1,1,1,Conditonal -> Conditional,3,0
119,2023-03-31T01:06:31Z,,2023-04-13T21:40:11Z,1,7,0,Shorten prompts and responses if they are too long to fit within the specified max length. This ensures that the model does not exceed the input length limit during training.,4,2
129,2023-03-31T07:55:44Z,2023-04-06T18:17:32Z,2023-04-06T18:17:33Z,1,4,2,"Added additional OS terminal commands to readme for the unfiltered bin:
M1 Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-m1 -m gpt4all-lora-unfiltered-quantized.bin
Linux: cd chat;./gpt4all-lora-quantized-linux-x86 -m gpt4all-lora-unfiltered-quantized.bin
Windows (PowerShell): cd chat;./gpt4all-lora-quantized-win64.exe -m gpt4all-lora-unfiltered-quantized.bin
Intel Mac/OSX: cd chat;./gpt4all-lora-quantized-OSX-intel -m gpt4all-lora-unfiltered-quantized.bin",4,4
132,2023-03-31T10:22:42Z,,2023-04-03T21:22:26Z,1,12,2,"Resolves 131 by adding instructions to verify file integrity using the sha512sum command, making sure to include checksums for gpt4all-lora-quantized.bin and gpt4all-lora-unfiltered-quantized.bin",3,2
146,2023-03-31T23:16:50Z,2023-04-03T21:16:23Z,2023-04-03T21:16:23Z,1,6,1,,2,1
148,2023-04-01T00:26:25Z,2023-04-03T21:17:52Z,2023-04-03T21:17:52Z,1,1,1,,2,0
161,2023-04-01T15:30:54Z,2023-04-03T21:20:29Z,2023-04-03T21:20:29Z,1,88,0,"The script detects the user's operating system, lists available .bin files and prompts the user to select a .bin file to run. Ensuring a more user-friendly experience.",3,6
166,2023-04-01T19:47:28Z,,2023-04-03T21:15:59Z,1,58,0,"Making a way to either include or use this project an easier way, still working on it a bit",2,2
174,2023-04-02T03:57:02Z,2023-04-03T21:34:23Z,2023-04-03T21:34:23Z,1,1,1,"Fixing the following bug:
  File ""/mnt/storage7T/gpt4all/data.py"", line 73, in load_data
    dataset = dataset.train_test_split(test_size=.05, seed=config[""seed""])
AttributeError: 'DatasetDict' object has no attribute 'train_test_split'
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /mnt/storage7T/gpt4all/train.py:207 in <module>                                                  │
│                                                                                                  │
│   204 │   else:                                                                                  │
│   205 │   │   accelerator = Accelerator()                                                        │
│   206 │                                                                                          │
│ ❱ 207 │   train(accelerator, config=config)                                                      │
│   208                                                                                            │
│                                                                                                  │
│ /mnt/storage7T/gpt4all/train.py:57 in train                                                      │
│                                                                                                  │
│    54 │                                                                                          │
│    55 │                                                                                          │
│    56 │   with accelerator.main_process_first():                                                 │
│ ❱  57 │   │   train_dataloader, val_dataloader = load_data(config, tokenizer)                    │
│    58 │                                                                                          │
│    59 │                                                                                          │
│    60 │   checkpoint = config[""gradient_checkpointing""]                                          │
│                                                                                                  │
│ /mnt/storage7T/gpt4all/data.py:73 in load_data                                                   │
│                                                                                                  │
│    70 │   else:                                                                                  │
│    71 │   │   dataset = load_dataset(dataset_path)                                               │
│    72 │                                                                                          │
│ ❱  73 │   dataset = dataset.train_test_split(test_size=.05, seed=config[""seed""])                 │
│    74 │                                                                                          │
│    75 │   train_dataset, val_dataset = dataset[""train""], dataset[""test""]                         │
│    76                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
AttributeError: 'DatasetDict' object has no attribute 'train_test_split'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.",2,0
175,2023-04-02T04:24:36Z,2023-04-06T18:17:06Z,2023-04-06T18:17:06Z,1,34,32,Type and formatting improvements.,2,0
181,2023-04-02T16:20:23Z,2023-04-03T21:21:51Z,2023-04-03T21:21:51Z,1,4,3,"git submodule configure isn't a valid command. I assume the intended command is init, which can be combined with update using the --init option.",2,0
208,2023-04-04T00:13:49Z,2023-04-06T18:15:57Z,2023-04-06T18:15:57Z,1,2,2,Fixing specific issues with consistency in the formatting not covered by other README PRs.,2,1
210,2023-04-04T04:43:01Z,,2023-05-11T16:34:41Z,2,19,1,"Quickly Demo
$ docker build -t nomic-ai/gpt4all:1.0.0 .
$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all:1.0.0
Multi Arch
$ docker buildx build --platform linux/amd64,linux/arm64 --push -t nomic-ai/gpt4all:1.0.0 .
Mac Intel
$ docker buildx build --platform linux/amd64 --push -t nomic-ai/gpt4all-amd64:1.0.0 .
$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all-amd64:1.0.0 /opt/gpt4all/gpt4all-lora-quantized-OSX-intel -m /opt/gpt4all/gpt4all-lora-quantized.bin
Mac m1
$ docker buildx build --platform linux/arm64 --push -t nomic-ai/gpt4all-arm64:1.0.0 .
$ docker run -it --rm -v ./gpt4all-lora-quantized.bin:/opt/gpt4all nomic-ai/gpt4all-arm64:1.0.0 /opt/gpt4all/gpt4all-lora-quantized-OSX-m1 -m /opt/gpt4all/gpt4all-lora-quantized.bin",5,4
267,2023-04-06T22:12:26Z,2023-04-07T14:50:28Z,2023-04-07T14:50:28Z,1,1,1,Mismatch of MD5 between the -ggml and regular gpt4all-lora-quantized.bin.,3,1
268,2023-04-06T23:59:36Z,2023-04-07T14:50:56Z,2023-04-07T14:50:56Z,1,3,4,,3,2
274,2023-04-07T13:34:57Z,,2023-04-15T16:34:04Z,1,76,60,,2,0
279,2023-04-08T03:57:01Z,,2023-05-11T16:35:09Z,1,1,1,"referring this issue #195 , i wasn't sure if it's RAM or VRAM, so i changed it to avoid confusion",3,1
334,2023-04-13T19:49:32Z,2023-04-13T21:39:19Z,2023-04-13T21:39:19Z,1,1,1,,2,0
370,2023-04-15T18:22:32Z,,2023-04-17T23:59:04Z,10,133,145,"PyCharm gave me a lot of warnings about the current main branch so I fixed many of those (not all).
The vast majority of fixes are auto-fixes.
The inference works perfectly after my changes.",3,2
396,2023-04-19T08:34:05Z,,2023-05-11T16:33:00Z,1,10,10,,5,8
426,2023-04-24T06:31:43Z,,2023-05-11T16:32:07Z,1,5168,0,,4,2
461,2023-04-27T18:42:51Z,,2023-05-11T16:31:58Z,1,11,9,added nice table for edge models,4,1
465,2023-04-28T06:54:47Z,,2023-05-11T16:32:22Z,1,2,1,Fixed setup,3,1
472,2023-04-29T22:07:38Z,2023-05-02T14:15:40Z,2023-05-02T14:15:40Z,1,1,1,README.md typo fix.,2,0
534,2023-05-11T10:48:32Z,2023-05-15T16:45:56Z,2023-05-15T16:45:57Z,11,801,0,"This is a wrap up of what's currently used by https://github.com/go-skynet/LocalAI, I think this is far from perfect, but a good stab for a first implementation, and polish it in a second round. It is WIP yet as I'm still testing it and need to tidy it a bit, feedback is welcome",5,9
537,2023-05-11T15:21:07Z,2023-05-11T16:36:42Z,2023-05-11T16:36:42Z,1,49,79,We uploaded an earlier version of the script that didn't add the right things to the ggml file. This is the correct one to use,2,0
539,2023-05-11T16:33:30Z,2023-05-11T18:18:11Z,2023-05-11T18:18:11Z,4,134,2,"Contributing.md
Pull request template
Some updates to README",3,0
542,2023-05-11T18:30:19Z,2023-05-12T14:21:14Z,2023-05-12T14:21:14Z,2,5,0,"Forgot a few small things:

Rename PR template folder so github will recognize
python bindings readme now includes pip package install.",2,1
543,2023-05-11T19:31:26Z,2023-05-11T20:49:15Z,2023-05-11T20:49:15Z,1,1,0,Adds mpt-7b-instruct to the model list.,3,0
547,2023-05-12T15:19:53Z,2023-05-12T16:27:48Z,2023-05-12T16:27:48Z,1,3,0,,2,0
548,2023-05-12T17:11:20Z,2023-05-12T19:21:56Z,2023-05-12T19:21:56Z,7,76,16,"Python bindings for MPT models

Updated bindings
Updated docs
Updated tests
Updated PyPI package",2,2
549,2023-05-12T18:13:57Z,2023-05-12T21:11:52Z,2023-05-12T21:11:53Z,4,21,0,,2,1
554,2023-05-13T11:17:20Z,2023-05-16T15:36:46Z,2023-05-16T15:36:46Z,3,42,1,"Summary
This pull request adds more general create and destroy functions:

llmodel_model_create: Creates an LLmodel instance from a model file.
llmodel_model_destroy: Destroys an LLmodel instance.

/**
 * Create a LLmodel instance.
 * Recognises correct model type from file at model_path
 * @param model_path A string representing the path to the model file. 
 * @return A pointer to the LLmodel instance.
 */
llmodel_model llmodel_model_create(const char *model_path);

/**
 * Destroy a LLmodel instance.
 * Recognises correct model type using type info
 * @param model a pointer to a LLmodel instance.
 */
void llmodel_model_destroy(llmodel_model model);
Details
The llmodel_model_create function recognises the correct model type using uint32_t magic; from model_path and creates llmodel_xxxx_create pointers accordingly.
The destroy function gets the modelTypeInfo == typeid() from the llmodel_model and calls llmodel_xxxx_destroy functions accordingly.
Why it would be helpful?
This way the user does not have to implement similar checks in their applications. Especially when new model types are added.",3,0
566,2023-05-13T23:21:32Z,2023-05-16T20:47:54Z,2023-05-16T20:47:54Z,2,163,5,"This PR implements a small CLI chat repl. There are small changes made to support this inside of the pyllmodel.py but most logic is contained to a new file.
Please let me know if I should make any changes!

  
    
    

    gpt4allcli.mp4",3,3
588,2023-05-15T20:46:08Z,2023-05-16T15:33:59Z,2023-05-16T15:33:59Z,7,29,6,"Describe your changes
Making gpt4all typo free from now and there on
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
589,2023-05-16T00:57:52Z,2023-05-16T14:30:19Z,2023-05-16T14:30:19Z,4,72,233,"Removes non-threadsafe use of static inference buffer in mpt code and actually uses model struct buffer
Deduplicates tokenizing and sampling code in gptj and mpt models",3,3
602,2023-05-16T18:10:26Z,2023-05-22T19:55:22Z,2023-05-22T19:55:22Z,14,757,1,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",4,6
611,2023-05-17T06:26:19Z,2023-05-17T11:54:10Z,2023-05-17T11:54:10Z,3,9,6,Caught with AddressSanitizer running a basic prompt test against llmodel standalone. This fix allows ASan builds to complete a simple prompt without illegal accesses but there are still notably several leaks.,2,0
615,2023-05-17T12:43:21Z,2023-05-17T13:56:21Z,2023-05-17T13:56:21Z,1,3,3,"Describe your changes
Fix typo in README
Issue ticket number and link
NA
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

NA
Steps to Reproduce

NA
Notes

NA",2,0
633,2023-05-18T20:26:16Z,2023-05-19T12:54:54Z,2023-05-19T12:54:54Z,32,1110,294,"We've discussed it all on Discord.
Basically, this change:

Improves:

API


Adds:

Support for dynamically loading model implementations
Ability to build stuff like avxonly, cuda, opencl, ... in the same build

allows developer/user to select those dynamically





Each model implementation may use a different fork of ggml, name collisions are taken care of.",5,1
636,2023-05-18T22:36:44Z,2023-05-19T01:40:18Z,2023-05-19T01:40:18Z,1,1,1,"install gpt4all, not gpt4all-chat in qt set up example.",2,0
639,2023-05-19T01:26:43Z,,2023-05-19T14:20:37Z,1,21,10,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,5
642,2023-05-19T13:30:21Z,2023-05-21T14:49:42Z,2023-05-21T14:49:42Z,10,114,39,"Describe your changes
This change introduces full compatibility with new ggml quanitzation without killing the old one (which is renamed to {llama,ggml}-old).
The API is be kept unchanged and these changes completely invisible to it.
Issue ticket number and link
Every single one that complains about new llama models not working :-)",5,10
644,2023-05-19T20:55:09Z,2023-05-20T20:36:30Z,2023-05-20T20:36:30Z,1,2,2,"Describe your changes
When using the golang bindings, cgo includes in gpt4all.go are relative, this makes any use of the bindings outside of the gpt4all repository fail.
This PR adds $(SRCDIR) to the includes so they are absolute.
Issue ticket number and link
This fixes issue: #614
Checklist before requesting a review

[x ] I have performed a self-review of my code.
[ x] If it is a core feature, I have added thorough tests.
[ x] I have added thorough documentation for my code.
[x ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
[X ] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Steps to Reproduce
Include the bindings in a new golang project.
Try to build:
$ make main 
        C_INCLUDE_PATH=/Users/XXX/code/src/github.com/XXX/intelligens/include \
        LIBRARY_PATH=/Users/XXX/code/src/github.com/XXX/intelligens/lib \
        go build
# github.com/nomic-ai/gpt4all/gpt4all-bindings/golang
binding.cpp:1:10: fatal error: '../../gpt4all-backend/llmodel_c.h' file not found
make: *** [main] Error 1",2,0
650,2023-05-20T18:22:04Z,2023-05-22T19:56:49Z,2023-05-22T19:56:49Z,33,1949,0,"Describe your changes
First working version of the C# binding.
Issue ticket number and link
#649
Notes

Tested the build on Windows and Linux(Ubuntu).

OSX support can be easily added but I do not own a Mac to test it.",4,7
652,2023-05-20T23:26:38Z,,2023-07-12T06:01:23Z,2,13,12,"Describe your changes
I moved the ### Prompt:  message to all user prompts and made newline/space usage consistent.
Issue ticket number and link
#653
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
This is an example of the outputs I was getting from ggm-gpt4all-j-v1.3-groovy before the changes: (output from verbose)
### Instruction:
            The prompt below is a question to answer, a task to complete, or a conversation
            to respond to; decide which and write an appropriate response.

### Prompt:
what is the mass of aluminium? is it strong?
### Response:  The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered strong.
it is used in
### Response:
  The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered strong.It's used in a variety of applications, such as construction materials and alloys.

After the formatting changes I got these responses:
### Instruction:
            The prompt below is a question to answer, a task to complete, or a conversation
            to respond to; decide which and write an appropriate response.

### Prompt:
what is the mass of aluminium? is it strong?
### Response:
 The mass of aluminium is approximately 2.8 x 10^-3 kilograms, and it is considered a strong metal.
### Prompt:
it is used in
### Response:
 Yes, aluminum is used in many applications such as construction materials, cookware, and even in the production of aluminum cans.

By prepending Prompt:  to user prompts, it seems to be must less likely to interpret the prompts as parts of its last response. It also repeats less of previous responses in some cases.",6,10
659,2023-05-21T11:21:53Z,,2023-05-22T21:23:06Z,1,5,1,"Describe your changes
An update to the FAQ documentation regarding the Prompt Templates field in settings.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes
I chose to add an invitation to the current more relevant issue about Prompt Engineering as a way to boost the effort towards new findings.",3,1
661,2023-05-21T13:06:26Z,2023-05-30T16:05:57Z,2023-05-30T16:05:57Z,14,47173,236,"Adds a more complete and correct C++implementation of the tokenizer used in MPT and GPT-J
namely:

proper BPE tokenization - differs from the current implementation by using the merges list from the tokenizer config to combine tokens in the correct order - this noticeably improves output quality as without this many words (such as ""assistant"" as found in MPT-chat's ChatML prompts) are encoded differently than they were when the model was trained
complete additional vocab handling - several tokens are added to the tokenizer after the BPE vocabulary is learned, these have to be handled in a preprocessing step instead of simply added to the tokenizer vocabulary, as otherwise they could get split apart and incorrectly tokenized - this previously existed but only handled ""special"" tokens (like endoftext, and ChatML's <|im_start|> etc), but is necessary for all added tokens - notably MPT adds several lengths of just runs of spaces (""  "", ""    "", ""        ""), presumably to allow more efficient tokenization of code indentation

And the parts that required introducing a dependency on ICU:

Unicode normalization - both MPT and GPT-J expect ""NFC"" (fully composed, accented characters are always represented as a single codepoint where possible) normalization and it is the only type implemented here. This makes sure different ways of representing the same character are always tokenized the same way.
Handling the GPT2 BPE pretokenization in unicode - that is, the regex needs to apply to Unicode codepoints, and word-splitting on ""alphanumeric"" codepoints requires knowing which codepoints represent alphanumeric characters. std::regex is unfortunately incomplete here and will improperly split non-English text, causing incorrect tokenization.
UTF8 encoding and decoding generally - especially for the byte-codepoint encoding which is necessary for proper encoding and decoding of multibyte characters

The ICU dependency also adds some annoyance to the build process - on Ubuntu I just needed to install libicu-dev, on windows it worked after installing MSYS2 and running pacman -S icu-devel, and I don't have a Mac to test with.
I prototyped this at https://github.com/apage43/bpe.cpp which also has a small standalone test
Demo
Before:

  
    
    

    Tok-Before-1.webm
    
  

  

  


After:

  
    
    

    Tok-After-1.webm
    
  

  

  


TODO

 Bake tokenizer configs into the library to sidestep the distribution problem and avoid introducing extra complexity to bindings users
 Use fixed tokenizers for decoding as well as encoding - currently this calls the responseCallback with std::strings that may not be valid utf8",5,22
665,2023-05-21T16:20:08Z,2023-05-22T03:14:18Z,2023-05-22T03:14:18Z,2,42,6,"Describe your changes
improved documentation landing page
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
666,2023-05-21T19:30:25Z,2023-05-22T13:04:37Z,2023-05-22T13:04:37Z,1,42,4,,3,0
671,2023-05-21T20:27:53Z,2023-05-22T13:04:26Z,2023-05-22T13:04:26Z,2,2,2,,3,0
679,2023-05-22T18:24:40Z,2023-05-22T21:22:06Z,2023-05-22T21:22:06Z,1,20,78,"Describe your changes
Reduced duplicate button code in ChatDrawer.qml, on top of the MyButton and MyComboBox changes
Issue ticket number and link
Discussed on Discord
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,1
684,2023-05-22T21:45:42Z,2023-05-25T15:34:22Z,2023-05-25T15:34:22Z,1,22,21,,3,0
686,2023-05-22T23:19:30Z,2023-05-25T15:34:07Z,2023-05-25T15:34:07Z,5,81,1,"Create a Gpt4All.Tests project to hold unit tests for the C# bindings.
Since the API is rapidly evolving right now, it only tests loading the three different types of models.
Help is welcome.
The plan is to eventually test the whole C# bindings library.",3,0
697,2023-05-23T13:55:47Z,2023-05-23T15:03:32Z,2023-05-23T15:03:32Z,4,24,110,"Describe your changes
Deduplicated button code
Issue ticket number and link
Continuing work and discussed on Discord
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
706,2023-05-24T08:37:25Z,,2023-05-24T19:15:13Z,1,1,1,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
711,2023-05-24T19:16:08Z,2023-05-25T15:28:06Z,2023-05-25T15:28:06Z,25,2210,29,"Describe your changes
Localdocs feature
Checklist before requesting a review

[X ] I have performed a self-review of my code.
 I have added thorough documentation for my code.",9,11
712,2023-05-24T20:04:42Z,2023-05-28T23:57:01Z,2023-05-28T23:57:01Z,5,55,3,"Describe your changes
Introduced the possibility to provide a prompt formatter (IPromptFormatter) to transform the prompt before passing it to the actual model.
A default implementation (DefaultPromptFormatter), based on the template used in the python bindings, is provided.
Issue ticket number and link
#707
Checklist before requesting a review

[x ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
[x ] I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
714,2023-05-24T21:54:33Z,2023-06-01T20:01:28Z,2023-06-01T20:01:28Z,8,122,29,"Describe your changes
Added support to (optionally) integrate with the .NET logging infrastructure, ready to be used with DI.
Checklist before requesting a review

[ x] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
[x ] I have added thorough documentation for my code.
[x ] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Example using Serilog (without DI):
using Gpt4All;
using Serilog;
using Serilog.Core;
using Microsoft.Extensions.Logging;

var logger = new LoggerConfiguration()
    .WriteTo.Console()
    .MinimumLevel.Debug()
    .CreateLogger();

var loggerFactory = new LoggerFactory().AddSerilog(logger);

var modelFactory = new Gpt4AllModelFactory(loggerFactory: loggerFactory);

// ...
[22:51:00 INF] Creating model path=C:\models\ggml-gpt4all-j-v1.3-groovy.bin type=GPTJ
[22:51:00 DBG] Model created handle=0x1762702781760
[22:51:00 INF] Model loading started
[22:51:02 INF] Model loading completed success=True
[22:51:02 INF] Start streaming prediction task
[22:51:02 INF] Prompt input='list 3 colors' ctx=
        {
            logits_size = 0
            tokens_size = 0
            n_past = 0
            n_ctx = 1024
            n_predict = 128
            [...truncated for brevity..]
        }
[22:51:10 INF] Prediction task completed elapsed=7.8376754s",2,2
718,2023-05-25T07:20:34Z,2023-05-25T15:35:48Z,2023-05-25T15:35:48Z,1,14,1,"This is an initial attempt to auto-detect what build variant should be loaded at runtime.

 avxonly detection (on x86_64)
 avxonly detection (on aarch64)
 CUDA detection and implementation",3,3
720,2023-05-25T15:58:00Z,2023-05-26T13:24:04Z,2023-05-26T13:24:04Z,2,84,75,"Describe your changes
Many improvements to the Python bindings:

fix ignoring --threads=4 (popular default, usually doesn't affect the users)
cleanup half-downloaded models
descriptive exceptions
make code simpler and more pythonic.

Issue ticket number and link



Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",4,5
728,2023-05-26T19:32:53Z,2023-05-28T23:56:24Z,2023-05-28T23:56:24Z,1,1,1,"Describe your changes
Noticed that the docs in the training directory are targeting the wrong repository. Small update to fix the indicated repo.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
738,2023-05-27T18:28:54Z,2023-05-28T23:51:11Z,2023-05-28T23:51:11Z,1,2,2,fix golang gpt4all import path,2,0
739,2023-05-27T22:10:35Z,2023-05-28T23:50:45Z,2023-05-28T23:50:45Z,1,1,1,,2,0
744,2023-05-28T12:18:26Z,2023-06-01T01:26:19Z,2023-06-01T01:26:19Z,1,18,8,"This MR attempt to implement detection for avxonly being required on the following platforms: Linux/GCC/x86_64, Windows/GCC/x86_64, Windows/MSVC/x86_64.
I know the Linux implementation works, but I need people to test it under Windows!",4,3
746,2023-05-28T13:08:09Z,,2023-12-29T23:57:30Z,5,87,10,"This PR aims to add support for CUDA and OpenCL. Once ready, I'll need someone to test CUDA support since I don't own an Nvidia card myself.
Testing instructions
Just a warning, old models as downloaded automatically will not work properly with OpenCL. Currently, it makes the GUI freeze, but that's some change on the GUI side that needs to be done. Old llama.cpp simply doesn't support them.
Download a GGML model from here: https://huggingface.co/TheBloke and place it in your models folder. Make sure it starts with ggml-! The GUI might attempt to load another model on the way there and crash, since updating that won't be part of this PR. To prevent this, move the other models somewhere else.
To make the GUI actually use the GPU, you'll need to add either buildVariant = ""cuda""; or buildVariant = ""opencl""; after this line:
https://github.com/tuxifan/gpt4all/blob/dlopen_gpu/gpt4all-backend/llmodel.cpp#L69
We also need some people testing on Windows with AMD graphics cards! And some people on Linux testing on Nvidia.",15,51
754,2023-05-29T14:27:59Z,2023-05-30T20:34:23Z,2023-05-30T20:34:23Z,1,24,8,"Describe your changes
The json response body of the web server's
endpoint ""POST /v1/chat/completions"" is adhering to the openAi api schema
Issue ticket number and link
issue #753
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
not a core feature
 I have added thorough documentation for my code.
The change makes the application behave consistenlty with the current documentation.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
i think i do not have permission to attach labels on the PR. github does not provide me the option to do so
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.
no changes in GUI.

Steps to Reproduce
described in the relevant issue
Notes",3,2
763,2023-05-30T15:35:02Z,2023-06-13T12:05:34Z,2023-06-13T12:05:34Z,21,850,671,"Describe your changes
The current .NET Binding includes build scripts for building and bundling locally built gpt4all libraries with cmake, and only for Windows and Linux x64 builds. I have been working on build scripts to build the gpt4all libraries for many more platforms, including Mac, iOS, and Android, and with some tweaks to this binding, those libraries can all work here. I have a proof of concept of it here, and now I'm trying to bring that code back into this repo.



This PR removes the hardcoded runtime bundle and introduces a library loader system, initially introduced in whisper.net. For libraries that are linked in as part of the build (iOS, Android, etc) the loader returns true. If it accepts dynamically loaded libraries (macOS, Windows, Linux) then you can load your libraries as part of your program by invoking NativeLibraryLoader.LoadNativeLibrary();
CC @mvenditto
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",5,24
774,2023-05-31T08:16:44Z,,2023-06-01T11:59:45Z,7,92,55,"After GitHub has decided to mess up my PR once, and then to just close it for no reason another time, here comes the third attempt.
This is a follow-up to the previous implementation management improvement PR that was reverted due to serious issues in it. I took the second chance to make even more fundamental changes (not API breaking tho!).
Changes:

Fixes the compile error
Allows the API user to have more control and insights into the different implementations
Specially useful for gpt4all-chat
Makes the code (much!) simpler",2,7
783,2023-06-01T09:43:52Z,,2023-06-04T19:29:33Z,12,225,117,"Originally, I only wanted to prompt evaluation progress reporting, however on the way there I noticed the whole callback thing isn't currently implemented particularly well.
I have:

Reworked the prompt() functions to accept a struct of callbacks instead
Made the callbacks return void instead of bool and provide a stop() function
Reimplemented the old prompt() function with the same signature to ""redirect"" to the new one.

Exactly the same behaviour as the original old one
Marked as deprecated


Added a prompt evaluation progress reporting callback

From the API user facing side, this isn't a breaking change. However, from the implementation facing side, it is.
I believe this change is important to make sure future additions of callbacks don't break the API. Also, it decreases code complexity for the API user by allowing that Callback struct to be constructed once and to then be just passed around.
Before, returning false from a callback didn't necessarily cause prompt() to stop. Calling stop() however always does, earlier or later.
Todo:

 Update all calls made from llmodel_c to the old prompt() function to the new one",2,4
785,2023-06-01T12:17:18Z,2023-06-01T14:50:43Z,2023-06-01T14:50:43Z,4,83,0,"This PR implements a simple message handler that:

Keeps only the previous (and ofc current) log file
Logs to stdout on log file creation error",2,0
786,2023-06-01T12:45:25Z,2023-06-01T14:36:22Z,2023-06-01T14:36:22Z,8,32,103,"Describe your changes
Note: you need to have *.so files nearby the binary produced by the binding in order to make this to work
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
$ make example/main
$ mv example/main buildllm
$ cd buildllm
$ ./main -m ~/_git/LocalAI/models/ggml-gpt4all-j -t 4
gptj_model_load: loading model from '/home/mudler/_git/LocalAI/models/ggml-gpt4all-j' - please wait ...
gptj_model_load: n_vocab = 50400
gptj_model_load: n_ctx   = 2048
gptj_model_load: n_embd  = 4096
gptj_model_load: n_head  = 16
gptj_model_load: n_layer = 28
gptj_model_load: n_rot   = 64
gptj_model_load: f16     = 2
gptj_model_load: ggml ctx size = 5401.45 MB
gptj_model_load: kv self size  =  896.00 MB
gptj_model_load: ................................... done
gptj_model_load: model size =  3609.38 MB / num tensors = 285
Model loaded successfully.
>>> what's up?

I'm sorry, as an AI language model, I don't have the capability to answer a greeting or question. Is there anything else

Steps to Reproduce

Notes",3,0
787,2023-06-01T12:47:06Z,2023-06-01T14:51:46Z,2023-06-01T14:51:46Z,2,29,43,This change fixes/explains all current FIXMEs,2,0
789,2023-06-01T14:10:09Z,2023-06-01T15:41:05Z,2023-06-01T15:41:05Z,1,3,1,This makes life easier for bindings devs :-),3,1
792,2023-06-01T15:29:42Z,2023-06-01T18:24:24Z,2023-06-01T18:24:24Z,3,6,5,"Describe your changes
There is one file that was missing a #define NOMINMAX when including the windows.h header. This was causing the msvc build to fail, since because the order of operations of how, I believe, it was being compiled, it would include the min/max headers that would break ""std::min"" and ""std::max""
No, I don't get why that is the case.
Adding the definition fixed it.
While I was here, I updated the build scripts to also pull in the new DLLs being generated.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
793,2023-06-01T15:55:33Z,2023-06-01T17:03:44Z,2023-06-01T17:03:44Z,1,0,1,"Describe your changes
This is a leftover which otherwise breaks compilation outside of the source
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
798,2023-06-01T18:15:52Z,2023-06-01T21:14:17Z,2023-06-01T21:14:17Z,9,90,90,Makes localdocs work with server mode.,2,0
803,2023-06-02T03:16:42Z,2023-06-02T11:20:59Z,2023-06-02T11:20:59Z,9,41,96,… backend model impl.,2,1
805,2023-06-02T05:54:42Z,2023-06-12T18:58:06Z,2023-06-12T18:58:07Z,12,1090,0,"Describe your changes
This is a complete implementation of Java bindings for Gpt4all.
Java developers will be able to load llmodel shared library into Java JVM and make requests to generate text similar to python, c#, go bindings. The shared libraries will have to be present on the machine of the JVM and found via the standard library loading process like setting LD_LIBRARY_PATH. Developers simply have to add a dependency for the Java binding library to their maven or gradle project as described in the README file. The library jar has already been published to maven central so is immediately available to any Java developer. Published to maven central under the PR submitter org id.
Issue ticket number and link
Checklist before requesting a review

[x ] I have performed a self-review of my code.
[x ] If it is a core feature, I have added thorough tests.
[x ] I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes
I plan to use the Java bindings for some internal projects at my company but I think others may be interested. Java is a very common deployment vehicle for microservice at many corporations and there are many developers that are generally familiar with the Java ecosystem as it is one of the top 3 programming languages around.",3,10
807,2023-06-02T10:06:54Z,,2023-06-04T19:29:17Z,3,32,12,"This change adds models that have failed to load to an ignore list and makes ChatLLM::loadDefaultModel iterate over all models found until one has succeeded to load.
It also makes sure to release the modelInfo on load failure in ChatLLM::loadModel.",2,3
808,2023-06-02T12:43:51Z,,2023-06-03T15:26:51Z,9,266,447,This PR unifies the code of the LLModel::prompt() function.,4,1
811,2023-06-02T14:50:28Z,2023-06-02T19:46:41Z,2023-06-02T19:46:41Z,12,28,25,"Nothing more to say 😄
Mostly trivial changes.",3,0
812,2023-06-02T15:15:54Z,2023-06-02T16:32:26Z,2023-06-02T16:32:26Z,6,82,146,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo",3,0
815,2023-06-02T18:41:52Z,2023-06-02T19:46:33Z,2023-06-02T19:46:33Z,1,1,1,,3,0
821,2023-06-03T10:42:54Z,2023-06-03T11:51:18Z,2023-06-03T11:51:18Z,1,4,0,"Describe your changes
Repeating the change that once was done in #663 but then was overriden by 48275d0",2,0
822,2023-06-03T15:27:31Z,2023-06-04T12:59:25Z,2023-06-04T12:59:25Z,10,281,452,Fixed up version of the tuxifan backend_prompt_dedup,2,0
827,2023-06-04T02:36:45Z,2023-06-04T12:46:38Z,2023-06-04T12:46:38Z,1,2,2,"Describe your changes
huggingface -> Hugging Face",2,0
836,2023-06-04T22:00:07Z,2023-06-05T19:35:40Z,2023-06-05T19:35:40Z,1,1,1,"Describe your changes
Otherwise threads count doesn't take effect
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
839,2023-06-05T01:32:57Z,2023-06-28T18:28:52Z,2023-06-28T18:28:52Z,21,603,2,"Describe your changes
Scaffolding for the GPT4All API that runs in a docker container with the OpenAI OpenAPI specification found here:
This should follow https://github.com/openai/openai-openapi/blob/master/openapi.yaml
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
See README.md of gpt4all-api
https://localhost:4891/docs

Notes
Intended to fully match the OpenAI OpenAI spec for engines, completions, chat completions and embeddings.
This can eventually support GPU inference by side-caring a triton inference server.",5,7
840,2023-06-05T01:44:47Z,2023-06-09T12:51:10Z,2023-06-09T12:51:10Z,1,4,4,"the defaults that we have right now aren't ideal, i tried many models(nous-vicuna, snoozy, groovy....) with the values given here: https://www.reddit.com/r/LocalLLaMA/wiki/index/#wiki_prompting , i used the ""Precise"" preset, and all of the models gave much better results.
Amount of times the models got stuck in a loop writing the same thing over and over significantly decreased using the new values, that allowed to make them write more longer responses, and follow the conversation for longer, without going schitzo.
And i believe the newcomers will appreciate not having to figure out why the models are constantly getting stuck in loops whenever the conversation gets a little long.
Also changed the batch size from 9 to 128, 9 is too conservative imo.",3,2
860,2023-06-05T19:26:27Z,,2024-03-06T18:57:47Z,1,73,59,"This commit includes updates to the GPT4All Python API for handling both GPT4All models and OpenAI chat models.
Key changes include:


Modified the GPT4All constructor to recognize if an OpenAI API key is provided. If a key is present, the code will use OpenAI's models instead of downloading a model locally.


The generate function is updated to accept a list of messages compatible with OpenAI's chat models when the OpenAI API key is set.


Adjusted the chat_completion method to work with the updated generate function. It no longer rebuilds the prompt twice, and it correctly handles the return structure from OpenAI's ChatCompletion.


Enhanced error handling and added extensive docstrings for better code clarity and understanding.


With these updates, users of GPT4All API can now seamlessly switch between using GPT4All models and OpenAI chat models, providing more flexibility and options for their AI applications.
Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",4,2
862,2023-06-05T19:45:06Z,2023-06-12T16:41:22Z,2023-06-12T16:41:22Z,2,12,10,There seems to be some memory cleanup missing from the bindings code.,4,3
867,2023-06-05T23:54:22Z,,2023-07-28T15:31:43Z,1,7,2,"Describe your changes
I combined the bytes of the emojis to be encoded accurately.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.

Demo
Before:

After:",4,10
868,2023-06-06T00:47:30Z,,2023-06-26T17:50:32Z,3,50,2,"My 2nd attempt at creating the ability to switch between 'LocalHost' and 'Any' and having the ability of switching ports from 4891
Hopefully I've done it in a way where it's default is still LocalHost, 4891. I'm still very new to this... I won't be offened by any feedback. Hopefully I added Qsettings in the right place as suggested on my last pull request.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,3
879,2023-06-06T20:07:24Z,,2023-06-10T14:07:42Z,5,64,5,"Describe your changes
As mentioned in #864 and #647 (comment) Markdown rendering can make some content invisible.
To address that, I've added a setting to toggle the state of Markdown rendering.
Note: It's been years since I've touched C++ and Qt. Although the changes didn't seem very complicated, there's no guarantee I got everything right.
Issue ticket number and link
#864
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Here is what it looks like with the fixes:

  
    
    

    markdown-toggle-h264.mp4
    
  

  

  


Steps to Reproduce
Simply try entering something like <this could be interpreted as HTML> as prompt, or get the model to respond with something like that to trigger the issue.
Notes
Created as a draft because it's adding a setting. There might be alternative ways to address it.",2,7
885,2023-06-07T02:34:00Z,2023-06-09T18:58:12Z,2023-06-09T18:58:12Z,7,141,67,"todos:

 Currently need to ask for a Metal build with cmake -DLLAMA_METAL=ON - this should probably be just the default on MacOS/arm, and probably off for MacOS/Intel where I don't think it's supported (Metal API is supported, but I believe llama.cpp currently expects unified memory)

built by default on mac now


 Metal currently only works with q4_0, any other files will fail to load on a Metal build - need to somehow, ugh, detect beforehand if the file is anything other than q4_0 to pass n_gpu_layers=0 to disable Metal

detected and now falling back to default impl


 ggml-metal.metal needs to be included in Contents/Resources in the  app bundle and the documented way to do that in CMake isn't doing the trick, but it works after manually copying it in (the code in llama.cpp looks for it usingpathForResource:)

 figured this out, also needed to be a source file on the relevant target - still need to figure out how to distribute it for the bindings though - technically it doesn't need to be a file, it's passed to the relevant metal API  just as NSString, so could be embedded in the binary, but right now the upstream code is hardcoded to search for it with pathForResource - changing this behavior would mean forking llama.cpp.. again.



this only curently does anything for llama ggmlv3 models. Metal (and ggml GPU offload generally) requires some additional support on the model code side to move stuff in and out of GPU memory (well, Metal on M1/M2 does unified memory so its simpler than say, CUDA), and in addition to the current q4_0 quant limitation, only a subset (the subset needed to run llama) of the ggml ops are currently implemented in Metal. Notably missing for supporting our other models are

GELU activation (most models use this, but llama uses swish)
ALiBi positional bias (MPT, Replit)
regular old layernorm (llama uses RMSNorm)",3,1
887,2023-06-07T07:58:48Z,2023-06-07T16:18:22Z,2023-06-07T16:18:22Z,2,149,79,"This change syncs llama.cpp.cmake with upstream llama.cpp CMakeLists.txt.
It makes room for:

metal support
CUDA/OpenCL support

Although both are untested.

Will simplify #746; important changes picked up from there
Will lay some groundwork for #885

And fixes avxonly builds still using AVX2.",4,6
891,2023-06-07T12:57:26Z,,2023-06-26T17:47:28Z,1,7,0,"Describe your changes
Released over 2 weeks ago, Manticore 13B Chat (as opposed to the original Manticore 13B without chat) is a SOTA model tuned on instruct, instruct augmented (datasets similar to Microsoft's Orca), and chat (sharegpt, pygmalion) datasets.
Manticore 13B Chat has top marks on ARC-C, ARC-E, PIQA, and Winogrande. (overall average score of 69.53)
eleutherai benchmarks:
hf-causal-experimental (pretrained=openaccess-ai-collective/manticore-13b-chat-pyg),
limit: None, provide_description: False, num_fewshot: 0, batch_size: None



Task
Version
Metric
Value

Stderr




arc_challenge
0
acc
0.4898
_
0.0146




acc_norm
0.5128
_
0.0146


arc_easy
0
acc
0.7803
_
0.0085




acc_norm
0.7584
_
0.0088


boolq
1
acc
0.8012
_
0.0070


hellaswag
0
acc
0.6122
_
0.0049




acc_norm
0.7978
_
0.0040


openbookqa
0
acc
0.3580
_
0.0215




acc_norm
0.4600
_
0.0223


piqa
0
acc
0.7998
_
0.0093




acc_norm
0.8090
_
0.0092


winogrande
0
acc
0.7285
_
0.0125



Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,6
892,2023-06-07T13:19:12Z,,2023-06-07T17:18:43Z,4,200,0,This PR adds some test suites targeting the llmodel C and C++ interfaces using google test.,2,6
895,2023-06-07T17:20:31Z,2023-06-09T14:17:44Z,2023-06-09T14:17:44Z,3,130,23,"Describe your changes
Method to return a generator for model response.
How it works: we kick off the C library prompt call in a separate thread so we don't block return of generator. The prompt response callback puts tokens into queue instead of printing them. Generator then dequeues and yields tokens as they are added.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code. (I manually made sure all other model prompt methods are working)
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo",4,5
900,2023-06-08T03:43:23Z,2023-06-08T18:08:10Z,2023-06-08T18:08:10Z,1,1,1,"no effect, but avoids a potential bug later if we use
actualVocabSize - which is for when a model has a larger
embedding tensor/# of output logits than actually trained token
to allow room for adding extras in finetuning - presently all of our
models have had ""placeholder"" tokens in the vocab so this hasn't broken
anything, but if the sizes did differ we want the equivalent of
logits[actualVocabSize:] (the start point is unchanged), not
logits[:-actualVocabSize] (this.)",3,0
901,2023-06-08T03:57:54Z,2023-06-08T18:08:31Z,2023-06-08T18:08:31Z,1,13,0,"copied directly from llama.cpp - without this temp=0.0 will just
scale all the logits to infinity and give bad output
set temp=0, before:

after:",3,1
907,2023-06-08T17:09:50Z,2023-06-09T08:13:35Z,2023-06-09T08:13:35Z,1,3,1,"Have own section for short usage example, as it is not specific to local build
Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
910,2023-06-08T18:04:02Z,2023-06-16T20:06:22Z,2023-06-16T20:06:22Z,1,3,12,"The CLI app.py is currently broken. See discussion starting in this issue here: #820 (comment)
Describe your changes

the bindings API changed in 057b9, but the CLI was not updated
change 'std_passthrough' param to the renamed 'streaming'
remove '_cli_override_response_callback' as it breaks and is no longer needed

Issue ticket number and link
The bug was discovered as part of a separate/wider issue: #820 (comment)
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo


bug traceback see: #820 (comment)


fixed version (Windows cmd):



Steps to Reproduce
try to run, it'll break:
python -m pip install --user -U gpt4all typer  # installs current gpt4all==0.3.0
python gpt4all-bindings/cli/app.py repl
Notes
I did not yet increase the version number, but that would probably be appropriate. Should it follow the PyPI package versioning?",3,5
911,2023-06-08T19:26:21Z,2023-06-09T12:44:47Z,2023-06-09T12:44:47Z,1,8,0,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
913,2023-06-08T20:41:15Z,2023-06-12T19:00:21Z,2023-06-12T19:00:21Z,25,4057,470,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",23,5
919,2023-06-09T08:20:07Z,2023-06-09T12:55:15Z,2023-06-09T12:55:15Z,1,3,4,The anonymous namespace already makes it static.,2,0
924,2023-06-09T14:51:17Z,2023-06-12T15:55:56Z,2023-06-12T15:55:56Z,1,3,3,"A bug introduced in 0cb2b86 breaks AVX on Windows with MSVC
currently getting: warning C5102: ignoring invalid command-line macro definition '/arch:AVX2'

Describe your changes

The solution is to use _options(...) not _definitions(...)

Issue ticket number and link

#923

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Description of the problem is in the accompanying issue: #923
Steps to Reproduce
Build on Windows without the fix produces warnings like:
command line : warning C5102: ignoring invalid command-line macro definition '/arch:AVX2' ...",4,8
926,2023-06-09T15:04:25Z,2023-06-13T03:11:54Z,2023-06-13T03:11:54Z,1,32,25,"Describe your changes
Updated python workflows to follow @manyoso's circleci patterns with hold approvals and easier pypi deployments.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
931,2023-06-09T22:17:21Z,2023-06-13T14:29:15Z,2023-06-13T14:29:15Z,3,102,32,"works now
TODOs:

 figure out why debug build generates garbage output (Edit: not a new issue, also exists on main)

will be fixed by #968


 unimplemented metal kernels for ggml ops

 ggml_norm
 ggml_gelu
 ggml_alibi
 ggml_cpy for f16 src (f16 kv cache)
 PR new metal impls to upstream


 add scratch buffers to save memory use

can't easily dynamically resize buffers when using metal, once a buffer has been ggml_metal_add_buffer'd to the ggml_metal_context it is there for the remaining life of the ggml_metal_context, so scrap the mem_per_token stuff and just lower the memory requirement altogether by using scratch buffers


 fall back to normal implementation for unsupported quant types
 fix for finding ggml-metal.metal next to the .dylib landed upstream, re-sync our fork",2,0
951,2023-06-12T00:00:05Z,2023-06-12T12:08:18Z,2023-06-12T12:08:18Z,1,115,4,,2,0
956,2023-06-12T13:22:55Z,2023-06-12T15:43:21Z,2023-06-12T15:43:21Z,1,10,1,,2,1
960,2023-06-12T20:11:27Z,2023-07-25T15:46:41Z,2023-07-25T15:46:41Z,30,8091,3205,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes

Features:

fixes improper named exports

ESM needs named exports, this addresses fixes so users can use our library in esm and cjs.


Empty strings would occur very often due to an improperly formed prompt with createCompletion thank you @iimez for discovering the bug
adds basic testing with jest
add circle ci integration to build documentation and package
downloading models come with better safety and md5sum checking thank you @iimez
resumable / partial downloads
Completes #913
embeddings api!
documentation",8,4
964,2023-06-13T00:11:56Z,2023-06-13T12:40:38Z,2023-06-13T12:40:38Z,1,1,1,,2,0
967,2023-06-13T01:53:26Z,2023-06-13T07:53:27Z,2023-06-13T07:53:27Z,2,2,2,"Fix typo in javadoc,
Add word to ignore list for codespellrc",2,0
968,2023-06-13T01:57:39Z,2023-06-13T11:14:02Z,2023-06-13T11:14:02Z,11,11,11,"fixes a definite use-after-free and likely avoids some other
potential ones - std::string will convert to a std::string_view
automatically but as soon as the std::string in question goes out of
scope it is already freed and the string_view is pointing at freed
memory - this is mostly fine when its returning a reference to the
tokenizer's internal vocab table but it's, imo, too easy to return a
reference to a dynamically constructed string with this as replit is
doing (and unfortunately needs to do to convert the internal whitespace
replacement symbol back to a space)",2,0
969,2023-06-13T02:49:01Z,2023-06-13T13:07:08Z,2023-06-13T13:07:08Z,2,35,31,"Describe your changes
Was not correctly using prompt context. Now it's preserved in class. Fixes, BOS issue.

Steps to Reproduce

Notes",3,0
976,2023-06-13T15:30:14Z,2023-06-20T14:21:51Z,2023-06-20T14:21:51Z,1,1,1,"Describe your changes
0.3.5 for metal libraries
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
981,2023-06-14T10:02:35Z,2023-06-14T14:27:19Z,2023-06-14T14:27:19Z,3,15,2,"This is used to identify the path where all the various implementations are
Describe your changes
This adds SetLibrarySearchPath as model option to search lib path in the specified directory instead of the current work directory.
Issue ticket number and link
#826
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Before:
11:59AM DBG Loading model in memory from file: /home/mudler/_git/LocalAI/models/ggml-gpt4all-j                                                                                                                       
Debug: New GPT4ALL                                                                                                                                                                                                          
Debug: Library search path: /tmp/localai/backend_data                                                                                                                                                                       
load_gpt4all_model: error 'Success'                                                                                                                                                                                  
11:59AM DBG [gpt4all] Fails: failed loading mode

After:
gptj_model_load: loading model from '/home/mudler/_git/LocalAI/models/ggml-gpt4all-j' - please wait ...                                                                                                              
gptj_model_load: n_vocab = 50400                                                                                                                                                                                     
gptj_model_load: n_ctx   = 2048                                                                                                                                                                                      
gptj_model_load: n_embd  = 4096
gptj_model_load: n_head  = 16
gptj_model_load: n_layer = 28
gptj_model_load: n_rot   = 64
gptj_model_load: f16     = 2
gptj_model_load: ggml ctx size = 5401.45 MB
gptj_model_load: kv self size  =  896.00 MB
gptj_model_load: ................................... done
gptj_model_load: model size =  3609.38 MB / num tensors = 285
12:01PM DBG [gpt4all] Loads OK
12:01PM DBG Response: {""object"":""chat.completion"",""model"":""ggml-gpt4all-j"",""choices"":[{""message"":{""role"":""assistant"",""content"":""I'm doing well, thank you. How about yourself?""}}],""usage"":{""prompt_tokens"":0,""comple
tion_tokens"":0,""total_tokens"":0}}

Steps to Reproduce
This affect only binaries that are not in the same directory within the implementations
Notes
Without this it's impossible to specify a search path to find the various implementations libs.",2,0
983,2023-06-14T12:42:20Z,2023-06-22T07:28:40Z,2023-06-22T07:28:40Z,1,7,2,Fixes #983,4,1
985,2023-06-14T14:14:14Z,2023-06-22T07:29:15Z,2023-06-22T07:29:15Z,1,23,13,To not show every little tiny network spike to the user,2,0
988,2023-06-14T19:40:08Z,2023-06-15T21:06:15Z,2023-06-15T21:06:15Z,1,2,2,"fix loading and running llama model files using the new ""k"" quantizations - the change here makes them work on non-Metal builds now instead of simply failing to load
 they load but still generate garbage output on a Metal build, even though a build of the llama.cpp we're using's main binary can run them on Metal without problems.

it turns out this bug exists upstream if you do what we do and set n_ctx to 2048: ggerganov/llama.cpp#1881",2,3
989,2023-06-15T08:28:40Z,2023-06-26T17:49:58Z,2023-06-26T17:49:58Z,1,4,5,Fixed several warnings and UB in the Replit implementation,3,0
991,2023-06-15T09:33:25Z,2023-06-16T19:56:23Z,2023-06-16T19:56:23Z,4,12,10,"This cleans up the code a very tiny bit by removing the MB variable from each implementation and instead having an operator """"_MiB in utils.h.",2,0
992,2023-06-15T11:52:31Z,,2023-06-27T14:00:38Z,5,55,55,"Describe your changes
DO NOT MERGE. This is for testing only and will not be merged into main
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",4,4
997,2023-06-15T20:29:52Z,2023-07-11T22:09:40Z,2023-07-11T22:09:40Z,14,521,19,"Describe your changes
This is an ongoing attempt to integrate the C# bindings build, test and deployment to the CircleCI pipeline.
Progress:

 build-csharp-deploy workflow:

 build-csharp-linux
 build-csharp-windows
 build-csharp-macos
 store-and-upload-nupkgs (pack and deploy NuGet packages eventually)



Let me know if I'm off track here.
Open issues

How to actually build the native libs, some options:

The build scripts (this draft, can be switched once the workflow is in place)
#984
@drasticactions runtimes


The deployment method that we assumed requires to have the native libraries for every support platform in place before creting the Gpt4All nuget package. As far as I understand this can be solved with circleci workspaces (see Update1)

we could also consider other ways of doing this, e.g a separated package for each platform native assets.



UPDATE  1
Share the Data between jobs

The workspace bit seem to work as expected


UPDATE  2
Test integration / collection

Got the test integration working


UPDATE 3

Experimented with the build-bindings-backend-* jobs
/tmp/workspace:
linux-x64  osx-x64  win-x64  win-x64_msvc

/tmp/workspace/linux-x64:
libgptj-avxonly.so		   libllamamodel-mainline-default.so
libgptj-default.so		   libllmodel.so
libllamamodel-230511-avxonly.so    libmpt-avxonly.so
libllamamodel-230511-default.so    libmpt-default.so
libllamamodel-230519-avxonly.so    libreplit-mainline-avxonly.so
libllamamodel-230519-default.so    libreplit-mainline-default.so
libllamamodel-mainline-avxonly.so

/tmp/workspace/osx-x64:
libgptj-avxonly.dylib		      libllmodel.0.2.0.dylib
libgptj-default.dylib		      libllmodel.0.dylib
libllamamodel-230511-avxonly.dylib    libllmodel.dylib
libllamamodel-230511-default.dylib    libmpt-avxonly.dylib
libllamamodel-230519-avxonly.dylib    libmpt-default.dylib
libllamamodel-230519-default.dylib    libreplit-mainline-avxonly.dylib
libllamamodel-mainline-avxonly.dylib  libreplit-mainline-default.dylib
libllamamodel-mainline-default.dylib  libreplit-mainline-metal.dylib
libllamamodel-mainline-metal.dylib

/tmp/workspace/win-x64:
libatomic-1.dll     libgfortran-5.dll  
libquadmath-0.dll  libstdc++-6.dll
libgcc_s_seh-1.dll  libgomp-1.dll      
libssp-0.dll	  libwinpthread-1.dll
gptj-avxonly.dll		 llamamodel-mainline-default.dll
gptj-default.dll		 llmodel.dll
llamamodel-230511-avxonly.dll	 mpt-avxonly.dll
llamamodel-230511-default.dll	 mpt-default.dll
llamamodel-230519-avxonly.dll	 replit-mainline-avxonly.dll
llamamodel-230519-default.dll	 replit-mainline-default.dll
llamamodel-mainline-avxonly.dll

/tmp/workspace/win-x64_msvc:
gptj-avxonly.dll		 llamamodel-mainline-default.dll
gptj-default.dll		 llmodel.dll
llamamodel-230511-avxonly.dll	 mpt-avxonly.dll
llamamodel-230511-default.dll	 mpt-default.dll
llamamodel-230519-avxonly.dll	 replit-mainline-avxonly.dll
llamamodel-230519-default.dll	 replit-mainline-default.dll
llamamodel-mainline-avxonly.dll


Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",5,23
999,2023-06-16T04:46:16Z,,2023-07-03T12:53:47Z,1,6,5,"Describe your changes
Updated the emojis in the readme doc to
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,2
1007,2023-06-17T05:45:53Z,2023-06-18T18:07:46Z,2023-06-18T18:07:46Z,1,2,2,"Describe your changes
Fix 2 spelling typos in gpt4all.py
No function change.
Issue ticket number and link
#832
Checklist before requesting a review

[ x] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
1008,2023-06-17T11:28:52Z,2023-06-22T16:19:50Z,2023-06-22T16:19:50Z,1,5,0,"Describe your changes
Add the Access-Control-Allow-Origin header to the chat GUI's web API to enable CORS.
Issue ticket number and link
It was mentioned in #941 ""Introduce a button in the UI Settings to enable CORS for the Web Server Mode of GPT4ALL UI.""
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Using the following example HTML document:
<html><head><title>CORS test</title></head>
<body>
    <script type=""text/javascript"">
        function foo() {
            console.log(""start click handler"");
            function xhrHandler(e) {
                console.log(""handled."")
                console.log(e);
            };
            var xhr = new XMLHttpRequest();
            xhr.addEventListener(""load"", xhrHandler);
            xhr.open(""GET"", ""http://localhost:4891/v1/models"");
            xhr.send();
            console.log(""end click handler"");
        }
    </script>
    <input type=""button"" onclick=""foo()"" value=""click me!""></input>
</body>
</html>
Without the CORS header:

With the CORS header:

Steps to Reproduce

Use the example HTML
Start the web server in the chat GUI
The request is not allowed without a CORS header

Notes
C++ is not my strength, but I followed the example in the docs and it seemed straight-forward.
Please have a close look, though.
The linked issue requests a button to add to the UI. I simply enabled the header for everyone. I could make the PR more sophisticated, if necessary/desired.",4,2
1012,2023-06-18T13:00:26Z,2023-06-18T18:08:43Z,2023-06-18T18:08:43Z,1,15,0,"Describe your changes

Add some notes about common Windows problems when trying to make a local build (MinGW and MSVC).

Issue ticket number and link
No direct issue for this docs PR, but I could point to a few where people encountered these problems.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce

Build on Windows without prior knowledge; current README.md doesn't provide this information.

Notes
If you have feedback, I'd gladly amend/expand on this.
Note: There are several approaches to try and build/get it working with both MinGW and MSVC. I tried to keep the instructions compact and general.",2,1
1013,2023-06-18T15:02:23Z,2023-06-18T18:10:30Z,2023-06-18T18:10:30Z,1,3,3,"Describe your changes

2 typos
1 formatting adjustment to match the others

Issue ticket number and link
None.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
It's mostly a cosmetic change.
Notes
None",2,0
1018,2023-06-19T10:25:01Z,2023-06-22T15:48:12Z,2023-06-22T15:48:12Z,1,2,2,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
1020,2023-06-19T14:39:48Z,2023-06-19T18:34:53Z,2023-06-19T18:34:53Z,5,84,2,,2,0
1021,2023-06-19T16:29:00Z,2023-06-23T19:09:31Z,2023-06-23T19:09:31Z,5,282,3,"Describe your changes
Mostly additions to the docs.

Add gpt4all-bindings/cli/README.md
Unify version information
Add gpt4all-bindings/cli/developer_notes.md
Add gpt4all-bindings/python/docs/gpt4all_cli.md
Update gpt4all-bindings/python/docs/index.md
Bump version to 0.3.5, as that is the latest gpt4all package on PyPI

Issue ticket number and link

Closes #646
It basically addresses (unless there are other plans), as well: #682

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None yet
Steps to Reproduce
None/mostly documentation
Notes
Suggestions welcome.",2,3
1023,2023-06-19T19:45:23Z,,2023-12-28T00:40:58Z,16,1049,0,"Add Dart binding
This implementation provides the necessary Dart binding to execute GPT4All in Dart Command Line applications. If it gathers enough interest, I would love to extend the package so that it is executable as Flutter plugin (which needs some more investigation how to bring it to mobile devices).
[No issue ticket]
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed. I cannot add labels myself, but (enhancement) and (bindings) seem appropriate
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
import 'dart:io';

import 'package:gpt4all/gpt4all.dart';

void main() async {
  LLModel model = LLModel();
  try {
    // Always load the model before performing any other work.
    await model.load(
      // Path to the downloaded model file (*.bin)
      modelPath: '/some/path/to/ggml-gpt4all-j-v1.3-groovy.bin',
      // Path to the library folder including compiled *.dll (Windows), *.dylib (Mac) or
      // *.so (Linux) files
      librarySearchPath: '/some/path/gpt4all-backend/build',
    );

    // Optional: Define what to do with prompt responses
    // Prints to stdout if not defined
    // For demo purposes we print to stderr here
    LLModel.setResponseCallback(
          (int tokenId, String response) {
        stderr.write(response);
        return true;
      },
    );

    // Generate a response to the given prompt
    await model.generate(
      prompt: ""### Human:\nWhat is the meaning of life?\n### Assistant:"",
      generationConfig: LLModelGenerationConfig()..nPredict = 256,
    );
  } finally {
    // Always destroy the model after calling the load(..) method
    model.destroy();
  }
}

Steps to Reproduce
The binding can already be used by downloading the package on pub.dev (https://pub.dev/packages/gpt4all) and following the README.md
Notes
The implementation itself worked out quite smoothly. The only drawback at the moment is that Dart FFI only works with STATIC callbacks, which effectively lets the developer create only ONE response callback for (potentially) more than one LLModel instance. As of now,  I do not have a solution (if there is any at all), but I do not see this as a major issue since the developers might anyway just run no more than a single instance simultaneously.",5,20
1038,2023-06-21T12:20:39Z,2023-06-22T16:09:39Z,2023-06-22T16:09:39Z,2,2,2,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Notes
Afais, it should be sufficient to just add .org to the list of supported extensions.
As Org Mode is a clear text format, it should give usable results.",2,1
1039,2023-06-21T13:36:05Z,2023-06-23T20:28:53Z,2023-06-23T20:28:53Z,1,2,2,"Describe your changes
Changed ""Enable web server:"" to ""Enable API server:""
Found a few of new people confusing Web server with a WebUI, with this change it should be clearer as to what this setting does.
Checklist before requesting a review

[*] I have performed a self-review of my code.
[*] If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo",6,8
1042,2023-06-22T08:48:08Z,2023-06-23T08:16:36Z,2023-06-23T08:16:36Z,1,5,1,"Describe your changes
Change python bindings
Added possibility to set number of cpu threads n_threads directly in class GPT4All, when instantiating.
mpt = gpt4all.GPT4All(model_name = ""ggml-mpt-7b-chat"",  model_path = ""D:/00613/nomic.ai/GPT4all"", allow_download=False, n_threads=6)
It seems to me, that the class GPT4All works as an interface to the python user. So the python user should be able to set the number of threads directly with a method of GPT4All.
Issue ticket number and link
#1029 Add the possibility to set the number of CPU threads (n_threads) with the python bindings
when instantiating.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,0
1045,2023-06-22T20:55:52Z,2023-06-22T22:49:58Z,2023-06-22T22:49:58Z,1,17,17,"Describe your changes
There were a number of grammatical errors in the README for the Java bindings. This corrects them.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",3,0
1050,2023-06-23T21:14:44Z,2023-06-27T14:49:46Z,2023-06-27T14:49:46Z,2,7,7,"Describe your changes
Cleaned up and made the sideloading part more readable, also moved Replit architecture to supported ones. (+ renamed all ""ggML"" to ""GGML"" because who calls it ""ggML""??)",3,2
1053,2023-06-24T21:44:50Z,2023-07-05T20:33:12Z,2023-07-05T20:33:12Z,9,355,116,"Describe your changes


The main change is to support gpt4all 2.4.8 (previous 2.4.6). Addition of Replit and Metal support in this Java binding.


A few minor additions and changes as described in Readme.md version history


Add working Unit tests in BasicTests.java (The test does not depend on loading the backend and tests all the Java code in isolation). Previous disabled Tests.java where depending on the backend removed. Will be covered by more advanced integration tests in the future.


Docs update


Tested on Windows 11, Linux WSL, and MacBook M1 hardware. Replit with metal support specifically tested on Macbook.


Version 1.1.3 published to maven central.


Issue ticket number and link
Checklist before requesting a review

[x ] I have performed a self-review of my code.
[x ] If it is a core feature, I have added thorough tests.
[x ] I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,3
1055,2023-06-25T12:55:25Z,2023-06-26T17:58:24Z,2023-06-26T17:58:24Z,1,2,2,"Describe your changes
The patch include support for running natively on a Linux Wayland display server/compositor which is successor to old Xorg. Cmakelist was missing WaylandClient so added it back.
Issue ticket number and link
This will fix #1047 .
Although I have tested it at my end, this PR should be reviewed properly in all configurations before merging.",2,2
1067,2023-06-26T09:55:28Z,,2024-03-06T18:59:07Z,1,109,0,"Hello from GitHub Security Lab!
Your repository is critical to the security of the Open Source Software (OSS) ecosystem and as part of our mission to make OSS safer, we are contributing a CodeQL configuration for code scanning to your repository. By enabling code scanning with CodeQL, you will be able to continuously analyze your code and surface potential vulnerabilities before they can even reach your codebase. In fact, you may have seen some alerts already appearing on this pull request!
We’ve tested the configuration manually before opening this pull request and adjusted it to the needs of your particular repository, but feel free to tweak it further! Check this page for detailed documentation.
Questions? Check out the FAQ below!
FAQ

Click here to expand the FAQ section
How often will the code scanning analysis run?
By default, code scanning will trigger a scan with the CodeQL engine on the following events:

On every pull request — to flag up potential security problems for you to investigate before merging a PR.
On every push to your default branch and other protected branches — this keeps the analysis results on your repository’s Security tab up to date.
Once a week at a fixed time — to make sure you benefit from the latest updated security analysis even when no code was committed or PRs were opened.

What will this cost?
Nothing! The CodeQL engine will run inside GitHub Actions, making use of your unlimited free compute minutes for public repositories.
What types of problems does CodeQL find?
By default, code scanning runs the default query suite.
How do I upgrade my CodeQL engine?
No need! New versions of the CodeQL analysis are constantly deployed on GitHub.com; your repository will automatically benefit from the most recently released version.
The analysis doesn’t seem to be working
If you get an error in GitHub Actions that indicates that CodeQL wasn’t able to analyze your code, please follow the instructions here to debug the analysis.
Which source code hosting platforms does code scanning support?
GitHub code scanning is deeply integrated within GitHub itself. If you’d like to scan source code that is hosted elsewhere, we suggest that you create a mirror of that code on GitHub.",2,3
1068,2023-06-26T15:33:15Z,2023-06-26T17:40:53Z,2023-06-26T17:40:53Z,2,7,6,add max_size param to ggml_metal_add_buffer - introduced in ggerganov/llama.cpp#1826,2,0
1069,2023-06-26T19:19:20Z,2023-06-26T21:27:59Z,2023-06-26T21:27:59Z,14,154,8,"most of these can just shortcut out of the model loading logic
llama is a bit worse to deal with because we submodule it so I have to at least parse the hparams, and then I just use the size on disk as an estimate for the mem size (which seems reasonable since we mmap() the llama files anyway)
tested via added method in python binding (.gpt4all is a symlink to my actual model dir)
from pathlib import Path
from gpt4all import pyllmodel

models = list((Path.home() / '.gpt4all').glob('*.bin'))
llm = pyllmodel.LLModel()

allsizes = {}
for model in models:
    print(f'get size for {model}: ', end='')
    sz = llm.memory_needed(str(model))
    print(f'{sz}')
    allsizes[model.name] = sz

print(allsizes)",2,1
1073,2023-06-27T02:57:10Z,2023-06-27T17:06:39Z,2023-06-27T17:06:39Z,8,1230,6,"Tested with https://huggingface.co/TheBloke/falcon-7b-instruct-GGML/blob/main/falcon7b-instruct.ggmlv3.q4_0.bin
does not support 40B models (requires changes to ggml that do not yet exist upstream) but otherwise uses the same format as currently distributed ggml falcon models",3,6
1076,2023-06-27T15:55:38Z,2023-06-27T17:23:56Z,2023-06-27T17:23:56Z,9,100,7,,2,0
1087,2023-06-28T17:50:43Z,2023-06-28T23:11:24Z,2023-06-28T23:11:24Z,5,459,158,is necessary to get rid of technical debt before we drastically increase the complexity of settings by adding per model settings and mirostat and other fun things. Right now the settings are divided between QML and C++ and some convenience methods to deal with settings sync and so on that are in other singletons. This change consolidates all the logic for settings into a single class with a single API for both C++ and QML.,3,4
1089,2023-06-28T21:13:16Z,2023-06-29T00:35:08Z,2023-06-29T00:35:08Z,6,81,202,prepping to hack on these by hopefully making there be fewer places to fix the same bug,2,0
1090,2023-06-28T22:20:22Z,2023-06-30T20:02:03Z,2023-06-30T20:02:03Z,15,442,412,"Testing, documenting and refactoring the python bindings to:

isort and black
include inference unit tests for orca model
expose writable c api parameters
change streaming to return an iterable, remove top level generator method in favor is just generate
introduces a chat session context manager for fast inference during chats with prompt templates.
bug fix to top_k and top_p to make them actually work",4,2
1092,2023-06-29T00:43:12Z,2023-06-29T03:44:48Z,2023-06-29T03:44:48Z,15,228,188,,2,0
1104,2023-06-29T19:26:56Z,2023-06-30T17:53:45Z,2023-06-30T17:53:45Z,2,44,24,"should reduce required eval memory for these model types and makes all of our models reserve eval memory in the same way (almost - some still do the mem_per_token thing and some don't - next task is to unify that behavior)

backend/gptj: use scratch buffers
backend/mpt: use scratch bufs
fix format-related compile warnings",2,1
1112,2023-06-30T20:48:50Z,2023-07-21T19:13:30Z,2023-07-21T19:13:30Z,14,269,110,"Describe your changes
Adds capability to run GPU accelerated inference server using GPT4All API
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Steps to Reproduce

Follow instructions in gpt4all-api/README.md",2,0
1120,2023-07-01T22:20:49Z,2023-07-06T02:42:15Z,2023-07-06T02:42:15Z,1,59,1,"Describe your changes

Adapted to Python bindings API changes
Version selection based on package information
Does not currently work with 1.0.1 however, as it's not fully implemented:
NotImplementedError: Streaming tokens in a chat session is not currently supported.


Does, however, work with main.



Issue ticket number and link

in the meantime, someone reported it as: #1123

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

(Commited version doesn't include the message version 1.0.1, using new loop)
Steps to Reproduce

CLI is currently broken with Python bindings v1.0.0 & v1.0.1; just try to run it
This PR won't fix that yet, but it should once streaming with chat session is supported. It is and it works.

Notes

Alternative ideas to fix it? Maybe I didn't understand the new API completely?
It's mostly a hack for now in any case. Fixing it is first priority, we can talk about improvements later.",2,0
1129,2023-07-03T19:52:01Z,2023-07-06T03:17:31Z,2023-07-06T03:17:31Z,3,95,37,"Describe your changes
Adding the ability to do streaming requests to the ""/completions"" endpoint of the API. Added in a test to showcase using the openai python client to stream the response from the api.
This addresses the NotImplementedError in the completions route of the api
https://github.com/nomic-ai/gpt4all/blob/main/gpt4all-api/gpt4all_api/app/api_v1/routes/completions.py#L54
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
    model = ""gpt4all-j-v1.3-groovy""
    prompt = ""Who is Michael Jordan?""
    for resp in openai.Completion.create(
            model=model,
            prompt=prompt,
            max_tokens=50,
            temperature=0.28,
            top_p=0.95,
            n=1,
            echo=True,
            stream=True):
        print(resp.choices[0].text)
The above results in the model output being streamed to your console, directly from the API
Notes
There is a great issue thread over on the openai-node library page, discussing a lot of this streaming work.
openai/openai-node#18",3,2
1131,2023-07-03T23:50:07Z,2023-07-04T01:30:25Z,2023-07-04T01:30:25Z,3,31,20,"Fixes some (not all) of the typechecking complaints I get editing these files
with a linter enabled.

python: do not mutate locals()
python: fix (some) typing complaints
python: queue sentinel need not be a str
python: make long inference tests opt in",2,0
1134,2023-07-05T12:26:42Z,,2023-07-05T13:35:13Z,1,15,10,"Describe your changes
Issue ticket number and link
#1026
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
After:

Before:

Steps to Reproduce
Prompt: ﻿Write a simple hello world in python
Notes
Nothing of note",2,1
1145,2023-07-06T15:35:04Z,2023-07-19T22:36:49Z,2023-07-19T22:36:50Z,3,212,145,"pyllmodel
The main idea of the PR is to use the callback functionality more heavily which fits with the style of the backend. The prompt_model function now requires an additional argument: callback, and uses it to return the generated tokens. This means that we get rid of DualStreamProcessor and the code is simplified. The prompt_model_streaming function used to duplicate a lot of code from prompt_model. Now, since prompt_model accepts a callback, the function prompt_model_streaming delegates the prompt generation to prompt_model and only manages the queue and the threading. I also added the verbose argument to prompt_model which prints out the prompt before it is passed to the model.
pygpt4all
I added the callback and verbose arguments to generate. Due to callbacks, the current chat session is updated properly even when streaming = True (before, the user had to add assistant's response to current_chat_session him/herself). Also, chat session now accepts a header, i.e. an instruction that comes before the transcript of the conversation. This can be used to give the assistant some character.
Update
The verbose argument of prompt_model was removed and replaced by a logging.info call. The header argument of the chat session was renamed to system_prompt and a new argument prompt_template was added. The retrieve_model method returns the model config received from models.json instead of the model path. The systemPrompt and promptTemplate from the config (if given) are used as default values for the system_prompt and prompt_template of the chat sessions.
Update 2
Added the possibility to set the prompt template using set_prompt_template. Now everything which is sent to the generate function is formatted even if the chat session is not activated. The users can use prompt templates with multiple placeholders which can be filled by passing a dictionary to the generate method. See an example in the showcase section.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Showcase
Using chat session header, verbose = True, and showing that the chat session is updated properly with streaming=True: https://pastebin.com/UAz8KC2x
Using a custom callback to make a your mama joke: https://pastebin.com/s4NgHzqD
Update: default system prompts and prompt templates are model-specific now. We can also change both if we want: https://pastebin.com/zqKHp5aN
Update 2: using prompt template with and without multiple placeholders outside of a chat session: https://pastebin.com/AShu4BcD",4,31
1154,2023-07-08T14:06:13Z,,2023-10-12T11:54:01Z,6,69,65,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,0
1163,2023-07-10T17:01:24Z,2023-07-10T20:23:33Z,2023-07-10T20:23:33Z,1,44,2,"Describe your changes
created highlighting rules for json formatted file and c# code
Checklist before requesting a review

 I have performed a self-review of my code.

Demo
json:",3,0
1164,2023-07-10T17:54:17Z,2023-07-12T19:18:24Z,2023-07-12T19:18:25Z,9,245,29,"Describe your changes
Updated training scripts with configs + attention masking correclty
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1168,2023-07-11T05:35:42Z,2023-07-11T16:43:45Z,2023-07-11T16:43:45Z,7,100,13,"Describe your changes
The bindings version changed to 1.1.4
Update Java bindings to be compatible with gpt4all 2.4.11
falcon model support.
Developer docs included for Java bindings with instructions on how to build Java bindings.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
1179,2023-07-12T11:24:09Z,2023-07-12T14:49:25Z,2023-07-12T14:49:25Z,2,2,0,always write build_commands.json to cmake build dir friendly for using editors with clangd integration that don't also manage the build themselves - this is currently already enabled in llama.cpp cmakelists but the exported build_commands are missing the chat and backend sources since it is not set early enough,2,0
1183,2023-07-12T14:26:53Z,2023-07-12T19:19:27Z,2023-07-12T19:19:27Z,1,9,3,"Describe your changes

Add information about the AVX/AVX2 requirement.
Update supported architectures.

Issue ticket number and link
No issue, but had a case on Discord and figured it'd be a good idea to mention it in the docs and on the website.
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
None
Notes
Basically, I created this PR to add the AVX/AVX2 information. Not sure if there are better links for the added foundation models, I just searched them on Hugging Face.",2,0
1186,2023-07-12T15:45:05Z,2023-07-12T16:46:47Z,2023-07-12T16:46:47Z,1,6,6,"see https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-isprocessorfeaturepresent
(uploading to check it builds on CI, don't push if it doesn't!)",3,1
1194,2023-07-13T11:28:57Z,2023-07-13T18:44:18Z,2023-07-13T18:44:18Z,1,2,2,"Describe your changes

fix Windows MSVC arch detection in llmodel.cpp
to fix AVX-only handling

Issue ticket number and link
there are several now. probably all of these are about the same bug:

#965
#1009
#1060
#1084
#1097
#1175

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Steps to Reproduce
Windows MSVC compiler doesn't have __x86_64__ for x86-64, but instead: _M_X64
Notes
Hope this finally fixes that.  😅",2,0
1196,2023-07-13T21:11:33Z,2023-07-14T15:36:01Z,2023-07-14T15:36:01Z,1,153,1,"Describe your changes
Created highlighting rules to make is easier for users to intake code in html, php, and latex
Checklist before requesting a review

 I have performed a self-review of my code.

Demo
HTML:

PHP:

LaTeX:

Steps to Reproduce
groovy model, write some [language] code",2,0
1204,2023-07-14T14:09:23Z,,2023-07-14T16:38:08Z,3,11,4,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1205,2023-07-14T15:17:59Z,,2023-09-07T17:02:34Z,7,42,36,"major difference here is the change from ggml_graph_compute to require doing
a ggml_graph_plan first and how scratch memory that is in addition to the dst
tensors built during normal graph construction is no longer allocated during
graph execution but figured out by ggml_graph_plan - this could change size
on each run and we write to the eval_buf during graph construction (copying
the input token IDs, for one) so this needs to be a new buffer, but it need not
retain information between calls
this is a submodule change - don't forget to git submodule update!",2,1
1206,2023-07-14T16:42:03Z,2023-07-15T22:07:43Z,2023-07-15T22:07:43Z,4,15,4,"Describe your changes
Make sure model file is both found and readable to java  before letting backend load it.
Issue ticket number and link
To solve issue #1167
Checklist before requesting a review

[x ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Notes",3,0
1209,2023-07-15T15:06:54Z,2023-07-19T11:05:43Z,2023-07-19T11:05:43Z,1,1,1,"Describe your changes
The main readme has installer links, too. So it makes sense to make the AVX/AVX2 requirement visible here.
Issue ticket number and link
None
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
None
Notes
None",3,0
1215,2023-07-16T16:46:08Z,2023-07-17T20:21:03Z,2023-07-17T20:21:03Z,4,16,1,"Describe your changes

Change to llmodel_embedding() to behave more gracefully when receiving unexpected input.
Instead of crashing, receiving NULL for the model or text, or getting an empty string returns NULL.

Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
On Windows:
\> py
Python 3.11.3 (tags/v3.11.3:f3909b8, Apr  4 2023, 23:49:59) [MSC v.1934 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from gpt4all import GPT4All, Embed4All
>>> text = ''
>>> embedder = Embed4All()
Found model file at  <...>\\ggml-all-MiniLM-L6-v2-f16.bin
>>> output = embedder.embed(text)
>>> print(output)
[]
Steps to Reproduce
Try the above without the patch. It doesn't like it very much.
Notes
I could do some error codes and messages instead, if desired (might be more appropriate if model == nullptr.",3,5
1219,2023-07-18T02:19:17Z,2023-07-19T14:36:23Z,2023-07-19T14:36:23Z,2,2,2,"Describe your changes
TopP 0.1 was found to be somewhat too aggressive, so a more moderate default of 0.4 would be better suited for general use.
Checklist before requesting a review

[*] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
[*] I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",7,4
1249,2023-07-23T01:14:02Z,2023-07-30T23:34:06Z,2023-07-30T23:34:06Z,9,441,59,"Describe your changes

Add Release build hint to Readme.
Documentation overhaul: extend Python docs and update with information on the last big PR.
black & isort linting in Python bindings.
Move Chat GUI out of the Bindings group in the docs navigation.

Issue ticket number and link
None
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
mainly: make documentation 😉
Notes
Comments welcome. Especially if something can be rearranged or made more clear & concise. I've been at it for a while.",2,7
1260,2023-07-24T12:19:31Z,2023-08-11T18:14:53Z,2023-08-11T18:14:53Z,1,5,1,"Describe your changes
Documentation
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",4,3
1270,2023-07-25T15:51:31Z,2023-07-25T21:24:20Z,2023-07-25T21:24:20Z,1,5,2,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1280,2023-07-26T19:44:54Z,2023-07-27T13:31:56Z,2023-07-27T13:31:56Z,4,89,31,"Describe your changes
added aa button to switch color modes and check functionality between light mode and dark mode
Checklist before requesting a review

 I have performed a self-review of my code.

Demo

  
    
    

    lightmodedemo.mov
    
  

  

  


Steps to Reproduce
click on settings, application settings, change theme
Notes
possible future features are more color based mode like a blue based mode or adding detection of system color as default color scheme",2,0
1281,2023-07-26T21:33:26Z,2023-07-30T18:29:51Z,2023-07-30T18:29:51Z,1,45,4,"This is a follow-up to the PR #867 which was left unfinished. I've implemented a buffer of bytes that gets filled until it can be decoded into a string. Most of the credit goes to the previous PR.
However, there is an issue that can be seen in the pastebin from the demo section. Something goes wrong with the bytes of an emoji and the rest of the messages doesn't get decoded. This can be fixed by ""cutting out"" the bad emoji leaving a ""?"" in its place and decoding the rest of the message without it. But why does a bad emoji happen in the first place? It's just the model being dumb or I'm missing something?
Update 27.07: In the new commit, the buffer is split into space-separated chunks. If the generation of a chunk was finished, but it wasn't decoded, we replace it by the <?> unicode symbol and keep going. You can see an example of the new behavior in the demo.
Update 28.07: Rewrote the the unicode decoding process by using the format of the multi-byte unicode symbols:

click for details
Previous PR
Checklist before requesting a review

 I have performed a self-review of my code.

Demo
https://pastebin.com/jivmECqT
Update 27.07: https://pastebin.com/qTTJPRiv
Update 28.07: https://pastebin.com/4RT3ajRf",5,2
1283,2023-07-27T01:55:57Z,2023-07-27T03:06:17Z,2023-07-27T03:06:17Z,2,8,5,"Describe your changes
Better error handling and display for nodejs users
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo",3,0
1285,2023-07-27T17:03:24Z,,2024-03-10T21:17:30Z,1,16,7,"Describe your changes
Fixes bug that wouldn't load checkpoints correctly
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,3
1287,2023-07-28T04:28:49Z,2023-08-08T15:29:56Z,2023-08-08T15:29:56Z,15,5860,6922,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes

InferenceModel & EmbeddingModel wrappers to hold some state in JS
simplified downloadModel a little
retrieveModel now returns the model config from the downloaded json
createPrompt -> formatChatPrompt and updated it to use promptTemplate & systemPrompt
added py defaults as defaults to createCompletion
removed ModelFile types
removed the extra yarn install as the issues it fixed seems to be solved

some work still needed:

kv cache reuse / proper cache session (and context reset if its not a chat session)
better types for loadModel
regarding downloadModel: prompt templates not getting used when allowDownload=false might be a gotcha and make models suddenly perform worse when used offline. we could cache models.json in the modelsPath maybe?",2,4
1293,2023-07-30T15:37:44Z,,2024-03-06T19:02:30Z,9,1320,80,"Describe your changes

Python tests:

Add pytest markers for better test selection control
Ability to set model folders other than the default with the new PYTEST_MODEL_PATH environment variable (tests only)
Add tests for the gpt4all.gpt4all module / GPT4All class
Add tests for backend code (ctypes related) in the gpt4all.pyllmodel module

not exhaustive but the important parts are there
uncovered minor bugs (embeddings memory management) and a minor missing feature (embeddings memory requirement)


Add tests for the rest of the gpt4all.pyllmodel module in test_pyllmodel.py (focused on the LLModel class)


Many more type hints:

pyllmodel module
gpt4all module


Improve embedding logic in LLModel class
Factoring out some things into separate methods in GPT4All to make it more flexible:

Model initialization logic: so the heavy lifting could be handled differently in a subclass (e.g. deferred).
Chat session open/close: so it's easier to manage a session if there are separate localities (e.g. when using a framework).



There are no API breaking changes.
Pytest Markers

For the documentation on how markers work see: https://docs.pytest.org/en/7.4.x/how-to/mark.html
Now these markers are defined (see pytest.ini):

inference: all tests that do inference on a model. These are typically very taxing on both CPU and RAM
online: tests that require an internet connection (e.g. for models.json or the embeddings model)
slow: tests which typically take more than a few seconds; e.g. the heavier inference tests
c_api_call: only in test_backend.py, on tests which call the C API. While other Python tests can ultimately call the backend, too, the idea is that these can be run after backend changes.


Example: pytest -m ""not inference and not online""

Specifying Model Path with PYTEST_MODEL_PATH
Tests now look for an environment variable PYTEST_MODEL_PATH which can be used to specify model folders. Example Linux:
$ export PYTEST_MODEL_PATH=""${HOME}/.local/share/nomic.ai/GPT4All:${HOME}/.cache/gpt4all""
More details in tests/__init__.py in the docstring.
Issue ticket number and link
None
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
mostly: pytest gpt4all-bindings/python  😉
Notes
Not sure about everything I'll include in this one, might split it up further.",4,17
1296,2023-07-31T03:52:56Z,2023-08-02T14:58:14Z,2023-08-02T14:58:14Z,1,16,0,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",3,5
1299,2023-07-31T15:05:09Z,2023-07-31T16:18:20Z,2023-07-31T16:18:20Z,1,68,0,"Describe your changes
fixed issue of code blocks in light mode would have dark text.
Issue ticket number and link
Issue #1291
https://github.com/nomic-ai/gpt4all/issues/1291
Checklist before requesting a review

 I have performed a self-review of my code.

Demo

Steps to Reproduce
ask for code while in light mode",2,0
1315,2023-08-04T01:16:24Z,2023-08-10T15:27:09Z,2023-08-10T15:27:09Z,1,1,0,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,0
1322,2023-08-07T16:53:54Z,2023-08-07T17:54:13Z,2023-08-07T17:54:13Z,20,174,21,"Describe your changes
added a dropdown in application settings to allow of transition of fontsize from small, medium, and large.
Issue ticket number and link
#986 #647 #488
Checklist before requesting a review

 I have performed a self-review of my code.

Demo

  
    
    

    Screen.Recording.2023-08-07.at.12.52.32.PM.mov
    
  

  

  


Steps to Reproduce
change font size in application settings",3,1
1323,2023-08-07T22:31:38Z,2023-08-09T07:27:43Z,2023-08-09T07:27:44Z,1,7,4,"Describe your changes

Replace high-level IsProcessorFeaturePresent
Reintroduce low-level compiler intrinsics implementation

Issue ticket number and link

#1288

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce


Try to run GPT4All on an outdated Windows version. It'll fail with the catch-all model load error despite having AVX/AVX2.


See the comments in linked issue (from here); the test script returns False for all AVX checks even if present.


Notes
It's quite unfortunate how this all came to be. The previous low-level code was changed in an attempt to fix the Windows AVX-only problem, but that ultimately turned out be caused by an incomplete pre-processor macro. That in turn masked this problem.
The (potential) fix here reintroduces the old low-level code. But because of the bug, this was never really tested to work as it should, although I think several people have looked at it, compared with the documentation, and found no obvious fault: https://learn.microsoft.com/en-us/cpp/intrinsics/cpuid-cpuidex
It does work on my machine, but I don't have an AVX-only system to test, nor do I have an older build of Windows 10.",3,1
1333,2023-08-10T16:35:45Z,2023-08-10T21:56:54Z,2023-08-10T21:56:54Z,1,15,5,"Describe your changes

minor oversight: there are now six supported architectures
LLAMA -> LLaMA (for v1)
note about Llama 2 and link to license
limit some of the paragraphs to 150 chars

Issue ticket number and link
None
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
None
Notes
Made the PR on my own repository by accident. The note on Llama 2 is what I was talking about on Discord.",2,1
1348,2023-08-15T12:49:17Z,,2023-10-12T18:24:48Z,1,9,4,"Describe your changes
Simply write some messages to stderr so it's easier to identify the problem when the create/create2 call fails.
Issue ticket number and link

#1256

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
None
Steps to Reproduce
None
Notes
Not sure if this is the right way to go about that, but it's better than not having anything and be left guessing when trying to troubleshoot an issue.",4,5
1351,2023-08-16T10:49:58Z,2023-10-12T18:01:44Z,2023-10-12T18:01:44Z,1,1,6,Solves #788,2,0
1366,2023-08-22T20:21:39Z,,2024-03-06T22:48:48Z,6,64,16,"updated typing in Settings
implemented list_engines - list all available GPT4All models
separate models into models directory
method response is a model to make sure that api v1 will not change
resolve #1371

Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
1369,2023-08-23T21:14:20Z,,2024-03-12T12:39:35Z,4,14,15,"Stop putting models inside images because they're huge and not part of the code
Mount an external volume inside the image for models
Use the system-defined number of virtual threads when starting the API service
Update README for new models folder setup + mounting
Update default LLM to latest, llama-2-7b-chat.ggmlv3.q8_0.bin

Describe your changes
In the spirit of docker, let's please not put LLM's inside the image.
Issue ticket number and link
N/A
Checklist before requesting a review

 I have performed a self-review of my code.
 I have added thorough documentation for my code.",4,3
1379,2023-08-28T09:43:14Z,,2024-03-20T22:24:54Z,1,1,0,"Describe your changes
Added promptContext to completion response (typescript bindings) to be able to display the prompt context in a user interface. I couldnt add a label myself tho
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,4
1383,2023-08-30T14:40:37Z,,2023-08-31T19:32:08Z,21,616,87,,3,1
1390,2023-09-02T06:39:15Z,2023-11-01T19:38:58Z,2023-11-01T19:38:58Z,17,5882,4347,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",9,3
1397,2023-09-05T14:13:07Z,,2023-10-12T11:55:23Z,1,1,1,"Fix for executable glscl not found on win11 when running build_win-mingw.ps1 in gpt4all-binding/csharp directory
Full error message:
-- Found Vulkan: C:/VulkanSDK/1.3.261.1/Lib/vulkan-1.lib (found version ""1.3.261"") found components: glslc glslangValidator                                                   CMake Error at llama.cpp.cmake:160 (message):                                                                                                                                   glslc not found                                                                                                                                                              Call Stack (most recent call first):                                                                                                                                             CMakeLists.txt:46 (include)
on win11, from PowerShell run with admin rights",3,0
1398,2023-09-05T20:54:44Z,2023-09-11T15:44:40Z,2023-09-11T15:44:40Z,1,1,1,"Removes extra transitive deps (except for libvulkan) when building model backends with Vulkan support (libkompute, libfmt)
to avoid some headaches with dynamic linking.
changes are in llama.cpp, related pr: nomic-ai/llama.cpp#2",2,0
1412,2023-09-12T19:40:09Z,2023-09-12T21:16:02Z,2023-09-12T21:16:02Z,3,3,3,,2,0
1414,2023-09-13T09:57:23Z,2023-10-12T11:53:12Z,2023-10-12T11:53:12Z,2,265,9,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

[ x] I have performed a self-review of my code.
[ x] If it is a core feature, I have added thorough tests.
[ x] I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",4,0
1427,2023-09-15T08:52:10Z,2023-10-05T14:12:44Z,2023-10-05T14:12:44Z,3,223,0,"Describe your changes
Add flatpak manifest for building flatpaks on Linux
Refer: #698
TODOs:

 Create flatpak manifests
 Add circleci tests
 submit to flathub

Testers and reviews welcome! You will need  org.kde.Sdk 6.5 and org.freedesktop.Sdk.Extension.node14 for it to build. Please do install it before running flatpak-builder.",5,3
1446,2023-09-21T13:56:31Z,2023-10-12T11:52:36Z,2023-10-12T11:52:36Z,1,11,0,"Describe your changes
Added star-history to README.md
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1462,2023-09-30T23:22:51Z,2023-10-11T18:12:40Z,2023-10-11T18:12:40Z,1,6,4,"This makes it easier to use the GPT4All interface from Python with arbitrary paths, which may be of type pathlib.Path instead of a string.",2,0
1485,2023-10-09T17:40:06Z,2023-10-10T15:44:43Z,2023-10-10T15:44:43Z,1,16,0,"Describe your changes
Added the just-released EM German Mistral Model to models2.json.
You can find more information, example outputs and so on in the models repository. The model comes with the Apache-2.0 license of the base model.
TBH, I still don't understand 100% how GPT4ALL`s model list works (I suppose its still based on a proprietary hosted model list?); the model wasn't available for download in the model list when I rebuild GPT4all, but I confirmed that the model works falwlessly within the most recent app.
I asked in Discord and they said I should just prepare a PR.
Reson for this is I am the author of the most-downloaded llama-2 model finetuned on German texts and people kept asking me how to use it in a GUI; would be good to have a user-friendly alternative to LM Studio, thus I would appreciate support within GPT4ALL.
Checklist before requesting a review

[X ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
1486,2023-10-10T14:18:36Z,2023-10-11T15:31:34Z,2023-10-11T15:31:34Z,1,1,1,"I saw this while building with Visual Studio. It seems like a good idea to initialize responseLogits before serializing it, even if its value isn't used anywhere.",2,0
1487,2023-10-10T14:49:06Z,2023-10-10T19:50:03Z,2023-10-10T19:50:03Z,1,2,2,I forgot to uncomment these lines when I finished GPT-J support.,2,0
1488,2023-10-10T15:41:21Z,2023-10-11T15:30:49Z,2023-10-11T15:30:49Z,1,1,1,Fixes #1479,2,0
1492,2023-10-10T18:12:09Z,2023-10-11T21:14:37Z,2023-10-11T21:14:37Z,2,20,3,"disable llama.cpp logging unless GPT4ALL_VERBOSE_LLAMACPP envvar is nonempty
make verbose flag for retrieve_model default false (but also be overridable via gpt4all constructor)

should be able to run a basic test:
import gpt4all
model = gpt4all.GPT4All('/Users/aaron/Downloads/rift-coder-v0-7b-q4_0.gguf')
print(model.generate('def fib(n):'))
and see no non-model output when successful",4,5
1493,2023-10-10T20:44:36Z,2023-10-11T13:16:02Z,2023-10-11T13:16:02Z,5,192,31,This restores the state from a file even if the model is missing for instance. This is necessary for the new release since all the models will have changed. This will also enable a new feature in an upcoming commit where we have a new setting for always restoring state from text instead of via kvcache so as to limit disk usage.,3,0
1495,2023-10-11T16:15:16Z,2023-10-12T11:52:12Z,2023-10-12T11:52:12Z,9,74,156,This also changes the UI behavior to always open a 'New Chat' and setting it as current instead of setting a restored chat as current. This improves usability by not requiring the user to wait if they want to immediately start chatting.,3,0
1500,2023-10-11T21:35:45Z,2023-10-12T11:52:56Z,2023-10-12T11:52:56Z,1,1,1,"file in models2.json has no ggml- prefix
https://discordapp.com/channels/1076964370942267462/1093558720690143283/1161778216462192692",3,0
1503,2023-10-12T03:54:15Z,2023-10-12T18:56:54Z,2023-10-12T18:56:55Z,2,5,1,"reenable gpu prompt processing
 q6k mat*mat works
 q4_1 mat*mat works",2,0
1511,2023-10-13T14:52:02Z,2023-10-18T16:23:01Z,2023-10-18T16:23:01Z,1,1,1,"I tried to install from pip on OS X 10.13 High Sierra and got ""load command 0x80000034 is unknown"" while trying to load a .dylib. This corresponds to the LC_DYLD_CHAINED_FIXUPS command, which is only available as of OS X 10.15 (see 10.14 SDK, 10.15 SDK).
related to #70 (although that issue mainly refers to the chat client)",2,0
1515,2023-10-16T02:06:25Z,2023-10-19T19:25:18Z,2023-10-19T19:25:18Z,10,24,1038,"Add replit-code-v1.5 support by fixing upstream first and then removing our custom MPT implementation in favor of llama.cpp.
This has been tested on CPU and CUDA, and via the python bindings. It does not currently support Vulkan due to not having an Alibi kernel.",2,3
1529,2023-10-18T21:14:38Z,2023-10-19T00:24:55Z,2023-10-19T00:24:55Z,6,15,11,"This PR bumps the version of the python bindings to 2.0.0rc1, and fixes the CircleCI Windows build.
Wish list

 Be more robust against interrupted model retrieval due to flaky connections
 Display a meaningful error message when trying to load a GGML file, instead of a generic one
 Test macOS build",3,0
1532,2023-10-19T00:37:42Z,2023-10-21T14:38:46Z,2023-10-21T14:38:46Z,9,58,63,This will make the speech output less verbose for screen reader users.,2,0
1542,2023-10-20T14:32:48Z,2023-10-21T18:56:07Z,2023-10-21T18:56:07Z,1,1,1,"corrected model download directory
Describe your changes
the model download directory was wrong
Issue ticket number and link
Checklist before requesting a review

[X ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,6
1546,2023-10-21T04:38:59Z,,2023-10-23T18:28:16Z,1,41,31,"this fixes some issues that were being seen on installed windows builds of 2.5.0
setImplementationsSearchPath did not work if llmodel was initialized before it was called


do not actually load impl dlls in the static initializer, wait until we're actually trying to load a model for the first time


rescan on the next model load attempt if the search path has been changed


only load dlls that actually might be model impl dlls, otherwise we pull all sorts of random junk into the process before it might expect to be",4,4
1547,2023-10-21T20:41:15Z,2023-10-22T15:58:28Z,2023-10-22T15:58:28Z,7,29,34,ref #1502 (comment),2,0
1548,2023-10-22T00:53:44Z,2023-10-22T04:22:37Z,2023-10-22T04:22:37Z,1,2,2,Follow-up to #1532,2,0
1555,2023-10-23T15:53:06Z,2023-10-24T13:28:21Z,2023-10-24T13:28:21Z,15,14,3,"Scripts should have a shebang and be mode 0755, so they can be executed directly at the command line on macOS or Linux.",2,0
1556,2023-10-23T18:24:16Z,2023-10-24T04:40:15Z,2023-10-24T04:40:15Z,1,11,0,"this fixes some issues that were being seen on installed windows builds of 2.5.0
only load dlls that actually might be model impl dlls, otherwise we pull all sorts of random junk into the process before it might expect to be
This is #1546 minus the intialization order changes",2,2
1574,2023-10-25T20:30:57Z,2023-10-26T14:07:06Z,2023-10-26T14:07:06Z,1,1,1,,3,0
1584,2023-10-27T22:39:54Z,2023-10-30T14:37:05Z,2023-10-30T14:37:05Z,3,0,9,This is a small issue I recently noticed - there were a few lingering references to dynamic libraries that are not built anymore.,2,0
1587,2023-10-28T01:42:05Z,2023-10-30T14:37:32Z,2023-10-30T14:37:32Z,1,3,1,"Only the Qt HTTP Server and Qt PDF libraries (and their implied dependencies) are needed, as far as I know. This could save the user a significant amount of disk space.",2,0
1602,2023-11-02T16:42:17Z,2023-11-02T21:25:33Z,2023-11-02T21:25:33Z,2,44,0,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes

Adds clangd optional LSP support for typescript.
If an editor needs intellisense and is not provided by default, users can run node scripts/mkclangd.js",2,0
1607,2023-11-03T07:40:18Z,2023-11-03T15:21:44Z,2023-11-03T15:21:44Z,1,3,3,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
1625,2023-11-06T03:37:12Z,2023-11-07T16:20:14Z,2023-11-07T16:20:14Z,11,61,70,"I've seen quite a few user reports that just have an error message like ""ValueError: Unable to instantiate model"" or ""ValueError: Unable to instantiate model: code=34, Numerical result out of range"" when they are e.g. just trying to load a model that has too new of a GGUF version.
The error code is often ERANGE because many floating point operations can set errno. errno really shouldn't be checked unless it is immediately after a failed system call.
I added some printing to cases in llamamodel.cpp that should only be seen when there is a fatal problem loading the model file. This should help actually understand what is happening when there is a failure.
(Note: I haven't tested the changes to the go, Java, and typescript bindings, there could be syntax errors.)",3,0
1629,2023-11-07T19:42:20Z,,2023-11-12T01:54:30Z,8,91,13,"Describe your changes

Edited docker-compose.yaml with the following changes:


New Variable: line 15 replaced bin model with variable ${MODEL_ID}
New volume: line 19 added models folder to place gguf llms



Added env file with instructions on using MODEL_BIN varialbe


created models folder under gpt4all_api with README place holder


Issue ticket number and link
Checklist before requesting a review

[x ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
[x ] I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
[] If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Before:
https://app.screenclip.com/GMfa
After change:
https://app.screenclip.com/Mi3r
Steps to Reproduce
docker compose up --build
Notes
Docker API build is downloading old Bin files which are now deprecated. The new file format is gguf.",1,1
1640,2023-11-11T19:40:09Z,2024-03-09T15:26:40Z,2024-03-09T15:26:40Z,4,58,2,"Added the option to modify the default server port in the application settings
#1635
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Notes
For the configuration to take effect, you need to restart the application",3,0
1641,2023-11-12T04:41:00Z,,2023-11-18T02:16:09Z,13,110,74,"Describe your changes

Edited docker-compose.yaml with the following changes:


New Variable: line 15 replaced bin model with variable ${MODEL_ID}
New volume: line 19 added models folder to place gguf llms


Added env file with instructions on using MODEL_BIN variable
created models folder under gpt4all_api with README place holder
Cleaned up some code in make test folder, specifically the batch completion function test
Added python-dotenv and openai==0.28.0 to requirements
Cleaning up some orphan init.py that don't need to be added to the branch
added *.gguf  exception to gitignore to avoid uploading models during push
using ""venv"" instead of ""env"" for virtual environment

Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
Before:
https://app.screenclip.com/GMfa
After change:
https://app.screenclip.com/Mi3r
Steps to Reproduce
1- docker compose up --build : test gpt4all_api
2- docker compose -f docker-compose.yaml -f docker-compose.gpu.yaml up --build -d : test gpt4all_gpu
3- make test : all tests pass except embedding, This is really outdated and openai new API using ""chat completion"" instead of completion during the inference.  The workaround is to use openai==0.28.0 (edited in requirements.txt)
Notes
Two problems addressed:
A) Docker API build is downloading old Bin files which are now deprecated. The new file format is gguf.
B) API using outdated completion functions in make file during testing. Workaround is to use openai==0.28.0
C) Not able to test embedding successfully
D) Adding variables
API should be functional now from its broken state.",3,3
1648,2023-11-14T15:29:03Z,2023-11-17T16:59:31Z,2023-11-17T16:59:31Z,30,3516,164,,2,0
1651,2023-11-14T23:27:53Z,2024-01-16T19:33:42Z,2024-01-16T19:33:42Z,10,17,57,"The C# bindings haven't been updated since the GGUF changes were merged. I had to remove the model type detection as it worked by checking the magic in the GGML header, but most models are handled by llama.cpp now so it's probably not important. If we want to add this back, it should go through gpt4all-backend API so it does not concern itself with the specifics of the file format.
The C# bindings now check llmodel_model_create2 for failure, and throw a useful exception instead of an access violation:
LLModel ERROR: Could not find any implementations for build variant: default
Unhandled exception. System.Exception: Model format not supported (no matching implementation found)
   at Gpt4All.Gpt4AllModelFactory.CreateModel(String modelPath) in /home/jared/src/forks/gpt4all/gpt4all-bindings/csharp/Gpt4All/Model/Gpt4AllModelFactory.cs:line 42
   at Gpt4All.Gpt4AllModelFactory.LoadModel(String modelPath) in /home/jared/src/forks/gpt4all/gpt4all-bindings/csharp/Gpt4All/Model/Gpt4AllModelFactory.cs:line 61
   at Program.<Main>$(String[] args) in /home/jared/src/forks/gpt4all/gpt4all-bindings/csharp/Gpt4All.Samples/Program.cs:line 13
   at Program.<Main>(String[] args)

This also fixes an issue with finding the implementation .dlls on Windows by making sure the path is absolute. Most users of Dlhandle use an absolute base path and were not affected, but the C# bindings do not call llmodel_set_implementation_search_path and use the default of ""."", which is relative. On *nix, dlopen interprets paths relative to the current working directory, but on Windows they are relative to the search path (which may not include the current working directory).
Fixes #1534",4,3
1658,2023-11-17T18:28:03Z,2023-11-20T15:04:55Z,2023-11-20T15:04:55Z,3,5,8,,2,0
1659,2023-11-18T02:32:46Z,2023-11-21T15:46:52Z,2023-11-21T15:46:52Z,13,110,74,"Describe your changes

Edited docker-compose.yaml with the following changes:
New Variable: line 15 replaced bin model with variable ${MODEL_ID}
New volume: line 19 added models folder to place gguf llms
Added env file with instructions on using MODEL_BIN variable
created models folder under gpt4all_api with README place holder
Cleaned up some code in make test folder, specifically the batch completion function test
Added python-dotenv and openai==0.28.0 to requirements
Cleaning up some orphan init.py that don't need to be added to the branch
added *.gguf exception to gitignore to avoid uploading models during push
using ""venv"" instead of ""env"" for virtual environment
Issue ticket number and link
Refactored Chat Completion and engines endpoints.

Issue ticket number and link
Checklist before requesting a review

[x ] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Before:
https://app.screenclip.com/GMfa
After change:
https://app.screenclip.com/Mi3r
##Steps to Reproduce
1- docker compose up --build : test gpt4all_api
2- docker compose -f docker-compose.yaml -f docker-compose.gpu.yaml up --build -d : test gpt4all_gpu
3- make test : all tests pass except embedding, This is really outdated and openai new API using ""chat completion"" instead of completion during the inference. The workaround is to use openai==0.28.0 (edited in requirements.txt)
Notes
Two problems addressed:
A) Docker API build is downloading old Bin files which are now deprecated. The new file format is gguf.
B) API using outdated completion functions in make file during testing. Workaround is to use openai==0.28.0
C) Not able to test embedding successfully
D) Adding variables
E) All endpoints working:
Engines.py

Added ""requests.get"" function to fetch engines from github list
Better error handling, moving loggers to bottom next to exception requests.
Removed unused libraries

chat.py

Error handling
Added api settings to fetch model to run tests
Added streaming response",4,4
1671,2023-11-21T20:25:32Z,2023-11-21T21:30:18Z,2023-11-21T21:30:18Z,2,51,2,,2,0
1692,2023-11-29T16:36:11Z,,2024-03-12T12:42:00Z,9,162,25,"Describe your changes
I added a new model type similar to OpenAI, but with the possibility to define the API entry point and the model name in order to allow using custom OpenAI compatible servers like those of vllm or llama.cpp
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.

Notes
Model settings are stored in chatgpt-custom.txt which contains 3 lines:

api key (backward compatiblity)
api base url
model name

A future release may use QSettings, Json library or any other to store these values in a dict for safety",4,3
1697,2023-11-30T10:12:57Z,2023-11-30T17:37:52Z,2023-11-30T17:37:52Z,1,1,1,"Describe your changes
Fixed a lil typo in an error message
Issue ticket number and link
n/a
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

n/a
Steps to Reproduce

n/a
Notes",3,2
1706,2023-12-01T18:51:17Z,2023-12-01T21:51:16Z,2023-12-01T21:51:16Z,5,115,65,"NOTE: Requires this llama.cpp branch, which is where most of the changes are: https://github.com/nomic-ai/llama.cpp/tree/update-llamacpp-base
We will have to reset our llama.cpp fork's master branch to that commit before we can properly merge this.
What I did:

Split the Vulkan changes into their own branch: https://github.com/nomic-ai/llama.cpp/tree/vulkan
Incrementally merged the latest llama.cpp into them (up to ggerganov/llama.cpp@6b0a7420), making commits as needed to maintain parity between Metal and Vulkan. This is the current state of the vulkan branch.
Merged the vulkan branch into the GPT4All-specific branch, and made several changes on top of that. This is update-llamacpp-base.
Updated the llama.cpp dependency in GPT4All and made the necessary changes for it to be used. That is this PR.

It seems to run fine with app.py on my RX 7800 XT. 0cc4m tested a slightly older version of the changes to llama.cpp and also had success (after some initial failures that we couldn't reproduce).",2,0
1707,2023-12-01T22:06:47Z,2024-01-10T15:18:31Z,2024-01-10T15:18:31Z,1,20,20,"This PR replaces the following models due to changes in llama.cpp:

GPT4All Falcon
MPT Chat
Replit
Starcoder

These models are fully compatible with upstream llama.cpp. They have all been tested with GPT4All on CPU and GPU (where applicable) and uploaded to S3. This PR assumes the next release will be version 2.6.0. We should hold off on merging this until shortly before/after the release as it will remove the downloads for these models in the old version of GPT4All.
We also need to warn users that the new version of GPT4All will not support any previously downloaded versions of these models.",2,1
1716,2023-12-05T02:04:52Z,,2024-03-10T04:27:05Z,1,18,0,"Following PEP440, adding __version__ variable into initialized package. https://peps.python.org/pep-0440/
Describe your changes
Adding __version__ variable into python __init__ module.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,2
1728,2023-12-07T20:14:13Z,2024-01-17T16:33:24Z,2024-01-17T16:33:24Z,1,3,3,"I actually tried to follow these instructions on an Arch Linux system with minimal graphical components, and realized that I missed a few things.",3,7
1729,2023-12-07T23:31:11Z,,2024-02-16T05:13:16Z,0,0,0,"Describe your changes
Update Makefile for Improved
Added commands to set up and clean up Docker test environment.

Modified test command to use Docker container.
Introduced commands for Docker image deletion and test build.

Implement Streaming in completions.py and Other Updates:

Added a new class and function in completions.py for streaming responses using yield and chunking
Implemented real-time response streaming for the chat feature.
Update .env File for Model

Configuration:

Introduced a new environment variable for model specification.
Clarified model file placement in the project directory.
Added notes on self-retrieving models during inference.
Update README.md with Detailed

Streaming Instructions:

Set up variables and configuration for the OpenAI API in a local server setting.
Detailed creating and printing responses from a ChatCompletion object with streaming.

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Steps to Reproduce
Used completion scripts in readme and curl to verify API function
Chat Completion Streaming
date;curl -X 'POST'   'http://leviathan:4891/v1/chat/completions'   -H 'accept: application/json'   -H 'Content-Type: application/json'   -d @request.json
Thu Dec  7 18:25:53 EST 2023
data: {""choices"": [{""delta"": {""content"": ""\n\nAnd my question is: What are the differences between an LLC""}}]}

data: {""choices"": [{""delta"": {""content"": "" and a corporation in terms of taxation, liability, management""}}]}

Completion Streaming (must use openai 0.28.1)
 curl -X 'POST'   'http://leviathan:4891/v1/completions/'   -H 'accept: application/json'   -H 'Content-Type: application/json'   -d @completion_request.json
data: {""id"": ""c8fc67ac-5439-4555-bef6-309e1b3dd256"", ""object"": ""text_completion"", ""created"": 1701991494, ""model"": ""mistral-7b-instruct-v0.1.Q4_0"", ""choices"": [{""text"": "" 5100 laptop"", ""index"": 0, ""logprobs"": null, ""finish_reason"": """"}]}

data: {""id"": ""c8fc67ac-5439-4555-bef6-309e1b3dd256"", ""object"": ""text_completion"", ""created"": 1701991494, ""model"": ""mistral-7b-instruct-v0.1.Q4_0"", ""choices"": [{""text"": ""\n\ni am looking"", ""index"": 0, ""logprobs"": null, ""finish_reason"": """"}]}

data: {""id"": ""c8fc67ac-5439-4555-bef6-309e1b3dd256"", ""object"": ""text_completion"", ""created"": 1701991494, ""model"": ""mistral-7b-instruct-v0.1.Q4_0"", ""choices"": [{""text"": "" for a new"", ""index"": 0, ""logprobs"": null, ""finish_reason"": """"}]}

data: {""id"": ""c8fc67ac-5439-4555-bef6-309e1b3dd256"", ""object"": ""text_completion"", ""created"": 1701991494, ""model"": ""mistral-7b-instruct-v0.1.Q4_0"", ""choices"": [{""text"": "" notebook that"", ""index"": 0, ""logprobs"": null, ""finish_reason"": """"}]}

data: {""id"": ""c8fc67ac-5439-4555-bef6-309e1b3dd256"", ""object"": ""text_completion"", ""created"": 1701991494, ""model"": ""mistral-7b-instruct-v0.1.Q4_0"", ""choices"": [{""text"": "" will serve"", ""index"": 0, ""logprobs"": null, ""finish_reason"": """"}]}

Notes

Moved env file to ./gpt4all/gpt4all-api
Removed GPU support from make for now, since it is using an external project (huggingface TGI)
Updated Readme files, still a work in progress
Embedding needs to be updated still",3,2
1737,2023-12-10T16:47:28Z,,2024-03-19T14:37:14Z,8,209,66,"Describe your changes
I finished the API side for chat completion, embeddings generator. Also made a script for starting container (start.sh). I am working now to make a web interface and will add soon this here. Also need to add a security layer and api tokens manager but later. I want to be active and help the project.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes

Please consider metioning our company as contributos: https://hypen.ro -> office@hypen.ro Thanks!",3,3
1742,2023-12-11T17:20:16Z,2023-12-12T16:45:03Z,2023-12-12T16:45:03Z,3,18,9,"Check the size of the KV cache state before we attempt to load it because it may be different between llama.cpp versions. Fall back to restoring the state from text in that case.
Fixes this assertion failure when loading the KV cache state from GPT4All v2.5.4 with the latest master:
GGML_ASSERT: /home/jared/src/forks/gpt4all/gpt4all-backend/llama.cpp-mainline/llama.cpp:9352: kv_self.buf.size == kv_buf_size",2,0
1746,2023-12-11T23:44:59Z,2023-12-15T18:44:40Z,2023-12-15T18:44:40Z,10,166,653,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes

removes false warning positive
fixes mac m1 cannot find dynamic libraries #1631
add some extra doc on listGpu
added extra readme information",2,0
1749,2023-12-12T17:31:15Z,2023-12-16T22:58:15Z,2023-12-16T22:58:15Z,31,291,135,"Tested working with the python bindings and the GUI. The other bindings are still hardcoded to 2048 but it shouldn't be hard to expose the context length via their APIs if desired.
For the python bindings, this is the n_ctx parameter of the GPT4All constructor:
class GPT4All:
	# ...

    def __init__(
        self,
        # ...
        n_ctx: int = 2048,
        verbose: bool = False,
    ):
In the UI, this is a per-model parameter:

This doesn't take effect until switching models or restarting. This fact is noted in the tooltip. For now, this is the simplest way to do it, although IMO it would be nice to have a way to reload the model in the future (similar TGWUI's ""Reload"" button on the model tab).",3,0
1750,2023-12-13T03:03:54Z,2023-12-13T17:11:10Z,2023-12-13T17:11:10Z,6,36,42,"Isolate the AVX2 libraries from the platform-independent ones again by loading llamamodel-mainline-*.so when we need to list the available GPU devices. (This does assume we only try to keep one version of llama.cpp around - hopefully we plan to keep it that way?)
Confirmed working on my AVX2 machine with an AMD GPU - the chat UI starts for once, and the GPU is correctly displayed in the list of devices.
I'm not sure why chat was directly linking to Bert because it builds fine without that link, even with -Wl,--no-undefined.
I also removed ""llama"" from the library regex since there is no matching library - the closest one is llamamodel-mainline, which is already listed.
Fixes #1674",2,0
1756,2023-12-15T07:29:36Z,2024-01-03T14:41:40Z,2024-01-03T14:41:40Z,4,8,16,"Describe your changes
Just some code cleanups
Issue ticket number and link
Checklist before requesting a review

[ X] I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,0
1763,2023-12-18T09:35:09Z,2023-12-18T19:06:25Z,2023-12-18T19:06:25Z,1,2,2,"""created"" is supposed to be an int64, not a string. Returning 0 makes them valid, though still useless. Addresses issue #1353",2,0
1769,2023-12-20T19:42:31Z,2023-12-20T21:01:03Z,2023-12-20T21:01:03Z,1,5,1,"Describe your changes
Adding a typer option for the device parameter in the GPT4ALL class by modifying the REPL function. This allows to choose cpu, cuda and GPU per https://docs.gpt4all.io/gpt4all_python.html#api-documentation
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
[] If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Pull the latest code from the branch.
Run the app.py script using the command: python app.py repl --device=gpu
Observe the model running on the specified GPU device.",2,0
1793,2023-12-29T23:04:41Z,2024-01-03T17:13:07Z,2024-01-03T17:13:07Z,1,5,1,"Fix for ""LLModel ERROR: Could not find CPU LLaMA implementation""
Solution provided by @NotArandomGUY
Describe your changes
Inspired by Microsoft docs for LoadLibraryExA (https://learn.microsoft.com/en-us/windows/win32/api/libloaderapi/nf-libloaderapi-loadlibraryexa). When using LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR, the lpFileName parameter must specify a fully qualified path, also it needs to be backslashes (), not forward slashes (/).
Issue ticket number and link
Fixes #1760
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",4,0
1808,2024-01-03T17:28:46Z,2024-01-03T19:06:08Z,2024-01-03T19:06:08Z,1,2,0,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1819,2024-01-08T20:09:18Z,2024-01-16T21:36:21Z,2024-01-16T21:36:21Z,4,47,46,"This updates llama.cpp to the commit immediately before per-layer KV cache, and includes the latest changes from the Vulkan PR.
Here is the llama.cpp diff: ggerganov/llama.cpp@d103d93...81bc921
Here is the Vulkan PR: ggerganov/llama.cpp#4456
The output matches with the llama.cpp 'main' example. I haven't tested with GPT4All itself.",3,5
1829,2024-01-12T21:29:45Z,2024-01-17T17:41:53Z,2024-01-17T17:41:53Z,2,29,9,"This should avoid the need for dummy commits like this f856439
when I make a change like this b964066",2,0
1859,2024-01-21T23:04:01Z,2024-01-22T15:01:31Z,2024-01-22T15:01:31Z,1,2,2,"I wrote >= 6 when I meant either > 6 or >= 7. Change it to the latter, which is what makes the most sense to me (n_ctx only applies to versions 7 and above).
This was causing a crash when loading chat files from versions of GPT4All prior to v2.6, at least when deserializeKV is true (KV cache saved to disk).
Fixes #1846 (symptom is a std::bad_alloc because tokensSize is too large, thanks to @ShadowsArt for helping debug this over Discord)",2,0
1871,2024-01-24T18:03:31Z,2024-01-24T23:00:49Z,2024-01-24T23:00:49Z,1,5,0,"Describe your changes
make QT and related packages installation easier for Fedora users who are building gpt4all-chat from source
Checklist before requesting a review

 I have performed a self-review of my code.",2,0
1874,2024-01-25T22:21:06Z,2024-01-29T18:57:42Z,2024-01-29T18:57:42Z,1,2,2,,3,0
1876,2024-01-26T16:32:09Z,2024-01-29T18:02:51Z,2024-01-29T18:02:51Z,31,1201,487,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,5
1897,2024-01-31T22:58:56Z,2024-02-24T22:50:14Z,2024-02-24T22:50:14Z,14,1526,964,"Describe your changes
Added streaming and Async generator and also correct token count for response tokens
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

The following was streamed token by token
Response:  I'm doing well, thank you for asking! How can I help you today?
Steps to Reproduce

import gpt from '../src/gpt4all.js'

const model = await gpt.loadModel(""mistral-7b-openorca.Q4_0.gguf"")  

process.stdout.write('Response: ')

for await (const token of gpt.generateTokens(model,[{
    role: 'user',
    content: ""How are you ?""
  }],{nPredict: 2048 })){
    process.stdout.write(token);
  }

model.dispose();
Notes",3,0
1911,2024-02-02T17:46:16Z,,2024-02-02T22:53:58Z,1,2,2,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,0
1912,2024-02-02T18:28:05Z,2024-02-07T14:56:25Z,2024-02-07T14:56:25Z,1,1,9,"Many of the formats we were attempting to read either were not plaintext, or were plaintext but not plain English. A conservative list of extensions is less likely to cause user confusion.",2,0
1924,2024-02-04T10:22:35Z,2024-02-04T17:04:58Z,2024-02-04T17:04:58Z,1,1,1,"Describe your changes
The readme recommends changing the files in ""api"" directory, which does not exists. The correct folder is ""app"".
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Steps to Reproduce
Open nomic-ai/gpt4all/gpt4all-api/README.md and
 and read ""and edit files in the api directory."" paragraph.",2,0
1928,2024-02-05T09:30:30Z,2024-03-12T12:42:22Z,2024-03-12T12:42:22Z,4,7,7,"Describe your changes
All Qt components required by gpt4all-chat already exist in Qt 6.4, there is no need to require a minimum version of Qt 6.5. This also facilitates users to compile source code in Debian and its derivative distributions, because Qt shipped in their repositories is still 6.4.2
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",3,1
1930,2024-02-05T19:36:14Z,,2024-02-07T14:54:54Z,1,4,4,"Smaller startup main window size.
Smaller main window minimum size limits.
Trying to make GPT4All work nicer with window tiling tools that typically use 1/3 or 1/4 screen slots.
I didn't go through and check other component sizing as I don't have Qt installed.
As a user, I would be fine expanding the window out further to deal with settings.
But most of the work is going to be done in the chat interface, which should just wrap the text for a narrower window.
Feel free to modify this. I just wanted to throw out a change to make an easy starting point.
Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
1931,2024-02-05T21:26:34Z,2024-02-06T16:01:15Z,2024-02-06T16:01:15Z,8,79,112,"* backend: update llama.cpp for Mixtral crash fix
From nomic-ai/llama.cpp@315102f:

We haven't implemented the necessary GPU kernels yet.
Fixes this crash:
ggml_vk_graph_compute: error: unsupported op 'ARGSORT'
GGML_ASSERT: /home/jared/src/forks/gpt4all/gpt4all-backend/llama.cpp-mainline/ggml-kompute.cpp:1508: !""unsupported op""

* python: release bindings version 2.2.0
Among other improvements, we have added support for many new models on CPU because we were able to update our llama.cpp dependency, so it is time to make a new release of the bindings.",3,1
1946,2024-02-08T21:00:49Z,2024-02-26T18:09:01Z,2024-02-26T18:09:01Z,1,3,3,"ref https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/7dbbc90392e2f80f3d3c277d6e90027e55de9125/config.json#L13
This is a quick-and-dirty fix since this code is going to be replaced anyway. It would be more correct to read layer_norm_eps when we convert to GGUF, and load that hyperparameter from the GGUF at inference time.
The difference between an epsilon of 1e-6 in LLaMA 1 and 1e-5 in LLaMA-2 created a significant difference in perplexity, so they implemented this parameter to ggml_norm and ggml_rms_norm soon after LLaMA-2 came out, and until the switch to GGUF they defaulted to 5e-6, which was a suitable middleground, and allowed the user to customize the parameter at inference time via a command-line option.
The difference between 1e-5 and 1e-12 is certainly more significant... if only we had benchmarks for this code.",2,0
1953,2024-02-09T22:18:49Z,2024-02-10T02:40:32Z,2024-02-10T02:40:33Z,5,109,119,"The CI built version 2.2.0 of the python bindings without Kompute because it was not using --recursive on git submodule update, and Kompute is now a submodule of llama.cpp.
To make GPT4All more robust to this problem:

Fail the build if Kompute is missing instead of warning

Allow users to pass -DLLAMA_KOMPUTE=OFF to explicitly build without Kompute


Print a message on stderr if the bindings attempt to enumerate GPU devices without Kompute being built in",2,0
1964,2024-02-13T01:00:50Z,2024-02-13T22:44:33Z,2024-02-13T22:44:33Z,1,15,2,"Add project links and the README file to the Python package that gets published to PyPI.
Refs:

#1963",2,2
1969,2024-02-15T18:36:44Z,2024-02-21T16:15:20Z,2024-02-21T16:15:21Z,17,515,212,Large revamp of model loading.,4,20
1970,2024-02-15T21:00:37Z,2024-02-21T20:45:32Z,2024-02-21T20:45:32Z,23,686,307,"Here is an example of the kind of broken output this PR attempts to fix:


PR #1935 switched Mistral OpenOrca to the official prompt template (ChatML). But users have been seeing <|im_end|> in the output since this change. There are two problems:

The current GGUF file was converted 5 months ago by TheBloke. It is out-of-date and does not contain the special tokens.
We were still using llama_tokenize with special=false. This was changed to true (which has some caveats).

Now the model is actually seeing tokens 32001 (<|im_start|>) and 32000 (<|im_end|>):

Token output from llama_tokenize

Before:
523:  <
28766: |
321: im
28730: _
2521: start
28766: |
28767: >
1838: user
13:

28708: a
28789: <
28766: |
321: im
28730: _
416: end
28766: |
3409: ><
28766: |
321: im
28730: _
2521: start
28766: |
28767: >
489: ass
11143: istant
13:


After:
Token debug:
32001: <|im_start|>
1838: user
13:

28708: a
32000: <|im_end|>
32001: <|im_start|>
489: ass
11143: istant
13:



One limitation of calling llama_tokenize with special=true is that we can have false-positive special tokens, e.g. if the user input (or LocalDocs context) contains the strings <s> or </s>, which will be interpreted as BOS and EOS, respectively. The proper fix for this is to be able to specify raw token IDs in the prompt template so we can use special=false, but I don't know what the UI for this would look like.
Also, I haven't checked what happens to the EOS token (<|im_end|>) in history - for ChatML to work properly, this must come after the assistant's responses.",4,9
1973,2024-02-16T15:53:17Z,2024-02-26T18:15:02Z,2024-02-26T18:15:02Z,2,11,3,"importlib.resources.files also didn't exist until python 3.9. Use the importlib_resources package on python 3.8.
Fixes #1972",2,0
1979,2024-02-18T21:11:08Z,2024-03-11T14:58:47Z,2024-03-11T14:58:47Z,2,52,8,"Describe your changes
The /v1/chat/completions endpoint was not implemented in gpt4all-api (only returned an ""Echo"" of the original message, as mentioned in #1700). This PR implements the endpoint for CPU mode and adds an appropriate test.
Issue ticket number and link
#1700
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo
try the openai.ChatCompletion.create() function (as described here: https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)
import openai

openai.api_base=""http://localhost:4891/v1""
openai.api_key=""ABCDEFG""

stream = openai.ChatCompletion.create(
    model=""mistral-7b-openorca.Q4_0.gguf"",
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Knock knock.""},
        {""role"": ""assistant"", ""content"": ""Who's there?""},
        {""role"": ""user"", ""content"": ""Orange.""},
        ],
       temperature=0
)
print(stream.choices[0].message.content)
Before the PR, we would just get an ""Echo: "" of the last message. After the PR, the we actually get a result.
Steps to Reproduce

see test_chat_completion() in gpt4all_api/app/tests/test_endpoints.py

Notes

GPU support will need to be added later. Worth merging anyway IMHO, as it actually adds functionality.",3,0
1989,2024-02-21T16:41:16Z,2024-02-21T17:54:26Z,2024-02-21T17:54:26Z,1,8,0,Simple solution to try and fix issue #1905,2,1
1996,2024-02-21T21:38:40Z,2024-02-26T18:11:15Z,2024-02-26T18:11:15Z,5,20,15,"Before, the model was seeing this for Alpaca-style chat sessions:
### User:
Question?
### Response:
Answer###User:
Follow-up question
### Response:

With this PR, the model sees this:
### User:
Question?
### Response:
Answer

### User:
Follow-up question
### Response:",4,7
2003,2024-02-22T17:52:11Z,2024-02-22T21:51:56Z,2024-02-22T21:51:56Z,2,17,16,,2,0
2006,2024-02-22T22:20:33Z,2024-03-08T22:18:38Z,2024-03-08T22:18:38Z,2,18,1,"Building on ggerganov/llama.cpp#4978 and ggerganov/llama.cpp#5650, I was finally able to implement a version of ggerganov/llama.cpp#3626 that upstream was satisfied by in ggerganov/llama.cpp#5670.
Now MPT Chat has gone from 3.64 GiB to 3.54 GiB on disk, without breaking upstream compatibility in either direction.",2,1
2011,2024-02-23T19:21:16Z,2024-02-26T18:09:29Z,2024-02-26T18:09:29Z,1,1,1,"s/contextLength/gpuLayers/
Fixes #2010",2,0
2014,2024-02-23T23:07:26Z,2024-02-24T22:51:35Z,2024-02-24T22:51:35Z,28,176,14,"Describe your changes
Added the code necessary to expose and support min_p sampling in the UI.
Basically everywhere top_p was, I duplicated the code and renamed top_p to min_p.
Will followup bindings and api in a later commit.
Issue ticket number and link
#1657 #1657
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.",2,1
2015,2024-02-23T23:16:42Z,2024-02-26T16:47:11Z,2024-02-26T16:47:11Z,1,21,21,I successfully used this change to build the offline installers here: https://app.circleci.com/pipelines/github/nomic-ai/gpt4all/1960/workflows/4d1eabf9-03a7-4c7e-9326-08538b6c1e7e,2,0
2023,2024-02-24T22:06:02Z,2024-02-26T18:11:51Z,2024-02-26T18:11:51Z,1,3,1,"mypy found a missing function argument, also complains about __func__ which we can silence because it's a false positive.",2,0
2027,2024-02-25T16:44:30Z,2024-02-27T14:28:43Z,2024-02-27T14:28:44Z,1,21,5,"Adds Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf, which is the new 7b flagship model of NousResearch, to models3.json.
Original Model location:
https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF
Model description:
Nous Hermes 2 on Mistral 7B DPO is the new flagship 7B Hermes! This model was DPO'd from Teknium/OpenHermes-2.5-Mistral-7B and has improved across the board on all benchmarks tested - AGIEval, BigBench Reasoning, GPT4All, and TruthfulQA.
The model prior to DPO was trained on 1,000,000 instructions/chats of GPT-4 quality or better, primarily synthetic data as well as other high quality datasets, available from the repository teknium/OpenHermes-2.5.
Original Dataset Location:
https://huggingface.co/datasets/teknium/OpenHermes-2.5
Dataset description:
This is the dataset that made OpenHermes 2.5 and Nous Hermes 2 series of models.
The Open Hermes 2/2.5 and Nous Hermes 2 models have made significant advancements of SOTA LLM's over recent months, and are underpinned by this exact compilation and curation of many open source datasets and custom created synthetic datasets.
The Open Hermes 2.5 dataset is a continuation of the Open Hermes 1 dataset, at a much larger scale, much more diverse, and much higher quality compilation, reaching 1M, primarily synthetically generated instruction and chat samples.
Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Screenshots


Notes

I inspected the OpenHermes-2.5 dataset, which to 99.9% does not contain any system prompts, so this is a model that should work really well with various system prompts.
sideloaded and tested a little (nothing extensive) on Windows 10 with Nvidia 1060 3GB and AMD Ryzen 5 5600 with GPT4All 2.7.1",3,1
2031,2024-02-26T13:45:05Z,2024-02-26T18:04:16Z,2024-02-26T18:04:16Z,2,3,1,,2,0
2034,2024-02-26T23:33:32Z,2024-03-06T22:14:54Z,2024-03-06T22:14:55Z,8,33,10,"It's simple but it does the job. The popup is easily dismissed by clicking basically anywhere, so it's not too intrusive.",2,0
2043,2024-02-27T19:58:50Z,2024-03-06T21:42:59Z,2024-03-06T21:42:59Z,5,9,8,"There were some harmless warnings at shutdown with the previous code. This version of the thread joining does things a little more cleanly so the warnings don't appear.
Fixes #1921",2,0
2044,2024-02-27T20:24:34Z,2024-03-06T19:22:09Z,2024-03-06T19:22:09Z,1,3,4,"I stashed these changes last Thursday but forgot to apply them. They are necessary for proper functioning of the new prompt template syntax with the python bindings, which will look like user_prefix {0} user_suffix assistant_prefix {1} assistant_suffix.",2,0
2045,2024-02-27T21:34:46Z,2024-03-28T16:08:24Z,2024-03-28T16:08:24Z,33,2527,1303,"Describe your changes

tokenStream published
chat sessions
number gpu layers (ngl) config option
nCtx config option
run multiple models concurrently

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Notes
This will feature some breaking changes to future proof and remove some of technical debt trying to adhere to the open ai spec.",4,7
2051,2024-02-28T18:01:25Z,2024-03-06T19:02:18Z,2024-03-06T19:02:18Z,2,31,6,"I'm actually not sure how certain parts of this were working before #1970, namely:

processRestoreStateFromText() - My understanding is that the LLModel would generate a new reply to both the original prompt and the original reply, because we called LLModel::prompt with the default n_predict (not zero). We were simply not displaying the reply (empty callback), even though it ended up in the LLModel's conversation history (m_context in the case of ChatGPT). I added fakeReply in PR 1970 because the prompt and response now need to happen within the same LLModel::prompt invocation, but as a bonus it fixes this apparent issue.
For ChatGPT, decrementing n_past to regenerate the reply meant the initial new reply would be based on only the previous context, but we never actually removed anything from m_context, so any further replies would see old messages. Now we resize m_context as needed.",2,2
2054,2024-02-28T22:40:35Z,2024-03-01T19:19:18Z,2024-03-01T19:19:18Z,1,2,2,"Describe your changes

Adds ""accepts various system prompts""
Removes system prompt
fix whitespace

Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Notes
Reason for removing the system prompt:

The current system prompt is suboptimal.
The dataset barely contains system prompts,
Rudimentary small tests show that various system prompts work with this model without causing extreme ""bleeding"" into the responses, but since the tests were not extensive, I would like to go the ""safe"" route and remove the given system prompt, as it works very well without system prompt.

Co-authored-by: 3Simplex 10260755+3Simplex@users.noreply.github.com""",2,0
2063,2024-03-01T22:40:42Z,2024-03-06T19:12:21Z,2024-03-06T19:12:21Z,2,19,24,"A simple change that will be useful when we remove support for the SBert model that we are currently hosting. I've already made sure that the model will not be shown in the chat model list, or used for localdocs.
I think the name ""removedIn"" is more clear as it explicitly indicates that the model will not be listed, and if it was removed in version 2.8.0 and we are on version 2.8.0, we obviously shouldn't show the model.",2,0
2081,2024-03-06T18:01:33Z,2024-03-06T22:52:18Z,2024-03-06T22:52:18Z,5,14,1,,2,0
2086,2024-03-07T17:22:40Z,2024-03-13T22:09:24Z,2024-03-13T22:09:24Z,23,797,1196,"Backend changes

Removed bert.cpp
Implemented LLamaModel::embed on top of new llama.cpp BERT implementation, which intelligently splits and batches inputs depending on length and n_ctx
Added BERT and Nomic BERT to llama.cpp arch whitelist
Explicitly blacklist old MiniLM GGUF
Dynamic supportsCompletion/supportsEmbedding based on model arch
Implemented compatibility with ggerganov/llama.cpp#5796
Implemented Matryoshka dimensionality reduction for nomic-embed-text-v1.5
Set n_batch to trained context length for embedding models
Implemented compatibility with Nomic Atlas API:

Built-in support for prefix to add to each embedding input
Hardcoded map of model names to query/storage prefixes
Change overlap to 8 tokens
Use the ""nomic empty"" placeholder when embedding an empty string
Add a mode where the input is truncated instead of averaging the embeddings
Optional length limits that match Atlas
L2 normalize before and after averaging embeddings


Return proper error strings or print warnings in certain cases instead of opaque failure or incorrect results:

Error when setting dimensionality on a model that isn't nomic-embed-text-v1.5
Error when setting dimensionality to larger than n_embd
Error when using a prefix that is not known to the model (if in hardcoded map)
Error when n_ctx - prefix is smaller than overlap
Warn when text tokenizes to an empty result


Added MiniLM, Nomic Embed v1 and v1.5 to models3.json
Implement embeddingModel key in models3.json so they can be identified before downloading

Python bindings changes

Embed4All now accepts a list of texts to embed
Added prefix, domensionality, do_mean, and atlas parameters to Embed4All
Dimensionality argument is pre-checked for sanity

Chat UI changes

Identify embedding models based on GGUF metadata, and remove special-casing of MiniLM GGUF
Use appropriate task type automatically in the chat UI, for any known embedding model
Restricted to use MiniLM for local embeddings for now, but the whitelist can easily be removed

TODO

 Update models3.json based on what GPT4All release this will be included in
 Add example usage to python docs
 Implement Nomic python client integration with local/dynamic mode",2,0
2089,2024-03-07T19:28:11Z,2024-03-08T15:44:20Z,2024-03-08T15:44:21Z,3,212,184,,3,0
2090,2024-03-07T19:38:35Z,2024-03-08T21:55:16Z,2024-03-08T21:55:16Z,1,2,1,,2,2
2098,2024-03-09T15:04:03Z,2024-03-11T17:52:24Z,2024-03-11T17:52:24Z,3,19,22,…ally deletes.,5,8
2099,2024-03-09T23:42:46Z,2024-03-10T14:26:38Z,2024-03-10T14:26:38Z,1,2,1,"…x to the currently selected chat model
Describe your changes
As described in issue #2024 the model/character settings does not default to the currently selected chat model when opened. This commit addresses that.
Issue ticket number and link
#2024
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Steps to Reproduce
Described in #2024",2,1
2106,2024-03-11T15:01:16Z,2024-03-11T16:05:49Z,2024-03-11T16:05:49Z,1,3,0,,2,0
2107,2024-03-11T16:28:13Z,2024-03-11T17:54:39Z,2024-03-11T17:54:39Z,1,2,2,,3,1
2108,2024-03-11T22:55:10Z,2024-03-12T00:36:18Z,2024-03-12T00:36:18Z,1,61,0,"Describe your changes
Added context menus for the prompt (Cut, Copy, Paste, Select All) and for the responses (Copy, Select All)
Issue ticket number and link
Closes #1966
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

  
    
    

    gpt4all-contextmenu.mp4",2,0
2117,2024-03-13T12:54:41Z,2024-03-13T19:23:04Z,2024-03-13T19:23:04Z,5,56,51,,2,0
2118,2024-03-13T19:22:53Z,2024-03-13T22:01:19Z,2024-03-13T22:01:19Z,1,4,1,,2,0
2121,2024-03-13T23:58:17Z,2024-03-14T14:42:23Z,2024-03-14T14:42:23Z,3,1458,1430,"This is a non-functional change that just splits the main.qml into two pieces. The point is to support multiple views in a stack in main.qml in the future, but right now all it does is load the one view we have: ChatView.qml",2,0
2125,2024-03-14T15:01:30Z,2024-03-14T16:06:07Z,2024-03-14T16:06:07Z,1,4,2,"I tried to use a C++20 feature that apparently even the latest Apple clang does not yet support.
Fixes this build failure on macOS:
[ 67%] Building CXX object llmodel/CMakeFiles/llamamodel-mainline-default.dir/llamamodel.cpp.o
In file included from /Users/jared/src/forks/gpt4all/gpt4all-backend/llamamodel.cpp:2:
In file included from /Users/jared/src/forks/gpt4all/gpt4all-backend/llamamodel_impl.h:7:
In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/functional:526:
In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__functional/boyer_moore_searcher.h:22:
In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__memory/shared_ptr.h:22:
In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__memory/allocation_guard.h:15:
/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__memory/allocator_traits.h:304:9: error: no matching function for call to 'construct_at'
        _VSTD::construct_at(__p, _VSTD::forward<_Args>(__args)...);
        ^~~~~~~~~~~~~~~~~~~
/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__config:897:17: note: expanded from macro '_VSTD'
#  define _VSTD std
                ^
/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/vector:919:21: note: in instantiation of function template specialization 'std::allocator_traits<std::allocator<split_batch>>::construct<split_batch, unsigned int &, std::vector<int> &, void, void>' requested here
    __alloc_traits::construct(this->__alloc(), std::__to_address(__tx.__pos_),
                    ^
/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/vector:1678:9: note: in instantiation of function template specialization 'std::vector<split_batch>::__construct_one_at_end<unsigned int &, std::vector<int> &>' requested here
        __construct_one_at_end(std::forward<_Args>(__args)...);
        ^
/Users/jared/src/forks/gpt4all/gpt4all-backend/llamamodel.cpp:801:35: note: in instantiation of function template specialization 'std::vector<split_batch>::emplace_back<unsigned int &, std::vector<int> &>' requested here
            auto &batch = batches.emplace_back(i, prefixTokens).batch;
                                  ^
/Library/Developer/CommandLineTools/SDKs/MacOSX14.4.sdk/usr/include/c++/v1/__memory/construct_at.h:39:38: note: candidate template ignored: substitution failure [with _Tp = split_batch, _Args = <unsigned int &, std::vector<int> &>]: no matching constructor for initialization of 'split_batch'
_LIBCPP_HIDE_FROM_ABI constexpr _Tp* construct_at(_Tp* __location, _Args&&... __args) {
                                     ^
1 error generated.",2,0
2127,2024-03-14T16:00:17Z,2024-03-20T15:16:25Z,2024-03-20T15:16:25Z,1,16,0,"I have added the Ghost 7B v0.9.1 model to the support list.
Nice to see it will be merged and officially used soon.
Model at HuggingFace:

Base: https://huggingface.co/lamhieu/ghost-7b-v0.9.1
GGUF: https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf

Thanks for review !",3,3
2129,2024-03-14T22:34:11Z,2024-03-19T21:25:22Z,2024-03-19T21:25:22Z,9,300,251,,4,2
2130,2024-03-14T22:38:10Z,2024-03-15T15:49:58Z,2024-03-15T15:49:58Z,7,132,148,"See the commits for more info. I put the word ""breaking"" in the title of changes that change the API in some way and may need further scrutiny.",3,4
2136,2024-03-17T18:47:54Z,2024-03-18T16:57:56Z,2024-03-18T16:57:56Z,1,4,1,"Describe your changes
This commit allow changing the install path during CMake configure step using the CMAKE_INSTALL_PREFIX variable. If the variable is not set, it still defaults to {CMAKE_BINARY_DIR}/install.
Issue ticket number and link
#2135
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,0
2138,2024-03-18T03:05:00Z,2024-03-18T13:34:18Z,2024-03-18T13:34:18Z,1,4,0,Adds Phorm AI Badge to Readme,2,0
2141,2024-03-18T22:45:35Z,2024-03-19T14:56:15Z,2024-03-19T14:56:15Z,6,79,84,"Key Changes

Fix a missing defined(_M_X64) that caused the ""you don't have AVX"" screen to not appear on Windows
Refactor CPU feature detection so that can't happen anymore
Improve the way we report this to the bindings

Chat
This message can now be shown on Windows. This screenshot is only an example, I haven't actually tested this on a Windows machine without AVX:

Bindings
Before (exception is generic and misleading, actual error is hidden above the traceback):
LLModel ERROR: CPU does not support AVX
Traceback (most recent call last):
  File ""/home/jared/src/own/gpt4all-scripts/test_any.py"", line 18, in <module>
    main()
  File ""/home/jared/src/own/gpt4all-scripts/test_any.py"", line 10, in main
    x = GPT4All(model_name=path.name, model_path=path.parent, allow_download=False, device=""Tesla P40"")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/src/forks/gpt4all/gpt4all-bindings/python/gpt4all/gpt4all.py"", line 132, in __init__
    self.model = _pyllmodel.LLModel(self.config[""path""], n_ctx, ngl)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/src/forks/gpt4all/gpt4all-bindings/python/gpt4all/_pyllmodel.py"", line 188, in __init__
    raise ValueError(f""Unable to instantiate model: {'null' if s is None else s.decode()}"")
ValueError: Unable to instantiate model: Model format not supported (no matching implementation found)

After:
Traceback (most recent call last):
  File ""/home/jared/src/own/gpt4all-scripts/test_any.py"", line 18, in <module>
    main()
  File ""/home/jared/src/own/gpt4all-scripts/test_any.py"", line 10, in main
    x = GPT4All(model_name=path.name, model_path=path.parent, allow_download=False, device=""Tesla P40"")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/src/forks/gpt4all/gpt4all-bindings/python/gpt4all/gpt4all.py"", line 132, in __init__
    self.model = _pyllmodel.LLModel(self.config[""path""], n_ctx, ngl)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jared/src/forks/gpt4all/gpt4all-bindings/python/gpt4all/_pyllmodel.py"", line 188, in __init__
    raise RuntimeError(f""Unable to instantiate model: {'null' if s is None else s.decode()}"")
RuntimeError: Unable to instantiate model: CPU does not support AVX",2,0
2142,2024-03-19T15:00:48Z,2024-03-19T16:20:53Z,2024-03-19T16:20:53Z,1,1,1,"Describe your changes
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo

Steps to Reproduce

Notes",2,1
2145,2024-03-19T22:47:20Z,2024-03-20T15:24:02Z,2024-03-20T15:24:02Z,11,105,52,"We need token counts to be compatible with the Atlas API
models2.json doesn't have Nomic Embed, use models3.json
Fix missing CLS token with Bert and Nomic Bert since #2130",2,0
2148,2024-03-20T15:19:39Z,2024-03-20T16:45:26Z,2024-03-20T16:45:26Z,10,492,382,,2,0
2150,2024-03-20T16:57:11Z,2024-03-20T20:10:26Z,2024-03-20T20:10:26Z,1,7,1,The server colors are messed up right now for some themes. This hardcodes the background and text colors for server that distinguishes it from all other chats and works for all themes.,2,0
2152,2024-03-20T22:06:12Z,2024-03-21T15:33:41Z,2024-03-21T15:33:41Z,3,92,33,"Write downloads to a file ending .part until they are complete, verified, and synced to disk
Verify file size and MD5 hash against models3.json, and delete the file if they don't match
Content-Encoding must be identity for resumed downloads because of how Range requests work

This hopefully prevents incomplete downloads from making it through to llama.cpp, which tends to crash hard when that happens.
Related to #2128",2,0
2162,2024-03-25T16:24:14Z,2024-03-26T15:01:02Z,2024-03-26T15:01:02Z,1,1,1,"Before this PR, attempting to load a file called 😊.gguf on Windows crashes GPT4All.
After this PR, it loads successfully.
The upstream change is ggerganov/llama.cpp#6248. It has simply been cherry-picked to avoid potentially breaking anything.
Related to #2111",2,0
2163,2024-03-25T20:57:23Z,2024-03-26T15:04:23Z,2024-03-26T15:04:23Z,1,1,1,"Depends on #2162 because that's the order in which I committed these to the fork, so I'm leaving this as draft until that one gets merged.
Fixes #2160",2,0
2164,2024-03-25T21:35:57Z,2024-03-26T05:16:50Z,2024-03-26T05:16:50Z,2,9,2,Fixes #2159,2,0
2172,2024-03-26T19:20:07Z,2024-03-27T15:03:10Z,2024-03-27T15:03:10Z,1,3,3,"This change is necessary to allow documentation pages to be renamed or deleted - otherwise, they still show up under the old name.
I have no easy way of testing this (e.g. locally with --dryrun) because my AWS account does not have permission to access the s3://docs.gpt4all.io/ bucket, so we'll just have to hope this doesn't break anything.",2,0
2177,2024-03-28T18:49:34Z,2024-03-28T20:48:07Z,2024-03-28T20:48:07Z,3,57,4,"This allows you to do this:
from gpt4all import GPT4All
model = GPT4All('orca-mini-3b-gguf2-q4_0.gguf')
model.generate(...)
model.close()
or even this:
from gpt4all import GPT4All
with GPT4All('orca-mini-3b-gguf2-q4_0.gguf') as model:
    model.generate(...)
In order to free resources after using a GPT4All or Embed4All instance. This will be important for the nomic client integration, which is expected to automatically (re-)create Embed4All instances as needed, and ideally it would explicitly avoid using up all system RAM or VRAM in the process.",2,0
2180,2024-03-29T20:20:07Z,2024-04-01T13:34:02Z,2024-04-01T13:34:02Z,1,5,0,"Describe your changes
This sets the app icon properly so that when built on MacOS, the app icon shows as intended. While the favicon.icns file appears to be specified in some places, it's necessary to include it in the Resources folder and the appropriate executable manifest for it to actually be used on MacOS.
Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.

I am unable to apply any labels myself. It would probably be an Enhancement


 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Demo




Before fix
With fix in place




Dock




Icon





Steps to Reproduce
Build the app and check the build output folder (e.g. build-gpt4all-chat-Desktop_arm_darwin_generic_mach_o_64bit-Release/bin) to view the app icon. It should be the GPT4All icon from favicon.icns. After applying the fix, it may be necessary to manually remove the build output dir and run Clean on the project to handle caching around the app icon.
Notes",2,0
2183,2024-03-30T18:28:23Z,2024-04-01T13:34:50Z,2024-04-01T13:34:50Z,1,26,0,"Include links for Documentation and FAQ for new users on the ""new chat view"".
Describe your changes
Before:

After:

Documentation link goes here:

FAQ link goes here:

Issue ticket number and link
Checklist before requesting a review

 I have performed a self-review of my code.
 If it is a core feature, I have added thorough tests.
 I have added thorough documentation for my code.
 I have tagged PR with relevant project labels. I acknowledge that a PR without labels may be dismissed.
 If this PR addresses a bug, I have provided both a screenshot/video of the original bug and the working solution.

Co-authored-by: ThiloteE 73715071+ThiloteE@users.noreply.github.com",2,0
2187,2024-04-01T14:54:49Z,2024-04-03T13:02:52Z,2024-04-03T13:02:52Z,16,328,382,,2,0
2194,2024-04-03T20:20:48Z,2024-04-04T18:52:13Z,2024-04-04T18:52:13Z,8,91,58,"This makes it possible to list the GPUs that can be passed to the device parameter of GPT4All from the python side, instead of asking users to run e.g. vulkaninfo --summary. This is important because the exact names of the devices are not obvious - e.g. I have a Tesla P40 and a NVIDIA GeForce GTX 970.
The mem_required parameter is not publicly exposed yet, because it hasn't done anything since GGUF support was added.
API changes:

GPT4All and Embed4All now only accept keyword arguments for parameters other than model_name

Other changes:

Don't leak the array returned by llmodel_available_gpu_devices
Remove model parameter from llmodel_available_gpu_devices as we can list devices without it",2,0
2211,2024-04-11T15:03:57Z,2024-04-15T13:37:40Z,2024-04-15T13:37:40Z,1,3,3,"Bumps tar from 6.2.0 to 6.2.1.

Commits

bef7b1e 6.2.1
fe8cd57 prevent extraction in excessively deep subfolders
fe7ebfd remove security.md
See full diff in compare view




Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting @dependabot rebase.


Dependabot commands and options

You can trigger Dependabot actions by commenting on this PR:

@dependabot rebase will rebase this PR
@dependabot recreate will recreate this PR, overwriting any edits that have been made to it
@dependabot merge will merge this PR after your CI passes on it
@dependabot squash and merge will squash and merge this PR after your CI passes on it
@dependabot cancel merge will cancel a previously requested merge and block automerging
@dependabot reopen will reopen this PR if it is closed
@dependabot close will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
@dependabot show <dependency name> ignore conditions will show all of the ignore conditions of the specified dependency
@dependabot ignore this major version will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
@dependabot ignore this minor version will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
@dependabot ignore this dependency will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the Security Alerts page.",3,1
2213,2024-04-11T20:54:41Z,2024-04-12T14:54:16Z,2024-04-12T14:54:16Z,2,16,7,"Allow increased n_ctx to actually allow multiple sequences to be submitted in one forward pass
Don't crash when llama_tokenize returns no tokens",2,0
2214,2024-04-12T15:23:30Z,2024-04-12T20:00:39Z,2024-04-12T20:00:39Z,11,95,28,"This allows a basic implementation of dynamic mode (choosing remote or local inference automatically based on expected latency/throughput) without changing the API surface too much.
The alternative would be to separate tokenization and embedding, and to keep them separate all the way to the python GPT4All API. But that seems to go against the idea of keeping the API as simple as possible.",2,0
2218,2024-04-15T15:23:45Z,2024-04-15T19:30:26Z,2024-04-15T19:30:26Z,4,99,46,"This fixes two problems with current context links:


When you have a long conversation and each response has context links, scrolling to the bottom of the conversation can cause the context links to no longer work for the responses at the top.


The context links do not work across application loads.


This patch fixes both these issues by implementing the QTextObjectInterface which allows for insertion of custom objects in the QTextDocument.
...
The second patch makes the setting for 'showReferences' simply a cosmetic toggle for the view. This way we always capture the references across restarts.",2,0
2220,2024-04-15T16:24:50Z,2024-04-15T22:02:51Z,2024-04-15T22:02:51Z,2,2,2,"This system prompt is based on the recommended one, but it can lead to strange results, as seen in #2198.",2,0
