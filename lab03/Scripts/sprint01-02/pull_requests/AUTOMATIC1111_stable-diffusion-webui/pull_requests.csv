number,created_at,merged_at,closed_at,files,additions,deletions,body_text,participants,comments
295,2022-09-11T20:03:56Z,2022-09-12T10:24:14Z,2022-09-12T10:24:14Z,2,21,14,"Adds --hide-ui-dir-config flag to disable editing directory configs from the web UI. This can be set to prevent users from setting the directory to somewhere they shouldn't, for public (or semi-public) interfaces.
Directories are still read from config.json, so the server admin can still set them in the web UI and then relaunch with the hide flag, or edit the config manually.
Also:

fix OptionInfo component_args keyword argument not being read if component isn't also set
ensure that hidden settings aren't still read from the web UI (otherwise they could still be changed by tampering with the interface)",2,0
417,2022-09-13T20:32:30Z,2022-09-14T05:58:14Z,2022-09-14T05:58:14Z,6,7,10,"#306
prevent extras from saving in dir
Extras have none of the vars used in dir names, so they cant be saved into dirs.
+grid code cleanup",2,5
429,2022-09-13T23:46:52Z,2022-09-14T05:35:27Z,2022-09-14T05:35:27Z,1,1,1,,2,2
483,2022-09-15T00:36:31Z,,2022-09-16T04:29:36Z,0,0,0,"(As mentioned in the issue #477)
It's an implementation of the prompt2prompt feature mentioned here: https://www.reddit.com/r/StableDiffusion/comments/xas2os/simple_prompt2prompt_implementation_with_prompt/
This allows you to insert or remove concepts at specific steps count to achieve better fine tuning:

Prompts work the same way as before but you can swap out text during rendering.Replacing concepts is done by:
[old concept:new concept:step]
where step is a step # or a percentage of all steps when < 1(so at 50 steps, .5 and 25 are the same), inserting new concepts:
[new concept:step]
removing concepts:
[old concept::step]

Without this feature, you would always add or remove concepts from step 1.
Some examples:
Prompt: fantasy landscape [with rocks:.25]
Description of what it should do: Add the concept ""with rocks"" at 25% of the generation

Prompt: fantasy landscape [with rocks:.75]
Description of what it should do: Add the concept ""with rocks"" at 75% of the generation

And seems to me that it works.
Thanks!",6,10
775,2022-09-21T01:16:09Z,2022-09-22T04:21:54Z,2022-09-22T04:21:54Z,4,84,31,"So, I maybe bumped some package versions to support the new ESRGAN models, and just wound up bumping everything.
Tested and working in both full precision and half.
This will probably conflict with #770 and my LDSR commit, as I'm fixing the fact that the ""unified"" bit broke other things. I can refactor as needed.
But, I added three new RealESRGAN models, General x4x3, General WDN x4x3, and AnimeVideo.
All three work fine on Mr. Whiskers, I can provide a grid if required.",3,10
1060,2022-09-25T20:40:48Z,,2022-10-15T07:44:24Z,2,44,1,"There is currently no user-friendly way to try the effect of multiple tokens in a single prompt (X/Y plot and other scripts are not satisfactory IMO)
I propose to add a support for that feature though bash-like brace expansion.
For instance, with this script, the prompt {A wise man, An idiot} watching {the moon, his finger}  will generate 4 images with respectively the prompts A wise man watching the moon , A wise man watching his finger, An idiot watching the moon and An idiot watching his finger
I made this modification a separate script in order to leave the base code untouched but this could easily be merged to the main prompt if wished.",6,9
1080,2022-09-26T02:58:04Z,2022-09-26T13:24:21Z,2022-09-26T13:24:21Z,2,3,2,"This PR add [datetime] to image file name pattern.
It's replaced to current local time in YYYYMMDDHHMMSS format such as  20220916173950.
Conflicts of datetimes are almost not happen in normal generation pace.
It is useful when searching from a large number of images, or people who generate a lot of images in a day.",3,2
1109,2022-09-26T15:51:40Z,2022-09-30T06:35:58Z,2022-09-30T06:35:59Z,23,2179,1416,"For the sake of sanity, ever scaler/model I'm referring to below will just be called ""scalers"".
Add universal model loader:
All scalers that require a model to be downloaded have at least one default URL so the user doesn't have to grab/add anything.
All scalers that download models will save models to their respective directory in /models, and create their own folders as needed.
All scalers that load/fetch models will not do so until they are needed during runtime.
All scalers have an option to override/append the default model path at runtime.
Check for existing models in ""old"" paths and move them to the new ones.
Fix various typos and optimize imports, etc, as I was going through files.
Add an arch file for gfpgan_model so we can also download weights to the /models directory.",7,11
1172,2022-09-27T16:49:21Z,,2023-01-04T15:46:58Z,4,41,30,"I implemented the possibility to run everything only on CPU, regardless of if there is a CUDA capable GPU available in the computer. I did this because I find it could be useful for the many people with a lot of time but no GPU on their device, and to solve issue #403. I also fixed a bug with the --skip-torch-cuda-test which wasn't actually working. What changed is:

--skip-torch-cuda-test now works as expected
--cpu allows to run the model on CPU, regardless of GPU availability (if used, there's no need of using --skip-torch-cuda-test)

I tested it on the main tasks using a 64-bit Arch Linux machine with a CUDA capable NVIDIA GPU (I used CUDA_VISIBLE_DEVICES=-1 to test a situation without GPU).",6,8
1202,2022-09-28T04:02:06Z,,2022-09-28T19:19:50Z,3,124,35,Reuses modules.images.save_image to give saved png images the same embedded data that generating images with txt2img or img2img writes. This allows an image in the saved fold to be dragged into the PNG Info tab to recover its generation parameters. Also results in images saved with the Save button having filenames that follow user preferences; filenames respect the Images filename pattern setting.,3,4
1254,2022-09-28T19:55:43Z,,2022-09-29T06:46:11Z,2,5,22,"This patch fixes sending an image from a batch to extras.
Here's the error I was seeing in my terminal:
Traceback (most recent call last):
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/gradio/routes.py"", line 273, in run_predict
    output = await app.blocks.process_api(
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/gradio/blocks.py"", line 753, in process_api
    result = await self.call_function(fn_index, inputs, iterator)
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/gradio/blocks.py"", line 630, in call_function
    prediction = await anyio.to_thread.run_sync(
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/anyio/to_thread.py"", line 31, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/anyio/_backends/_asyncio.py"", line 937, in run_sync_in_worker_thread
    return await future
  File ""/home/noprompt/miniconda3/envs/automatic/lib/python3.8/site-packages/anyio/_backends/_asyncio.py"", line 867, in run
    result = context.run(func, *args)
  File ""/home/noprompt/git/stable-diffusion-webui/modules/ui.py"", line 1066, in <lambda>
    fn=lambda x: image_from_url_text(x),
  File ""/home/noprompt/git/stable-diffusion-webui/modules/ui.py"", line 78, in image_from_url_text
    if filedata.startswith(""data:image/png;base64,""):
AttributeError: 'NoneType' object has no attribute 'startswith'

This was due to a -1 return value computing the index for select_gallery_index. Should be fixed now, works for me. Kinda hard to prove it w/o automated tests though 🤷
In addition it also simplifies extract_image_from_gallery , and select_gallery_index. BTW extract_image_from_gallery was assigning a global variable index.",4,12
1273,2022-09-28T22:05:59Z,,2023-01-04T14:13:47Z,2,148,2,"Like so: {a fire|an ice@3} dragon, {fantasy,sci-fi} art
As discussed in #972 we figured a syntax more like this would be more helpful.
Ex.
a fire dragon, fantasy art

an ice dragon, fantasy art

{a fire|an ice} dragon, fantasy art

{a fire|an ice@3} dragon, fantasy art

also supports nested groups:
{{a fire|an ice}|plasma} dragon, fantasy art
gives 0.25 to fire and ice each and 0.5 to plasma

also supports combination with scheduling
[{a fire|an ice}:plasma:0.1] dragon, fantasy art

Or the other way around:
{a fire|[an ice:plasma:0.2]@3} dragon, fantasy art",12,58
1375,2022-09-30T14:40:07Z,,2022-10-12T16:01:28Z,4,12,0,,4,5
1407,2022-09-30T21:39:20Z,2022-10-03T05:20:19Z,2022-10-03T05:20:19Z,1,1,1,"Unexpected behavior when using eta = 0 in something like XY and your default eta was set to something non-zero, which would use your default eta instead of 0.",3,0
1446,2022-10-01T15:08:14Z,2022-10-15T07:47:51Z,2022-10-15T07:47:51Z,1,58,1,"This implements parsing for GUI options in prompts from file, allowing users to select a custom number of steps, seed, sampler, etc on a per line basis. Prompts that have been written without option parsing in mind will work as they have up to this point, even in the same file as prompts with options.
An example prompt to demonstrate the option syntax is as follows:
--prompt ""a man surfing in hawaii"" --negative_prompt ""extra_limbs"" --seed 350 --steps 50
A full list of supported options can be found in the prompts_from_file.py file inside the prompt_tags dictionary.
Due to the way the code is currently written, total image count and batch size are being calculated before any lines get parsed, meaning that with these new changes, they're nearly always wrong. Functionally this has no consequences other than the loading bar displaying incorrect completion percentages. As I'm not sure how to go about fixing this, I've left it untouched.",4,4
1476,2022-10-01T23:02:09Z,2023-01-04T15:57:37Z,2023-01-04T15:57:37Z,1,12,0,"Supersedes PR #1420
Still learning git and should have made my changes to a new branch instead.",3,7
1517,2022-10-02T16:02:00Z,,2022-10-03T07:43:09Z,4,144,138,"Wrote an LALR(1) grammar with lark, as suggested in #1273 and #972. Also includes prompt blending. One obvious upside of this is you can now nest prompt edits like [A:[B:C:0.6]:0.3]. The downside is the grammar is quite strict in that all special characters must be escaped, so previously working prompts may be broken.
The full grammar:
start: prompt
prompt: (emphasized | scheduled | weighted | plain)*
emphasized: ""("" prompt "")"" -> emph_more
          | ""("" prompt "":"" NUMBER "")"" -> emph_valued
          | ""["" prompt ""]"" -> emph_less
scheduled: ""["" (prompt "":"")? prompt "":"" NUMBER ""]""
weighted: ""{"" weighted_item (""|"" weighted_item)* ""}""
weighted_item: prompt ("":"" NUMBER)?
plain: /([^\\\[\](){}:|]|\\.)+/
%import common.SIGNED_NUMBER -> NUMBER
TODO

More testing
Fix broken stuff (UI token count, prompt emphasis flags)
Code cleanup",3,6
1623,2022-10-04T12:41:27Z,,2022-10-24T21:37:29Z,1,33,6,"Sanitize pathname after all patterns applied.
os.path.normpath() seems to work, but it doesn't give you finer control over parts of the path.
invalid_filename_prefix and *_postfix strip are done here, so I removed them from sanitize_filename_part().",2,4
1647,2022-10-04T17:45:11Z,2022-10-18T14:24:21Z,2022-10-18T14:24:21Z,2,2,2,"The bash script doesn't pass arguments into python, so things like --shared and --listen are ignored.",4,6
1851,2022-10-07T02:28:30Z,2022-10-08T13:29:59Z,2022-10-08T13:29:59Z,6,55,6,"This PR adds xformer optimized cross-attention, a flag to disable it and use split instead, _maybe_init function that - for some reason - seems to be necessary for xformers to work in this instance and enables functorch in xformers, which further increased performance on my machine.
We still need a way for easy distribution of xformers. Otherwise, this PR is good to go (barring bugs I've not been able to perceive)
cc. @Doggettx @Thomas-MMJ @ArrowM @consciencia
PS. Much thanks to @fmassa @danthe3rd @yocabon and many others for their generous efforts to bring xformers to Windows.
I've seen a %15 improvement with batch size 1, 100 steps, 512x512 and euler_a. xFormers allows me to output 2048x2048 whereas I would previously OOM.
closes #576",22,73
1867,2022-10-07T11:48:27Z,2022-10-12T07:40:11Z,2022-10-12T07:40:11Z,2,64,32,"User can specify hypernetworks in xy-plot by short matched name instead of full name.
This is similar functionality to checkpoint in xy-plot.",2,3
1870,2022-10-07T13:13:22Z,,2022-10-08T11:55:58Z,1,1,1,Close #1869,2,0
1904,2022-10-07T19:40:34Z,2022-11-27T11:17:44Z,2022-11-27T11:17:44Z,1,13,1,"Lets you run it as root. I ran into this issue while installing on Vast.ai, their default image starts u as root. It was easier to disable this than to setup new users, etc. The default behavior is left unchanged: it will complain and quit if you are root.
./webui.sh -f  # skip root check and run anyway",7,12
2002,2022-10-08T21:01:10Z,,2022-10-30T17:26:59Z,17,2474,129,"Add basic UI implementation and stuff to unpack a selected checkpoint and then use it with Dreambooth.
There's also code to re-merge the output with said selected checkpoint, but I can't currently test with my potato because I don't know how to incorporate the necessary ""accelerate launch"" command to make it only run on GPU.
@AUTOMATIC1111 - Need help with this bit. It's useless to me if I can't get the accelerate launch stuff to work so I can force it just to my GPU, unless you know some other magick to make it work with 8GB.",17,145
2003,2022-10-08T21:03:52Z,2022-10-10T14:11:25Z,2022-10-10T14:11:25Z,1,4,0,,4,2
2035,2022-10-09T04:08:18Z,,2022-10-12T03:43:31Z,8,245,204,"Implements #842
Uses the current read generation from prompt button to load from file if prompt is empty
Uses code I wrote previously to saving / parsing JSON generation file: #1265
Not sure if this is ideal when using --shared, since you could load someone else's last generation",1,2
2037,2022-10-09T04:44:32Z,2022-10-12T12:59:24Z,2022-10-12T12:59:24Z,4,265,2,"Making the preview pngs themselves sharable embeddings.
So images like this:

Can go straight into your embeddings folder...maybe wait until you've done a little more training before sharing them in general though.
Initially concerned about torch.load doing unsafe pickling but it seems as if it's restricted on the type of classes it'll deserialise to str, I'm not sure if we're happy that's protection enough.",4,10
2067,2022-10-09T11:40:23Z,2022-10-23T10:43:41Z,2022-10-23T10:43:41Z,4,563,292,"@AUTOMATIC1111 @d8ahazard  This is the PR for #1805, which updates the ESRGAN architecture and model to support all ESRGAN models in the database (https://upscale.wiki/wiki/Model_Database), the models from the original repo, as well as BSRGAN and real-ESRGAN models.
Updating from the comment in the conversation:

The 1x models work, but currently the app skips running the models if scale = 1
There are models with scales larger than 4 (8x and there are even 16x not in the DB), but the app has a max of scale 4
The BSRGAN model and arch files are not needed and I'm removing them in the PR. Just need to load the models as regular ESRGAN models.
The real-ESRGAN models now work with the same architecture, but a bit of a dependency exists currently in the application with the models that can be downloaded automatically with the get_realesrgan_models() function: 
  
    
      stable-diffusion-webui/modules/realesrgan_model.py
    
    
         Line 82
      in
      77a7196
    
  
  
    

        
          
           def get_realesrgan_models(scaler): 
        
    
  

, so I'm not removing the real-ESRGAN bits, but they can be loaded as regular ESRGAN models now. For this it would be possible to consider a separate list of suggested models to use that can be sent to the ""model downloader"" function (maybe one that does not depend on basicsr) to fetch them automatically. This list could include not only these real-ESRGAN models, but maybe also BSRGAN and any other, that way the entire real-ESRGAN file and import can be removed.
The parameter inference function (infer_params()) has the limitation that it depends on the models being translated beforehand to infer their parameters, which is why for two particular cases of real-ESRGAN (realesr-animevideov3.pth and RealESRGAN_x4plus_anime_6B.pth) there is one parameter for each that is set according to the filename. The others are automatic.

The first 2 points are not a major issue, but will need some thinking about how to go about it, also because the same slider is user for every upscaler option.
I tested with a few models from the database, the real-ESRGAN models, BSRGAN and some others and they all worked fine, let me know if you find anything that doesn't work.",3,10
2107,2022-10-09T19:50:44Z,,2022-10-15T13:31:04Z,0,0,0,"Doing this gives better compute and lower gpu temperature raise.. less artifacts (but not really faster generations / or they are really negligible)
would need to do more testing but getting only 63 degrees Celsius temps against 78 degrees Celsius with the same settings).
and better generations (less artifacts and much cleaner edges).",4,1
2110,2022-10-09T20:37:58Z,2022-10-12T07:36:22Z,2022-10-12T07:36:22Z,3,50,27,"This adds the ability to resize to a specified image size rather than just the scale factor.
Screenshot:",3,7
2124,2022-10-10T00:48:17Z,2022-10-11T18:19:31Z,2022-10-11T18:19:31Z,1,3,0,"When using the arrow keys to increase or decrease the attention of the highlighted text, the raw value of the textarea is modified but no event is dispatched, so the controlled Svelte component doesn't know about it.  This means that if you submit the prompt without making any other changes, it will be submitted with whatever the textarea's value was before using the arrow keys.
This PR modifies edit-attention to trigger an input event, simulating the user having modified the textarea's contents and causing the usual set of event listeners to run.
Fixes #2114.
Tested on:

Windows 11 build 22000
Firefox
RTX 3080",2,2
2155,2022-10-10T09:03:54Z,,2022-10-10T11:56:14Z,39,2762,407,"Not sure if this is outside the scope of what is maintainable by the webui project.
The implementation for the backend server runs alongside the Gradio webUI in another thread, allowing the webUI to function as per normal. It builds on top of the existing internal API in a non-intrusive manner, meaning it should not affect the webUI functionality at all.
The frontend and backend code are quite well-organized in order to aid with further development and eventually exposing all features present in the webUI.
Installation of the plugin isn't too difficult as documented here: https://github.com/Interpause/auto-sd-krita/wiki/Install-Guide#plugin-installation.
And heres a demo video:
https://youtu.be/nP8MuRwcDN8
I would like to ask if there is even a remote possibility of this getting merged (of course pending cherry picking commits to remove some modifications I made as well as to fix the CRLF problem).",4,1
2164,2022-10-10T12:36:20Z,,2022-10-11T12:18:25Z,0,0,0,"""It pains me to reject the hard work someone else did""
It doesn't matter at all. This improvement is very useful for me, even if it is not accepted. I described my vision and requirements in detail on this issue, and then spent two days to implement this idea. Without this function, I need to switch back and forth between InvokeAI and webui, which is very troublesome.
This pull request is trying to build a tab browser  of  previous pictures in output directory for txt2img and img2img. We can easily browse the previously generated images, display the image generation information in PNG meta, push the image information to ""txt2img"" or ""img2img"", or delete the images we no longer need.
I try my best to encapsulate the new code independently to avoid affecting other functions of the project .
To facilitate the operation of HTML controls, I import jQuery Framework in the js script.
OS: both  Win10, and Ubuntu Server
Browser  Chrome,
Graphics card NVIDIA RTX 2080ti 11GB",6,7
2195,2022-10-10T18:40:10Z,2022-10-11T12:37:04Z,2022-10-11T12:37:04Z,2,39,15,"Before :

After :",3,5
2267,2022-10-11T12:57:31Z,,2022-10-12T12:49:04Z,0,0,0,"Last closed  PR is here
Compared with the previous version, the following changes have been made:

Replace tab  as 4 spaces indentation. Fix the indents also fix the double indents to be single ones.
Removed unneeded prints
Remove jquery framework
Change the way to delete files, making deleting files very fast
Show output directory for extras
About 500 images  in directories when I tested, , but it doesn't matter, the browser only load 48 images every page
images in subdirectories  will be added next step

——————————————————————————————————
This pull request is trying to build a tab browser  of  previous pictures in output directory for txt2img and img2img. We can easily browse the previously generated images, display the image generation information in PNG meta, push the image information to ""txt2img"" or ""img2img"", or delete the images we no longer need.
I try my best to encapsulate the new code independently to avoid affecting other functions of the project .
To facilitate the operation of HTML controls,I import jQuery Framework in the js script.
OS: both  Win10, and Ubuntu Server
Browser  Chrome,
Graphics card NVIDIA RTX 2080ti 11GB",4,5
2295,2022-10-11T18:50:52Z,2022-10-12T08:38:41Z,2022-10-12T08:38:41Z,1,11,1,"Fixes code breaking when prompt editing is used causing one line to be <= 75 and another to be > 75.
Closes #2275",2,0
2299,2022-10-11T20:01:12Z,,2022-10-12T06:00:16Z,1,1,1,Unload VAE and CLIP form VRAM when training -> Unload VAE and CLIP from VRAM when training,2,1
2300,2022-10-11T20:08:31Z,2022-10-12T07:11:06Z,2022-10-12T07:11:06Z,1,1,1,Fixed typo,3,0
2312,2022-10-11T23:23:56Z,2022-10-12T06:07:49Z,2022-10-12T06:07:49Z,1,1,0,"for the situation where you make an overlapping selection with the same start:
make a selection, press up, and then press shift+backwards, press up again.
Slicing out the float fails with a nan, presumably because it's reading the colon as a float, which overwrites the weight.",2,0
2316,2022-10-12T00:11:23Z,2022-10-13T10:25:30Z,2022-10-13T10:25:30Z,1,40,17,"What:
Maintain all the same xy_grid functionality as before, but provide an option for the xy_grid to append each individual images as additional output results, in addition to the huge monolith grid
Why:

Allows a user to easily and quickly save an individual result out of the XY grid, for the case where a user finds a few images that they particularly like and want to save, out of a large batch of images
Makes it easier to use xy_grid generation parameters to generate individual images that can be more easily post-processed, such as being used to generate a comparative gif/animation/video
Makes it easier to compare two individual images from inside the webUI, by clicking on the two images consecutively and seeing them update in-lace
Makes it easier to see the images in more detail from the webUI, without needing to deal with manually zooming and scrolling parts of the large image, and reduces browser chug when trying to manipulate the huge monolith image
Makes use of the empty UI space, since xy_grid only returns a single image, and doesn't make any use of the functionality provided for viewing multiple images",2,4
2318,2022-10-12T00:22:56Z,2022-10-12T05:35:59Z,2022-10-12T05:35:59Z,1,1,1,"Correct typo to ""Unload VAE and CLIP from VRAM when training"" in settings tab.",3,0
2321,2022-10-12T00:54:12Z,2022-10-12T06:08:44Z,2022-10-12T06:08:45Z,1,5,5,"This file was not updated and contained older versions of python and pytorch which would cause conflicts with the currently required packages, leading to broken environment when subsequently running launch.py in an environment initialized with environment-wsl2.yaml.",2,0
2324,2022-10-12T01:16:42Z,2022-10-13T05:05:41Z,2022-10-13T05:05:41Z,4,24,14,"Prior discussion: #2149
(Sorry for the churn -- I am not used to branch management and pull requests in GitHub)
This diff provides the option to add model's confidence as attention/emphasis

Why this change:

This is particularly useful when using Interrogate DeepBooru with low ""Score thresholds""

EG: If using Interrogate DeepBooru with a threshold of 0.5 (or even lower), you can get tags for minor scene components, which you can feed back into txt2img with the appropriate rates, to get scenes that more closely match the feel of the reference image.


It's mostly useful for keeping minor scene components in the prompt, without completely discarding them, and without having them contribute too much emphasis / influence over the final output
This can also give much better results when feeding the interrogate results back into either img2img or txt2img, especially when trying to generate a specific character or scene for which you have a similar concept image

Testing Steps:

Launch Webui with command line arg: --deepdanbooru
Navigate to img2img tab, use interrogate DeepBooru, verify tags appears as before. Use ""Interrogate CLIP"", verify prompt appears as before
Navigate to Settings tab, enable new option, click ""apply settings""
Navigate to img2img, Interrogate DeepBooru again, verify that weights appear and are properly formatted. Note that ""Interrogate CLIP"" prompt is still unchanged

In my testing, this change has no effect to ""Interrogate CLIP"", as it seems to generate a sentence-structured caption, and not a set of tags.",2,5
2336,2022-10-12T03:42:53Z,2022-10-13T09:21:20Z,2022-10-13T09:21:20Z,4,14,1,"Rebased to latest code
Implements #842
Uses the current read generation from prompt button to load from file if prompt is empty
Not sure if this is ideal when using --shared, since you could load someone else's last generation",2,4
2344,2022-10-12T05:13:27Z,2022-10-12T06:55:57Z,2022-10-12T06:55:57Z,1,2,0,"If the directory for saving images using the Save button doesn't yet exist, pressing the Save button will fail to save the image and instead print information about the image followed by:
Traceback (most recent call last):
  File ""/Users/brkirch/Documents/GitHub/stable-diffusion-webui/modules/ui.py"", line 182, in f
    res = list(func(*args, **kwargs))
  File ""/Users/brkirch/Documents/GitHub/stable-diffusion-webui/modules/ui.py"", line 134, in save_files
    with open(os.path.join(opts.outdir_save, ""log.csv""), ""a"", encoding=""utf8"", newline='') as file:
FileNotFoundError: [Errno 2] No such file or directory: 'log/images/log.csv'

Traceback (most recent call last):
  File ""/Users/brkirch/miniconda/envs/web-ui/lib/python3.10/site-packages/gradio/routes.py"", line 275, in run_predict
    output = await app.blocks.process_api(
  File ""/Users/brkirch/miniconda/envs/web-ui/lib/python3.10/site-packages/gradio/blocks.py"", line 789, in process_api
    predictions = self.postprocess_data(fn_index, result[""prediction""], state)
  File ""/Users/brkirch/miniconda/envs/web-ui/lib/python3.10/site-packages/gradio/blocks.py"", line 740, in postprocess_data
    if predictions[i] is components._Keywords.FINISHED_ITERATING:
IndexError: tuple index out of range

This PR creates the directory before attempting to open log.csv, preventing these errors from occurring. Fixes #2225.",2,0
2345,2022-10-12T05:19:06Z,2022-10-12T07:03:46Z,2022-10-12T07:03:46Z,1,1,3,"Problems
1. Randomness for img2imgalt not work
Randomness use p.seed as its seed.
But in processing.process_images, p.seed is not set:
    seed = get_fixed_seed(p.seed)
    subseed = get_fixed_seed(p.subseed)
As a result, rand_noise in sample_extra is always generated by the seed 0.
My solution is to use the parameter seeds as seeds, since this modifies the code changes very little(only 1 line).
2. Batch for img2imgalt's is not allowed
I know, without randomness, the batch for img2imgalt is meaningless.
However, since randomness is supported now, batch should be allowed.
I just delete two lines code.",2,0
2375,2022-10-12T12:24:55Z,2022-10-13T15:03:17Z,2022-10-13T15:03:18Z,1,33,3,"Defect
According to wiki and my testing, these params should be fixed by default:

Sampling method: must be Euler
prompt/negative prompt: should be the same as original prompt/negative prompt.
Samping Steps: should be the same as decode steps
Denoising Strength: should be 1.0

Manual adjustments for these params should be saved.
Improvements

Automatically override Sampling method
Add checkboxes for overriding prompt, steps and  strength.

Preview

The advice comes from #1852 .",3,3
2396,2022-10-12T16:59:21Z,2022-10-14T16:32:19Z,2022-10-14T16:32:19Z,4,375,0,"Last two PR closed is here and here
I think I completed all requirements in this PR comment：



this repo uses 4 spaces indentation, no tabs. Fix the indents also fix the double indents to be single ones.




there are unneeded prints




was this tested on images in subdirectories? There is an option to save images to subdirs, for example I save all mine to directories with current date, so I have stable-diffusion-webui/outputs/txt2img-images/2022-10-10/00017-3508344613.png.




how many images did you have in directories when you tested it?




adding jquery is a big decision. This seems to have just a tiny bit of JS so i think it should be possible to write it without jquery.



Originally posted by @AUTOMATIC1111 in #2164 (comment)_
and standardized the code of JS according to  suggestions in these PR comment by @moreRGB
It is worth mentioning that the problem of images in subdirectories has been completely solved
I think this “History”page has been running very well and smoothly. I hope it can be merged this time， thanks
——————————————————————————————————
This pull request is trying to build a tab browser  of  previous pictures in output directory for txt2img and img2img. We can easily browse the previously generated images, display the image generation information in PNG meta, push the image information to ""txt2img"" or ""img2img"", or delete the images we no longer need.
I try my best to encapsulate the new code independently to avoid affecting other functions of the project .
To facilitate the operation of HTML controls,I import jQuery Framework in the js script.
OS: both  Win10, and Ubuntu Server
Browser  Chrome,
Graphics card NVIDIA RTX 2080ti 11GB",8,25
2407,2022-10-12T18:27:35Z,2022-10-14T16:57:25Z,2022-10-14T16:57:25Z,2,14,2,"#2366
Servers like Google Colab (free edition), have low RAM (12.56 GB), it will force exit by Colab and crash when loading large model (for example, 7GB). However it has large VRAM (16GB) that will not run out of.
Google Colab output
/content/stable-diffusion-webui
Python 3.7.14 (default, Sep  8 2022, 00:06:44) 
[GCC 7.5.0]
Commit hash: 80f3cf2bb2ce3f00d801cae2c3a8c20a8d4167d8
Installing requirements for Web UI
Launching Web UI with arguments: --share --gradio-debug --gradio-auth ljzd:114514
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
Loading weights [06c50424] from /content/stable-diffusion-webui/models/Stable-diffusion/model.ckpt
^C

Screenshot - without --lowram


Screenshot when using --lowram",2,0
2430,2022-10-12T21:48:00Z,2022-10-14T19:44:00Z,2022-10-14T19:44:00Z,3,36,2,"Describe what this pull request is trying to achieve.
This will make it so that the loss is written to a csv file along with the epoch and epoch step in the case of textual inversion, and just the step for hypernetworks. Using this one could graph the loss and see how well the model is learning. The times when the loss should be written can be defined in the options.
Additional notes and description of your changes
To understand exactly what loss means, this page has a simple explanation about loss in machine learning. To make a graph in Libreoffice, don't forget to check First column as label in data range.
Environment this was tested in
OS: Linux, Ubuntu 22.04 LTS
Graphics card: Geforce RTX 3060
Screenshots or videos of your changes",2,1
2440,2022-10-12T23:36:41Z,2022-10-14T19:02:21Z,2022-10-14T19:02:21Z,1,1,1,code that was somehow lost.,2,0
2459,2022-10-13T06:37:25Z,,2023-01-14T13:30:35Z,2,72,10,"Issue
This issue may happen with specific ckpt files when merged with interpolations pairs that add up to 1 like you can see in the screenshot bellow:

First I thought it could be just the first characters, but it turns out the whole hash is equal, as you can see:

Checking the files proves their content are different:

So just the bytes being used to create the hash are equal as I could verify:

Solution
To solve this issue I added a option to create the hash using the entire model and save it to a .sha256 file next to the .ckpt file in the first execution and reading from it in the following executions.

The content of the sha256 file follow the default sha256 format, allowing to be checked using the command sha256sum -c model.sha256 and you can even create the sha256 file yourself using a command like sha256sum -b model.ckpt > model.sha256 or equivalent and upload it to the models folder along with the ckpt to avoid the hash generation time on the first start.
This solves the issue of differents ckpt files with the same hash without breaking the code to who don't have this issue, and just the first execution is slower, the following executions are as fast as the default hash code.
The new hashes for comparison:

Environment this was tested in

OS: Ubuntu 20.04.5 LTS Linux 5.4.0-128-generic x86_64
Browser: Brave 1.44.108 Chromium 106.0.5249.103 64 bits
Graphic card: NVIDIA GeForce MX150 2GB",10,32
2479,2022-10-13T10:25:45Z,,2022-10-13T17:14:55Z,14,569,687,The response of the webui has been optimized. Thanks for some help from Roj234,4,4
2507,2022-10-13T16:55:01Z,2022-10-14T19:03:33Z,2022-10-14T19:03:33Z,1,1,0,Add option denoising_strength in Script -> X/Y plot,2,0
2533,2022-10-13T20:45:28Z,2022-10-14T18:26:54Z,2022-10-14T18:26:54Z,1,1,1,,2,0
2554,2022-10-14T03:12:02Z,2022-10-15T07:12:16Z,2022-10-15T07:12:16Z,1,1,1,Make sure a random seed will be used if user left the field blank,3,5
2573,2022-10-14T05:46:26Z,2022-10-15T07:35:26Z,2022-10-15T07:35:26Z,2,33,26,switching time reduced from ~1500ms (in disk cache) to ~280ms (Linux 5950X),3,2
2585,2022-10-14T09:00:22Z,2022-10-21T13:16:51Z,2022-10-21T13:16:51Z,11,491,87,here the original repo: https://github.com/vicgalle/stable-diffusion-aesthetic-gradients,18,37
2588,2022-10-14T09:39:59Z,2022-10-15T07:10:22Z,2022-10-15T07:10:22Z,1,3,2,Pip often cause timeout in China because pypi.org is slow in China.,2,1
2597,2022-10-14T11:20:37Z,2022-10-14T13:31:40Z,2022-10-14T13:31:40Z,3,11,11,"Add 'interrogate' and 'all' choices to --use-cpu
Change type for --use-cpu argument to str.lower, so that choices are case insensitive

Interrogate doesn't currently work correctly on MPS so adding it as a choice to the --use-cpu option provides a way to use that feature without also forcing Stable Diffusion onto CPU.
Adding the choice of 'all' to --use-cpu is to make the option more convenient for anyone who wants to run as much as possible on CPU.",2,1
2604,2022-10-14T12:25:22Z,2022-10-14T16:54:25Z,2022-10-14T16:54:25Z,2,5,0,"Instead of just rounding (sometimes resulting in grids with ""empty"" spots), find a divisor.
For example: 8 images will now result in a 4x2 grid instead of a 3x3 with one empty spot.
(Much better for 8, 10, 15, 18, etc. but probably worse for large prime numbers.)",3,4
2611,2022-10-14T14:25:51Z,2022-10-15T14:23:39Z,2022-10-15T14:23:39Z,1,13,4,"Generalizes support for a number of lossless formats.
Adds v{vector-size} {stepcount}s to the corner of the embeddings:

Only generates a new embedding_image when there's a new image AND a new embedding to embed.",3,3
2642,2022-10-14T18:25:43Z,,2023-01-05T14:22:29Z,0,0,0,"Allows for explicitly padding to the next token set of 75 using 'OR' keyword.
I don't know if thats the best keyword; what are your thoughts?
Closes #2305",3,4
2643,2022-10-14T18:29:49Z,2022-10-15T07:21:22Z,2022-10-15T07:21:22Z,1,1,1,"Compiling model produces following warning when using Interrogate DeepBooru.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.

Compiling is only necessary when you have to train, so I've just disabled compiling.
Ref: https://stackoverflow.com/a/57054106/12818768",3,0
2645,2022-10-14T18:34:05Z,2022-10-15T06:32:02Z,2022-10-15T06:32:02Z,1,35,0,"Fix #2326 #2398 #2445
As above tickets reported, after bump version to gradio 3.4.1(d7474a5), selected image no longer stays selected when you re-generate an image. Every thing is reset to small thumbnail mode in gallery, making re-touch image multiple times a pain.
It is not clear to me if it's a gradio bug or intended.  A  javascript walkaround on webui might be best approach for now.
How it works
This PR attaches an observer to gallery when generation in progress

if there was an image selected in gallery and gradio closes the image after re-generation, auto re-open new result at the previous index after generation.
If no image is selected, nothing happens.

This matches behavior of prior to Gradio 3.4.1 version bump. Because it's done with observer, user should not notice the preview window ever being closed.
single image:

multiple images:",6,7
2649,2022-10-14T19:04:25Z,2022-10-15T07:29:41Z,2022-10-15T07:29:41Z,1,2,2,"Check the actually used $python_cmd and $GIT executables instead of the hardcoded ones
Fix typo in comment

This is the fix for #2648",3,0
2661,2022-10-14T20:48:35Z,2022-10-15T07:13:24Z,2022-10-15T07:13:24Z,1,1,0,"If you accidently set a folder that contains no images, the call to __getitem__ will fail with an division by zero Instead, now we simply raise an assertion error to tell the user we did not find any images.",3,0
2663,2022-10-14T21:27:39Z,2022-10-15T07:20:30Z,2022-10-15T07:20:30Z,1,4,1,"Highres. fix doubles the number of processing steps so the count in the UI is inaccurate
This checks for that first",2,0
2667,2022-10-14T23:06:39Z,2022-10-15T06:59:40Z,2022-10-15T06:59:40Z,3,10,14,"Problem
Already an implementation for ctrl+enter in script.js:
/**
 * Add a ctrl+enter as a shortcut to start a generation
 */
 document.addEventListener('keydown', function(e) {
    var handled = false;
    if (e.key !== undefined) {
        if((e.key == ""Enter"" && (e.metaKey || e.ctrlKey || e.altKey))) handled = true;
    } else if (e.keyCode !== undefined) {
        if((e.keyCode == 13 && (e.metaKey || e.ctrlKey || e.altKey))) handled = true;
    }
    if (handled) {
        button = get_uiCurrentTabContent().querySelector('button[id$=_generate]');
        if (button) {
            button.click();
        }
        e.preventDefault();
    }
})
So the implementation for alt+enter in ui.js may be redundant:
function submit_prompt(event, generate_button_id) {
    if (event.altKey && event.keyCode === 13) {
        event.preventDefault();
        gradioApp().getElementById(generate_button_id).click();
        return;
    }
}
What I have done

Remove the redundant implementation for alt + enter,
and move alt + enter into the same place with ctrl + enter, which works for txt2img, img2img and extras, no matter the field in focus.
Add placeholder hint for the shortcut.

Preview",3,2
2673,2022-10-15T02:31:04Z,2022-10-15T07:07:46Z,2022-10-15T07:07:46Z,1,4,2,"Fixed two issues.

In images_history.py, the output directory was treated as a relative path even though it is an absolute path in the configuration.
""outdir_samples"" directory is not used even though it is specified.",2,0
2675,2022-10-15T03:36:41Z,2022-10-16T07:51:07Z,2022-10-16T07:51:07Z,2,27,20,"Problem
everytime I modified pt files in embedding_dir or hypernetwork_dir, I
need to restart webui to have the new files shown in the dropdown of
Train Tab
Solution
refactored create_refresh_button out of create_setting_component so we
can use this method to create button next to gr.Dropdowns of embedding
name and hypernetworks
Extra Modification
hypernetwork pt are now sorted in alphabetic order
Test Video
https://drive.google.com/file/d/1N4Jw4vek8WwMmS8czPvI0bv8bLXTTbSX/view?usp=sharing",2,3
2693,2022-10-15T08:27:07Z,2022-10-15T13:26:07Z,2022-10-15T13:26:07Z,2,39,0,"This PR adds Pylint on PRs, to try to prevent breaking syntax errors being merged in.
There are a lot of warnings/errors/style issues, so I just disabled them all in the .pylintrc file, except for Fatals, which should still catch obvious typos.
If you still run into issues, you can disable it for a file by adding this comment.
# pylint: skip-file
I put links to relevant docs inline in files.
I ran this locally and confirmed it works:
❯ time pylint $(git ls-files '*.py')

-------------------------------------------------------------------
Your code has been rated at 10.00/10 (previous run: 8.64/10, +1.36)

pylint $(git ls-files '*.py')  14.13s user 5.29s system 50% cpu 38.671 total

You can see a successful run here on my fork: https://github.com/robertsmieja/stable-diffusion-webui/actions/runs/3255008846/jobs/5343878420

Side note:
I tried running with errors enabled and ran into the following messages, which mostly complains about imports:
pylint $(git ls-files '*.py')
************* Module modules.bsrgan_model
modules/bsrgan_model.py:50:14: E1101: Module 'torch' has no 'from_numpy' member (no-member)
************* Module modules.bsrgan_model_arch
modules/bsrgan_model_arch.py:51:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/bsrgan_model_arch.py:52:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/bsrgan_model_arch.py:53:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/bsrgan_model_arch.py:54:24: E1101: Module 'torch' has no 'cat' member (no-member)
************* Module modules.codeformer.codeformer_arch
modules/codeformer/codeformer_arch.py:67:19: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/codeformer/codeformer_arch.py:67:89: E1101: Module 'torch' has no 'bool' member (no-member)
modules/codeformer/codeformer_arch.py:68:19: E1130: bad operand type for unary ~: NoneType (invalid-unary-operand-type)
modules/codeformer/codeformer_arch.py:69:43: E1101: Module 'torch' has no 'float32' member (no-member)
modules/codeformer/codeformer_arch.py:70:43: E1101: Module 'torch' has no 'float32' member (no-member)
modules/codeformer/codeformer_arch.py:76:16: E1101: Module 'torch' has no 'arange' member (no-member)
modules/codeformer/codeformer_arch.py:76:55: E1101: Module 'torch' has no 'float32' member (no-member)
modules/codeformer/codeformer_arch.py:81:16: E1101: Module 'torch' has no 'stack' member (no-member)
modules/codeformer/codeformer_arch.py:84:16: E1101: Module 'torch' has no 'stack' member (no-member)
modules/codeformer/codeformer_arch.py:87:14: E1101: Module 'torch' has no 'cat' member (no-member)
modules/codeformer/codeformer_arch.py:154:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/codeformer/codeformer_arch.py:180:41: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/codeformer/codeformer_arch.py:256:21: E1101: Module 'torch' has no 'topk' member (no-member)
************* Module modules.codeformer.vqgan_arch
modules/codeformer/vqgan_arch.py:22:13: E1101: Module 'torch' has no 'sigmoid' member (no-member)
modules/codeformer/vqgan_arch.py:42:16: E1101: Module 'torch' has no 'matmul' member (no-member)
modules/codeformer/vqgan_arch.py:44:24: E1101: Module 'torch' has no 'mean' member (no-member)
modules/codeformer/vqgan_arch.py:47:52: E1101: Module 'torch' has no 'topk' member (no-member)
modules/codeformer/vqgan_arch.py:49:30: E1101: Module 'torch' has no 'exp' member (no-member)
modules/codeformer/vqgan_arch.py:51:24: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/codeformer/vqgan_arch.py:55:14: E1101: Module 'torch' has no 'matmul' member (no-member)
modules/codeformer/vqgan_arch.py:57:15: E1101: Module 'torch' has no 'mean' member (no-member)
modules/codeformer/vqgan_arch.py:57:61: E1101: Module 'torch' has no 'mean' member (no-member)
modules/codeformer/vqgan_arch.py:62:17: E1101: Module 'torch' has no 'mean' member (no-member)
modules/codeformer/vqgan_arch.py:63:21: E1101: Module 'torch' has no 'exp' member (no-member)
modules/codeformer/vqgan_arch.py:63:32: E1101: Module 'torch' has no 'sum' member (no-member)
modules/codeformer/vqgan_arch.py:63:51: E1101: Module 'torch' has no 'log' member (no-member)
modules/codeformer/vqgan_arch.py:79:24: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/codeformer/vqgan_arch.py:82:14: E1101: Module 'torch' has no 'matmul' member (no-member)
modules/codeformer/vqgan_arch.py:112:32: E1101: Module 'torch' has no 'sum' member (no-member)
modules/codeformer/vqgan_arch.py:112:47: E1101: Module 'torch' has no 'log' member (no-member)
modules/codeformer/vqgan_arch.py:217:13: E1101: Module 'torch' has no 'bmm' member (no-member)
modules/codeformer/vqgan_arch.py:224:13: E1101: Module 'torch' has no 'bmm' member (no-member)
************* Module modules.codeformer_model
modules/codeformer_model.py:38:8: E0401: Unable to import 'facelib.utils.face_restoration_helper' (import-error)
modules/codeformer_model.py:103:37: E1102: self.net is not callable (not-callable)
modules/codeformer_model.py:120:35: E1101: Module 'cv2' has no 'resize' member (no-member)
modules/codeformer_model.py:120:180: E1101: Module 'cv2' has no 'INTER_LINEAR' member (no-member)
************* Module modules.deepbooru
modules/deepbooru.py:87:4: E0401: Unable to import 'deepdanbooru' (import-error)
modules/deepbooru.py:88:4: E0401: Unable to import 'tensorflow' (import-error)
modules/deepbooru.py:111:4: E0401: Unable to import 'deepdanbooru' (import-error)
modules/deepbooru.py:112:4: E0401: Unable to import 'tensorflow' (import-error)
************* Module modules.devices
modules/devices.py:10:6: E1101: Module 'torch' has no 'device' member (no-member)
modules/devices.py:15:15: E1101: Module 'torch' has no 'device' member (no-member)
modules/devices.py:38:8: E1101: Module 'torch' has no 'float16' member (no-member)
modules/devices.py:39:12: E1101: Module 'torch' has no 'float16' member (no-member)
modules/devices.py:44:20: E1101: Module 'torch' has no 'Generator' member (no-member)
modules/devices.py:46:16: E1101: Module 'torch' has no 'randn' member (no-member)
modules/devices.py:50:11: E1101: Module 'torch' has no 'randn' member (no-member)
modules/devices.py:56:20: E1101: Module 'torch' has no 'Generator' member (no-member)
modules/devices.py:57:16: E1101: Module 'torch' has no 'randn' member (no-member)
modules/devices.py:60:11: E1101: Module 'torch' has no 'randn' member (no-member)
modules/devices.py:69:16: E1101: Module 'torch' has no 'float32' member (no-member)
************* Module modules.esrgan_model
modules/esrgan_model.py:126:10: E1101: Module 'torch' has no 'from_numpy' member (no-member)
************* Module modules.esrgan_model_arch
modules/esrgan_model_arch.py:32:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/esrgan_model_arch.py:33:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/esrgan_model_arch.py:34:35: E1101: Module 'torch' has no 'cat' member (no-member)
modules/esrgan_model_arch.py:35:24: E1101: Module 'torch' has no 'cat' member (no-member)
************* Module modules.extras
modules/extras.py:200:21: E1101: Module 'torch' has no 'zeros_like' member (no-member)
************* Module modules.hypernetworks.hypernetwork
modules/hypernetworks/hypernetwork.py:12:0: E0401: Unable to import 'ldm.util' (import-error)
modules/hypernetworks/hypernetwork.py:173:25: E1101: Module 'torch' has no 'finfo' member (no-member)
modules/hypernetworks/hypernetwork.py:187:15: E1101: Module 'torch' has no 'stack' member (no-member)
modules/hypernetworks/hypernetwork.py:195:23: E1101: Module 'torch' has no 'vstack' member (no-member)
modules/hypernetworks/hypernetwork.py:197:11: E1101: Module 'torch' has no 'stack' member (no-member)
modules/hypernetworks/hypernetwork.py:239:13: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/hypernetworks/hypernetwork.py:265:16: E1101: Module 'torch' has no 'stack' member (no-member)
************* Module modules.images
modules/images.py:12:0: E0611: No name 'Roboto' in module 'fonts.ttf' (no-name-in-module)
************* Module modules.interrogate
modules/interrogate.py:46:8: E0401: Unable to import 'models.blip' (import-error)
modules/interrogate.py:46:8: E0611: No name 'blip' in module 'models' (no-name-in-module)
modules/interrogate.py:48:21: I1101: Module 'models' has no 'blip' member, but source is unavailable. Consider adding this module to extension-pkg-allow-list if you want to perform analysis based on run-time introspection of living objects. (c-extension-no-member)
modules/interrogate.py:106:21: E1101: Module 'torch' has no 'zeros' member (no-member)
************* Module modules.ldsr_model_arch
modules/ldsr_model_arch.py:12:0: E0401: Unable to import 'ldm.models.diffusion.ddim' (import-error)
modules/ldsr_model_arch.py:13:0: E0401: Unable to import 'ldm.util' (import-error)
modules/ldsr_model_arch.py:73:22: E1101: Module 'torch' has no 'randn' member (no-member)
modules/ldsr_model_arch.py:73:37: E1136: Value 'custom_shape' is unsubscriptable (unsubscriptable-object)
modules/ldsr_model_arch.py:73:54: E1136: Value 'custom_shape' is unsubscriptable (unsubscriptable-object)
modules/ldsr_model_arch.py:73:71: E1136: Value 'custom_shape' is unsubscriptable (unsubscriptable-object)
modules/ldsr_model_arch.py:74:58: E1136: Value 'custom_shape' is unsubscriptable (unsubscriptable-object)
modules/ldsr_model_arch.py:117:17: E1101: Module 'torch' has no 'clamp' member (no-member)
modules/ldsr_model_arch.py:133:8: E1101: Module 'torch' has no 'unsqueeze' member (no-member)
modules/ldsr_model_arch.py:140:13: E1101: Module 'torch' has no 'device' member (no-member)
modules/ldsr_model_arch.py:177:12: E1101: Module 'torch' has no 'randn' member (no-member)
modules/ldsr_model_arch.py:191:65: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/ldsr_model_arch.py:193:66: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/ldsr_model_arch.py:215:29: E1101: Module 'torch' has no 'abs' member (no-member)
************* Module modules.lowvram
modules/lowvram.py:5:6: E1101: Module 'torch' has no 'device' member (no-member)
************* Module modules.ngrok
modules/ngrok.py:1:0: E0401: Unable to import 'pyngrok' (import-error)
************* Module modules.processing
modules/processing.py:31:24: E1101: Module 'cv2' has no 'cvtColor' member (no-member)
modules/processing.py:31:63: E1101: Module 'cv2' has no 'COLOR_RGB2LAB' member (no-member)
modules/processing.py:37:28: E1101: Module 'cv2' has no 'cvtColor' member (no-member)
modules/processing.py:38:8: E1101: Module 'cv2' has no 'cvtColor' member (no-member)
modules/processing.py:40:12: E1101: Module 'cv2' has no 'COLOR_RGB2LAB' member (no-member)
modules/processing.py:44:7: E1101: Module 'cv2' has no 'COLOR_LAB2RGB' member (no-member)
modules/processing.py:197:12: E1101: Module 'torch' has no 'acos' member (no-member)
modules/processing.py:198:9: E1101: Module 'torch' has no 'sin' member (no-member)
modules/processing.py:199:11: E1101: Module 'torch' has no 'sin' member (no-member)
modules/processing.py:199:62: E1101: Module 'torch' has no 'sin' member (no-member)
modules/processing.py:259:36: E1101: Module 'torch' has no 'stack' member (no-member)
modules/processing.py:261:8: E1101: Module 'torch' has no 'stack' member (no-member)
modules/processing.py:418:29: E1101: Module 'torch' has no 'clamp' member (no-member)
modules/processing.py:565:29: E1101: Module 'torch' has no 'clamp' member (no-member)
modules/processing.py:577:30: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/processing.py:697:16: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/processing.py:711:24: E1101: Module 'torch' has no 'asarray' member (no-member)
modules/processing.py:712:25: E1101: Module 'torch' has no 'asarray' member (no-member)
************* Module modules.prompt_parser
modules/prompt_parser.py:214:10: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/prompt_parser.py:254:25: E1101: Module 'torch' has no 'vstack' member (no-member)
modules/prompt_parser.py:256:23: E1101: Module 'torch' has no 'stack' member (no-member)
************* Module modules.safety
modules/safety.py:40:8: E1101: Module 'torch' has no 'from_numpy' member (no-member)
************* Module modules.scunet_model
modules/scunet_model.py:56:14: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/scunet_model.py:61:21: E1102: model is not callable (not-callable)
************* Module modules.scunet_model_arch
modules/scunet_model_arch.py:26:12: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/scunet_model_arch.py:44:20: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/scunet_model_arch.py:44:56: E1101: Module 'torch' has no 'bool' member (no-member)
modules/scunet_model_arch.py:64:33: E1101: Module 'torch' has no 'roll' member (no-member)
modules/scunet_model_arch.py:88:38: E1101: Module 'torch' has no 'roll' member (no-member)
modules/scunet_model_arch.py:93:15: E1101: Module 'torch' has no 'tensor' member; maybe 'Tensor'? (no-member)
modules/scunet_model_arch.py:161:27: E1101: Module 'torch' has no 'cat' member (no-member)
modules/scunet_model_arch.py:179:33: E1101: Module 'torch' has no 'linspace' member (no-member)
************* Module modules.sd_hijack
modules/sd_hijack.py:15:0: E0401: Unable to import 'ldm.modules.attention' (import-error)
modules/sd_hijack.py:16:0: E0401: Unable to import 'ldm.modules.diffusionmodules.model' (import-error)
modules/sd_hijack.py:335:37: E1101: Module 'torch' has no 'cat' member (no-member)
modules/sd_hijack.py:349:17: E1101: Module 'torch' has no 'asarray' member (no-member)
modules/sd_hijack.py:360:28: E1101: Module 'torch' has no 'asarray' member (no-member)
modules/sd_hijack.py:389:25: E1101: Module 'torch' has no 'cat' member (no-member)
modules/sd_hijack.py:393:15: E1101: Module 'torch' has no 'stack' member (no-member)
************* Module modules.sd_hijack_optimizations
modules/sd_hijack_optimizations.py:9:0: E0401: Unable to import 'ldm.util' (import-error)
modules/sd_hijack_optimizations.py:18:8: E0401: Unable to import 'xformers.ops' (import-error)
modules/sd_hijack_optimizations.py:40:9: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/sd_hijack_optimizations.py:77:9: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/sd_hijack_optimizations.py:132:4: E0401: Unable to import 'psutil' (import-error)
modules/sd_hijack_optimizations.py:141:8: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/sd_hijack_optimizations.py:148:8: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/sd_hijack_optimizations.py:184:11: E0602: Undefined variable 'self' (undefined-variable)
modules/sd_hijack_optimizations.py:251:13: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/sd_hijack_optimizations.py:271:17: E1101: Module 'torch' has no 'bmm' member (no-member)
modules/sd_hijack_optimizations.py:282:30: E1101: Module 'torch' has no 'bmm' member (no-member)
************* Module modules.sd_models
modules/sd_models.py:8:0: E0401: Unable to import 'ldm.util' (import-error)
modules/sd_models.py:147:35: E1101: Module 'torch' has no 'channels_last' member (no-member)
modules/sd_models.py:152:24: E1101: Module 'torch' has no 'float32' member (no-member)
modules/sd_models.py:152:70: E1101: Module 'torch' has no 'float16' member (no-member)
modules/sd_models.py:153:28: E1101: Module 'torch' has no 'float32' member (no-member)
modules/sd_models.py:153:105: E1101: Module 'torch' has no 'float16' member (no-member)
************* Module modules.sd_samplers
modules/sd_samplers.py:7:0: E0401: Unable to import 'k_diffusion.sampling' (import-error)
modules/sd_samplers.py:8:0: E0401: Unable to import 'ldm.models.diffusion.ddim' (import-error)
modules/sd_samplers.py:9:0: E0401: Unable to import 'ldm.models.diffusion.plms' (import-error)
modules/sd_samplers.py:87:15: E1101: Module 'torch' has no 'clamp' member (no-member)
modules/sd_samplers.py:151:41: E1101: Module 'torch' has no 'hstack' member (no-member)
modules/sd_samplers.py:190:47: E1101: Module 'torch' has no 'tensor' member; maybe 'Tensor'? (no-member)
modules/sd_samplers.py:232:15: E1101: Module 'torch' has no 'cat' member (no-member)
modules/sd_samplers.py:232:26: E1101: Module 'torch' has no 'stack' member (no-member)
modules/sd_samplers.py:233:19: E1101: Module 'torch' has no 'cat' member (no-member)
modules/sd_samplers.py:233:30: E1101: Module 'torch' has no 'stack' member (no-member)
modules/sd_samplers.py:236:22: E1101: Module 'torch' has no 'cat' member (no-member)
modules/sd_samplers.py:241:24: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/sd_samplers.py:247:20: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/sd_samplers.py:257:19: E1101: Module 'torch' has no 'clone' member (no-member)
modules/sd_samplers.py:325:16: E1136: Value 'self.sampler_noises' is unsubscriptable (unsubscriptable-object)
modules/sd_samplers.py:330:18: E1101: Module 'torch' has no 'randn_like' member (no-member)
************* Module modules.shared
modules/shared.py:220:119: E0601: Using variable 'sd_upscalers' before assignment (used-before-assignment)
************* Module modules.swinir_model
modules/swinir_model.py:113:10: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/swinir_model.py:119:14: E1101: Module 'torch' has no 'cat' member (no-member)
modules/swinir_model.py:119:30: E1101: Module 'torch' has no 'flip' member (no-member)
modules/swinir_model.py:120:14: E1101: Module 'torch' has no 'cat' member (no-member)
modules/swinir_model.py:120:30: E1101: Module 'torch' has no 'flip' member (no-member)
modules/swinir_model.py:142:8: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model.py:142:48: E1101: Module 'torch' has no 'half' member (no-member)
modules/swinir_model.py:143:8: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/swinir_model.py:143:34: E1101: Module 'torch' has no 'half' member (no-member)
modules/swinir_model.py:150:33: E1101: Module 'torch' has no 'ones_like' member (no-member)
************* Module modules.swinir_model_arch
modules/swinir_model_arch.py:90:12: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch.py:93:19: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch.py:94:19: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch.py:95:17: E1101: Module 'torch' has no 'stack' member (no-member)
modules/swinir_model_arch.py:96:25: E1101: Module 'torch' has no 'flatten' member (no-member)
modules/swinir_model_arch.py:219:19: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch.py:250:24: E1101: Module 'torch' has no 'roll' member (no-member)
modules/swinir_model_arch.py:270:16: E1101: Module 'torch' has no 'roll' member (no-member)
modules/swinir_model_arch.py:331:12: E1101: Module 'torch' has no 'cat' member (no-member)
modules/swinir_model_arch.py:662:24: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch.py:695:51: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch.py:701:33: E1101: Module 'torch' has no 'linspace' member (no-member)
************* Module modules.swinir_model_arch_v2
modules/swinir_model_arch_v2.py:85:40: E1101: Module 'torch' has no 'log' member (no-member)
modules/swinir_model_arch_v2.py:85:55: E1101: Module 'torch' has no 'ones' member (no-member)
modules/swinir_model_arch_v2.py:93:28: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch_v2.py:93:96: E1101: Module 'torch' has no 'float32' member (no-member)
modules/swinir_model_arch_v2.py:94:28: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch_v2.py:94:96: E1101: Module 'torch' has no 'float32' member (no-member)
modules/swinir_model_arch_v2.py:95:32: E1101: Module 'torch' has no 'stack' member (no-member)
modules/swinir_model_arch_v2.py:105:32: E1101: Module 'torch' has no 'sign' member (no-member)
modules/swinir_model_arch_v2.py:105:68: E1101: Module 'torch' has no 'log2' member (no-member)
modules/swinir_model_arch_v2.py:106:12: E1101: Module 'torch' has no 'abs' member (no-member)
modules/swinir_model_arch_v2.py:111:19: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch_v2.py:112:19: E1101: Module 'torch' has no 'arange' member (no-member)
modules/swinir_model_arch_v2.py:113:17: E1101: Module 'torch' has no 'stack' member (no-member)
modules/swinir_model_arch_v2.py:114:25: E1101: Module 'torch' has no 'flatten' member (no-member)
modules/swinir_model_arch_v2.py:125:39: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch_v2.py:126:39: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch_v2.py:144:23: E1101: Module 'torch' has no 'cat' member (no-member)
modules/swinir_model_arch_v2.py:144:47: E1101: Module 'torch' has no 'zeros_like' member (no-member)
modules/swinir_model_arch_v2.py:151:22: E1101: Module 'torch' has no 'clamp' member (no-member)
modules/swinir_model_arch_v2.py:151:56: E1101: Module 'torch' has no 'log' member (no-member)
modules/swinir_model_arch_v2.py:151:66: E1101: Module 'torch' has no 'tensor' member; maybe 'Tensor'? (no-member)
modules/swinir_model_arch_v2.py:158:38: E1101: Module 'torch' has no 'sigmoid' member (no-member)
modules/swinir_model_arch_v2.py:248:19: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch_v2.py:278:24: E1101: Module 'torch' has no 'roll' member (no-member)
modules/swinir_model_arch_v2.py:298:16: E1101: Module 'torch' has no 'roll' member (no-member)
modules/swinir_model_arch_v2.py:357:12: E1101: Module 'torch' has no 'cat' member (no-member)
modules/swinir_model_arch_v2.py:716:24: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch_v2.py:749:51: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/swinir_model_arch_v2.py:755:33: E1101: Module 'torch' has no 'linspace' member (no-member)
************* Module modules.textual_inversion.dataset
modules/textual_inversion/dataset.py:72:24: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/textual_inversion/dataset.py:72:74: E1101: Module 'torch' has no 'float32' member (no-member)
modules/textual_inversion/dataset.py:73:24: E1101: Module 'torch' has no 'moveaxis' member (no-member)
modules/textual_inversion/dataset.py:94:44: E1101: Module 'torch' has no 'randperm' member (no-member)
************* Module modules.textual_inversion.image_embedding
modules/textual_inversion/image_embedding.py:6:0: E0611: No name 'Roboto' in module 'fonts.ttf' (no-name-in-module)
modules/textual_inversion/image_embedding.py:21:4: E0202: An attribute defined in json.decoder line 319 hides this method (method-hidden)
modules/textual_inversion/image_embedding.py:23:19: E1101: Module 'torch' has no 'from_numpy' member (no-member)
modules/textual_inversion/image_embedding.py:113:24: E1121: Too many positional arguments for method call (too-many-function-args)
modules/textual_inversion/image_embedding.py:139:42: E0602: Undefined variable 'opts' (undefined-variable)
modules/textual_inversion/image_embedding.py:139:63: E0601: Using variable 'fontsize' before assignment (used-before-assignment)
modules/textual_inversion/image_embedding.py:140:23: E0602: Undefined variable 'opts' (undefined-variable)
************* Module modules.textual_inversion.textual_inversion
modules/textual_inversion/textual_inversion.py:119:56: E1101: Module 'torch' has no 'float32' member (no-member)
modules/textual_inversion/textual_inversion.py:161:10: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/textual_inversion/textual_inversion.py:241:13: E1101: Module 'torch' has no 'zeros' member (no-member)
modules/textual_inversion/textual_inversion.py:266:16: E1101: Module 'torch' has no 'stack' member (no-member)
************* Module modules.ui
modules/ui.py:151:36: E1101: Instance of 'MyObject' has no 'index_of_first_image' member (no-member)
modules/ui.py:152:49: E1101: Instance of 'MyObject' has no 'index_of_first_image' member (no-member)
modules/ui.py:154:66: E1101: Instance of 'MyObject' has no 'all_seeds' member (no-member)
modules/ui.py:154:89: E1101: Instance of 'MyObject' has no 'all_prompts' member (no-member)
modules/ui.py:154:133: E1101: Instance of 'MyObject' has no 'infotexts' member (no-member)
modules/ui.py:1366:16: E1101: Module 'os' has no 'startfile' member (no-member)
************* Module img2imgalt
scripts/img2imgalt.py:14:0: E0401: Unable to import 'k_diffusion' (import-error)
scripts/img2imgalt.py:33:15: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:34:19: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:35:18: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:77:15: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:78:19: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:79:18: E1101: Module 'torch' has no 'cat' member (no-member)
scripts/img2imgalt.py:84:31: E1101: Module 'torch' has no 'cat' member (no-member)
************* Module xy_grid
scripts/xy_grid.py:225:15: E1120: No value for argument 'p' in constructor call (no-value-for-parameter)
scripts/xy_grid.py:225:15: E1120: No value for argument 'images_list' in constructor call (no-value-for-parameter)

------------------------------------------------------------------
Your code has been rated at 8.64/10 (previous run: 8.63/10, +0.01)


Other improvements you can add later are (black)[https://pypi.org/project/black/] or (isort)[https://pypi.org/project/isort/].",2,4
2703,2022-10-15T09:27:04Z,2022-10-15T14:14:58Z,2022-10-15T14:14:58Z,2,2,9,"Before:

After:

What changed

gr.Group() probably won't meant to be used here, it has 0 gaps/margins.  Changed to gr.Column() which has a 1rem gap between rows.
the texts for file downloads are stacked together because of custom css override, the selector.gr-panel div.flex-col div.justify-between div was meant to control range input label position. After gradio 3.4.1 bump, the element has been changed to input, which has a different selector .gr-box > div > div > input.gr-text-input in css file. The old selector is no longer needed.

before:

after gradio 3.4.1:",2,0
2714,2022-10-15T10:44:35Z,2022-10-15T13:13:13Z,2022-10-15T13:13:13Z,1,1,1,,3,0
2732,2022-10-15T12:47:20Z,2022-10-15T14:15:26Z,2022-10-15T14:15:26Z,1,6,4,,2,0
2741,2022-10-15T16:09:44Z,2022-10-15T17:25:28Z,2022-10-15T17:25:28Z,2,4,3,"This PR updates the prebuilt wheel with my new one that supports Pascal to Hopper, and adds a --reinstall-xformers argument to force reinstall xformers as large amounts of users struggled to perform this operation manually.",2,3
2755,2022-10-15T20:20:29Z,2022-10-16T07:04:14Z,2022-10-16T07:04:14Z,1,10,1,"Now user can set Styles from ui-config.json.
To set styles other than None at the start of the program, set style's name at the ui-config.json.
ex: ""txt2img/Style 1/value"": ""my_style""
If user uses bad UI setting, (ex: use style name that doesn't exists) program warns it and uses default UI setting.
(Previous behaviour was to quietly use default UI setting, without warning.)",2,0
2778,2022-10-16T00:24:45Z,2022-10-16T06:56:33Z,2022-10-16T06:56:33Z,3,7,4,Allows specifying the ngrok region using the --ngrok-region argument,2,0
2790,2022-10-16T02:49:48Z,2022-10-16T13:40:04Z,2022-10-16T13:40:04Z,1,2,4,,2,3
2807,2022-10-16T05:05:31Z,2022-10-16T06:47:31Z,2022-10-16T06:47:31Z,2,30,5,"Currently, the batch process function in the Extras tab throws an error when adding many files. So, similar to the batch function in the img2img tab, new function was added to work from specific directory. And added an option to choose whether to show the result images to avoid possible delays when completing a large number of tasks. This option only affects to batch work from directory.
It would be nice to be able to add a progress bar, but unfortunately I haven't been able to add that functionality.
related isses : #2165",2,0
2835,2022-10-16T09:09:03Z,2022-10-19T06:44:00Z,2022-10-19T06:44:00Z,1,19,14,"hot-reload the javascript files without restart the backend-app when click the reload_script_bodies button, that's really useful when we just want to upgrade static scripts.
I have already used this feature to develop my custom frontend scripts  .",3,6
2838,2022-10-16T09:50:36Z,2022-10-18T12:10:09Z,2022-10-18T12:10:09Z,2,23,2,"Add an update warning to launch.py that compares https://api.github.com/repos/AUTOMATIC1111/stable-diffusion-webui/branches/master to the current commit hash.
very notiable 80s hacker message is shown
--------------------------------------------------------
| You are not up to date with the most recent release. |
| Consider running `git pull` to update.               |
--------------------------------------------------------
(Colours are sadly not accurate.)
alternatively using {git} rev-parse origin/master may be another option.",2,1
2846,2022-10-16T11:38:38Z,2022-10-19T18:30:02Z,2022-10-19T18:30:02Z,5,122,52,"This is not a large change at all and doesn't affect the code of the app, it's just github related
Describe what this pull request is trying to achieve.
I've tried to implement a new way to submit Bug Reports & Feature Requests, via Yaml forms instead of simple .md's, inspired by what's done on hlky's repo and github examples of their yaml forms.
Additional notes and description of your changes
The point is to create a standard for this kind of submissions that prevents people from ignoring the template & incentivise them as much as possible to check that this won't be a duplicate/hasn't already been fixed
Environment this was tested in

OS: Windows
Browser Brave
Graphics card RTX3090

The new Bug Report Form

With and example of a submitted issue

The New Feature Request Form

With and example of the resulting issue",2,3
2894,2022-10-16T19:13:33Z,2022-10-17T05:43:41Z,2022-10-17T05:43:41Z,1,26,22,"What:
Move logic related to caching and restoring global settings into a separate helper class, which can be wrapped in a using() statement
Why:

exit() will be run in all cases where control exits this block, including early returns or thrown exceptions
useful for keeping settings cache and restore logic together, instead of scattering it through the run() function
nice for code reuse (other third party scripts can import this class)

Testing steps:

Run xy_grid with a clip skip override value that ends in a number other than the configured setting
disable xy_grid and run another job with the same settings and seed
verify that the clip skip value used is the same as the global setting one, and is not the leaked value from the last xy_grid run",2,1
2895,2022-10-16T19:28:57Z,2022-10-17T05:42:18Z,2022-10-17T05:42:18Z,1,1,0,"Prints in the format:
Loaded a total of 5 textual inversion embeddings.
Embeddings: dr-strange, victorian-lace, ricar, indiana
Running on local URL:  http://127.0.0.1:7860",3,3
2907,2022-10-16T22:25:19Z,2022-10-17T05:01:59Z,2022-10-17T05:01:59Z,1,2,2,"Sorry if something is wrong, it's my first pull request ever made...
This PR fixes strange interrogator behavior when it includes ranks to CLIP Interrogator described here #2641
Besides that it also adds a crazy prompt weights to words (12-18 per word)
I looked into interrogate.py file, found an if statement which is IMO really wrong and changed it, now it works as intended
Example of wrong interrogation:

Example of right interrogation:",3,1
2908,2022-10-16T22:55:29Z,2022-10-17T04:54:36Z,2022-10-17T04:54:36Z,1,17,10,"Current implementation of ""Add difference"" merge requires accessing the same key in all three model dicts at once, which is a problem on machines (such as mine) without enough ram to load three models simultaneously.
This change makes it so that the difference is calculated in a separate step, so that theta_0 can be swapped to disk during the first step, and theta_2 can be unloaded entirely during the second.",2,0
2919,2022-10-17T01:37:00Z,2022-10-17T04:57:17Z,2022-10-17T04:57:17Z,1,6,0,can save some time in install deps,2,0
2924,2022-10-17T03:33:01Z,2022-10-17T04:48:28Z,2022-10-17T04:48:28Z,1,3,3,"Left / top alignment of the inpainting image mask was necessary with gradio 3.4.1. In gradio 3.5 the parent div of the image mask is centered, so the left / top alignment put the mask in the wrong place as described in #2750 #2795 #2805. This fix was tested on Windows 10 / Chrome.",2,0
2932,2022-10-17T07:09:15Z,2022-10-17T08:34:23Z,2022-10-17T08:34:23Z,5,157,31,"FastAPI template which instruments the use of the existing processing.py objects already used to talk between functions in this repo.
In this way, whenever StableDiffusionProcessing changes - the API also changes to reflect additional parameters
StableDiffusionProcessingAPI = pydanticModelGenerator(""StableDiffusionProcessing"", 
                                                      StableDiffusionProcessing().__dict__, 
                                                      inspect.signature(StableDiffusionProcessing.__init__).parameters).generate_model()
I did not finish out the other API methods as I am requesting feedback on the current scheme for calling the functions themselves. Should this format be desired stubbing out the other methods would look similarly by generating a pydanticModelGenerator of the processing class then processing the image.  This also would work with gradio - would just need to use the same queue lock defined in webui.py
p = StableDiffusionProcessingTxt2Img(**vars(txt2imgreq))
p.sd_model = shared.sd_model
print(p)
processed = process_images(p)
prior to processed = process_images(p)",3,0
2939,2022-10-17T09:07:17Z,2022-10-18T11:04:26Z,2022-10-18T11:04:26Z,3,11,1,Sets the UI to dark mode with the --dark-mode command line argument,3,7
2945,2022-10-17T10:15:18Z,,2022-11-09T12:36:00Z,8,312,19,"Propose a better method of prompt tuning (train embedding), which can super dramatically improve the image quality compared to the current embedding training method. Excellent performance can be active even with just one image for training (one-shot learning).
Performance Comparison with same extra prompt:
train image:

current method:


my APT method:


or add some details prompt:

no prompt tuning (textual inversion):

Learn Genshin Nahida from single image:

combination Nahida with additional prompt:


**Note. **
The results from this version are currently inconsistent with the version I used before, and there are some discrepancies in performance. The reason for this is currently unknown. Probably due to the scheduler, the training 1000 steps learning rate decreases too fast.
my old version :

new version :

It may be better to use https://github.com/7eu7d7/APT-stable-diffusion-auto-prompt when training embedding",27,99
2946,2022-10-17T10:27:16Z,2022-10-18T04:58:51Z,2022-10-18T04:58:52Z,1,1,0,"Occasionally when clicking on an image just right, it causes the image to become selected. This small styling change prevents it from being selected.
before

after",3,0
2959,2022-10-17T15:08:31Z,2022-10-18T11:19:07Z,2022-10-18T11:19:07Z,2,5,0,"This config option will disable the model weights from being automatically changed from the prompt's model hash key.
I often find when iterating over new dreambooth models that I want to test old prompts with new models.  But when you use PNGInfo to load old prompts, it auto swaps back to my old dreambooth model, and I have to manually change back to the new one.  This ends up happening a lot and I got tired of it.
Tested working with, and without, the config option selected.
This should also address #2831",3,1
2967,2022-10-17T16:52:55Z,2022-10-18T08:53:04Z,2022-10-18T08:53:04Z,1,10,4,"In various threads, there's been reports of xformers degrading quality. After some debugging, I narrowed it down to xformers_attnblock_forward.
I failed at fixing this and believe it's probably impossible to fix. I think it's some issue related to the strides of q, k ,v. The CompVis code to rearrange tensors for this function leads to uneven stride sizes and that makes it incompatible with memory_efficient_attention.
Using legacy attnblock_forward fixes the quality loss issue. %5 slower on my 4090.
cc. @danthe3rd
Do you have any ideas about how we could properly implement xformers for this function?",8,23
2984,2022-10-17T19:14:31Z,2022-10-18T11:18:05Z,2022-10-18T11:18:05Z,1,1,2,"Prevent colons in deepbooru output from being replaced by spaces, :D -> D for example.
I don't know why this line was added. Perhaps the author was not sure if (:D:1.5) works? But it does.",3,3
2990,2022-10-17T20:01:04Z,2022-10-17T21:27:16Z,2022-10-17T21:27:16Z,1,1,1,Fixes #2987,2,0
2995,2022-10-17T20:40:59Z,2022-10-19T06:39:28Z,2022-10-19T06:39:28Z,1,20,1,"This fixes the Issue #1412 that I'm facing too. It displays estimated time left for job to finish - right in the progress bar, next to percentage. Works for batches and works on mobile.
There's no way to see how much time it's left for rendering to finish in the UI. It's essential for long batch jobs and makes UI more friendly and predictable.
This is my first and only contribution to this repo so please pardon any mistakes I made. Seems to work fine on my end. Also css works on mobile (added ""hidden"" style).",3,6
3002,2022-10-17T21:47:08Z,2022-10-18T05:29:52Z,2022-10-18T05:29:52Z,1,2,1,,2,0
3008,2022-10-17T23:21:10Z,2022-10-18T11:24:01Z,2022-10-18T11:24:01Z,1,32,3,"This is a very minor enhancement to the prompt editing UI. It improves the edit attention script by automatically selecting a wrapping parenthesis block if you don't have text selected. This makes editing weights of existing weighted blocks much easier.

Demo video",2,0
3035,2022-10-18T06:30:26Z,2022-10-22T16:38:34Z,2022-10-22T16:38:34Z,1,413,0,"As for the settings, it is WIP.
It is now in a reasonable state of practical use.
それなりに実用に耐える状態にはなりました。日本語話者のユーザーの方は、以下からダウンロードして使うことが出来ます。( Japanese-speaking users can download and use the software from the following links.)
https://github.com/yuuki76/stable-diffusion-webui/raw/ja-translation/localizations/ja_JP.json
Most of the main UI has been translated, but due to the lack of Japanese resources, it would be difficult to understand if everything were translated, so it has been intentionally left untouched.",4,5
3041,2022-10-18T08:32:37Z,2022-10-18T12:08:23Z,2022-10-18T12:08:23Z,2,18,4,"Script added to ui-config.json.
{
    ""txt2img/Script/value"": ""Prompt matrix"",
    ""img2img/Script/value"": ""None"",
}
Dropdown visibility added to ui-config.json.
{
    ""txt2img/Style 1/visible"": true,
    ""txt2img/Style 2/visible"": true,
    ""txt2img/Script/visible"": true,
    ""img2img/Style 1/visible"": true,
    ""img2img/Style 2/visible"": true,
    ""img2img/Script/visible"": true
}
Added init_field so that Dropdown switches values.
if init_field is not None:
    init_field(saved_value)",2,1
3049,2022-10-18T10:13:08Z,2022-10-24T05:55:33Z,2022-10-24T05:55:33Z,1,497,0,"A WIP KR localization for very basic stuff, including the txt2img window elements.
Something that I found is that the hint texts for buttons that are icons(Such as the random seed button, or save style button) does not work well with the localization update, it seems it's ignoring the localization file.
Although this might not be a problem, as I couldn't find a guide or tutorial for the localization so I just jumped in.
Please tell me if anything is wrong, as I am not used to using git environment.",2,1
3058,2022-10-18T12:21:32Z,2022-10-18T15:11:20Z,2022-10-18T15:11:20Z,1,0,5,Removing 5 duplicate artists from artists.csv,2,0
3081,2022-10-18T17:19:10Z,2022-10-19T08:52:12Z,2022-10-19T08:52:12Z,1,1,1,assoicated -> associated,2,0
3086,2022-10-18T17:43:51Z,2022-10-19T15:40:53Z,2022-10-19T15:40:53Z,5,79,23,"Added options to configure the structure of hypernetworks when create it.
Almost all of backend logic is borrowed from #2869



hyper network layer structure

If write ""1, 2, 1"", hypernetworks are composed of 2 fully connected layers whose intermediate dim is 2x, which is same as up to now.
The more you add the number, like ""1, 2, 4, 2, 1"", the more the structure of hypernetworks becomes deeper. Deep hypernetworks are suited for training with large datasets.



Add layer normalization

If checked, add layer normalization after every fully connected layer. It would be meaningful to prevent hypernetworks from overfitting and make training more stable. -> It would make training unstable, with Hypernetworks, but might be able to make converge training faster.



For example, if write ""1, 2, 2, 1"" and keep checkbox of layer normalization unchecked, hypernetworks would be like this:
Sequential(
  (0): Linear(in_features=768, out_features=1536, bias=True)
  (1): Linear(in_features=1536, out_features=3072, bias=True)
  (2): Linear(in_features=3072, out_features=1536, bias=True)
  (3): Linear(in_features=1536, out_features=768, bias=True)
)",5,9
3092,2022-10-18T20:07:36Z,2022-10-19T08:50:25Z,2022-10-19T08:50:25Z,1,0,5,"In Safari, the style of ""Torch active/reserverd~"" breaks down when accessed from narrow devices such as iPhone.
I was able to fix the style mess by removing the style below.
Before correction

After correction",2,0
3105,2022-10-18T23:31:52Z,2022-10-19T06:35:53Z,2022-10-19T06:35:53Z,1,1,1,"""self"" was not declared provoking the next error when opting for InvokeAI optimization
Traceback",2,0
3106,2022-10-18T23:35:14Z,2022-10-19T18:29:32Z,2022-10-19T18:29:32Z,1,2,0,"Note: In some testing that I did (via HunterVacui@9ae61f7), most embeddings vectors seem to initialize with absolute min/max value ranges of (-0.05 to 0.05), and with average values in the range of (-0.00008,0.000019) +/- 0.014",2,1
3139,2022-10-19T10:21:20Z,2022-10-26T06:24:21Z,2022-10-26T06:24:21Z,4,394,5,"Option to write a cropped image that highlights the focus point of the image.
Three algorithms:

edge/corner detection - more corners pulls the crop in that direction - weighted at 32%
entropy/complexity detection - more complexity pulls the crop in that direction - weighted at 10%
face detection - pulls the crop in that direction - weighted at 58%

Relative weights can be adjusted in the UI




image set
num images
process method
poor crop
good crop
success rate




photos 1
64
default
26
38
60%


photos 1
64
split
39
70
64% (makes more files than are input)


photos 1
64
auto
7
58
91% (IMPROVED)


digital portraits 1
104
default
3
101
97%


digital portraits 1
104
auto
3
101
97%  (PAR)


photos 2
17
default
2
15
88%


photos 2
17
split
8
20
83% (makes more files than are input)


photos 2
17
auto
1
16
94% (IMPROVED)



There is an increase in accuracy when using ""auto"" compared to the default or split method in these handful of tests.
It is worth noting that the ""split"" method can give you more usable results at the cost of having to go thru them and delete larger numbers of bad crops.

Here are annotated examples on what is going on with the algorithms.
Green dot is weighted center",2,1
3144,2022-10-19T11:23:55Z,2022-10-22T17:36:04Z,2022-10-22T17:36:04Z,3,30,2,"Pull Request Summary
This PR makes the image creation progress display a grid of all images within the current batch instead of just the first.
There are two functions implemented:

The first (samples_to_image_grid) decodes the samples one at a time, converting them to an image, appending them to a list and then finally passing it to images.image_grid.
The second (samples_to_image_grid_combined) decodes the samples simultaneously. This is slightly faster than the previous function, but uses significantly more VRAM when using large batch sizes.

I've added a setting to allow a user to switch decoding methods, it defaults to individual decoding.
Note: This is my first time diving into PyTorch/Stable Diffusion, and it's been a while since I've written Python, so it's mostly just me trying to join already existing things together.
While a relatively small PR, if something is incorrect/needs adjusting, please explain it clearly so I can try to understand better.
Benchmarks
These are some benchmarks I did locally, they should be considered approximate.
All times taken with process time NOT wall time.
Setup
Hardware
CPU: i5-4690K @4.5GHz
GPU: GTX1080 (8GB VRAM)
RAM: 24GB DDR3 @1866MHz
Settings
Arguments: --opt-split-attention --xformers
Steps: 65
Size: 512x512
Batch size: 4
Image progress steps: 10
Data
Individual Decoding (samples_to_image_grid)
First generation:



Step
Process Time




10
2.640625s


20
2.78125s


30
2.78125s


40
2.671875s


50
3.75s


60
2.859375s



Total: 17.484375
Mean: 2.914062
Median: 2.78125
Maximum VRAM during generation (including previews): 4.9GB
Second generation:



Step
Process Time




10
2.78125s


20
3.234375s


30
3.25s


40
2.90625s


50
2.9375s


60
3.15625s



Total: 18.265625
Mean: 3.044270
Median: 3.046875
Maximum VRAM during generation (including previews): 4.7GB
Combined Decoding (samples_to_image_grid_combined)
First generation:



Step
Process Time




10
2.546875s


20
2.015625s


30
2.59375s


40
2.1875s


50
2.234375s


60
2.65625s



Total: 14.234375
Mean: 2.372395
Median: 2.390625
Maximum VRAM during generation (including previews): 7.8GB
Second generation:



Step
Process Time




10
2.515625s


20
2.3125s


30
2.953125s


40
2.28125s


50
2.46875s


60
2.765625s



Total: 15.296875
Mean: 2.549479
Median: 2.492187
Maximum VRAM during generation (including previews): 7.9GB",2,0
3153,2022-10-19T13:47:53Z,2022-10-19T17:20:26Z,2022-10-19T17:20:26Z,1,1,1,"Previously, generating first one image with Hypernetwork set to johndoe-10000.pt then another with it set to johndoe.pt would create infotext listing ""Hypernet: johndoe"" for both cases. This is wrong, because the former image's PNG info would point to the latter one's checkpoint.
Instead, let's list Hypernet: johndoe-10000 for the former case, and Hypernet: johndoe for the latter. In general, we list shared.loaded_hypernetwork.filename split from its final backslash to the period before its file extension, noninclusive.",2,0
3176,2022-10-19T17:54:39Z,2023-01-04T15:40:14Z,2023-01-04T15:40:14Z,1,7,4,"For big tasks (1000+ steps), waiting 1 minute to see ETA is long and this changes it so the number of steps done plays a role in showing the text as well.",2,0
3189,2022-10-19T20:42:46Z,2022-10-21T06:55:00Z,2022-10-21T06:55:01Z,1,7,2,"As per #2921, Hypernet preview images now support infotext if the user so desires.
Tested and works with...
✅ ...both enabled and disabled checkbox “Read parameters (prompt, etc...) from txt2img tab when making previews.”
✅ ...both enabled and disabled checkbox “Create a text file next to every image with generation parameters.”
✅ ...both enabled and disabled checkbox “Save text information about generation parameters as chunks to png files.""
✅ ...every permutation of those settings.
Additional thoughts: Although this implementation does not list the exact checkpoint; each image itself is named after that checkpoint, so the user should have no trouble finding it. Besides, even if it were to list the generation's closest checkpoint, the user will not yet have placed that checkpoint into the hypernetwork folder anyway, and so attempting to send the PNG info anywhere would cause the hypernetwork to unload entirely.
Appending the checkpoint to the Hypernet name in processing.py (via shared.loaded_hypernetwork?) seems like it'll require some nontrivial refactoring, though, so I will study more before attempting to implement it myself. Once added, though, it might be useful for the program to copy that relevant checkpoint to the hypernetwork folder on its own instead of unloading.",3,1
3192,2022-10-19T21:02:29Z,2022-10-21T06:03:23Z,2022-10-21T06:03:23Z,5,451,19,"Apologies about the mess, I wrecked my repo trying to merge changes from upstream. It was easier to just make a new one.
This commit adds support for the v1.5 in-painting model located here:
https://github.com/runwayml/stable-diffusion
We can use this model in all modes, txt2img and img2img, by just selecting the mask for each mode.
Working

K-Diffusion img2img, txt2img, inpainting
DDIM txt2img, img2img, inpainting
Switching models between in-painting and regular v1.4

TODO

Test all sorts of variations, image sizes, etc.",24,51
3197,2022-10-20T00:01:13Z,2022-10-21T06:58:16Z,2022-10-21T06:58:16Z,6,50,23,"Mostly just usability when I've been going around these screens and making stupid errors, or having to restart.
Use an existing caption file is for https://github.com/dfaker/quick-ti-cropper where I've got it to spit out tagged directory names, but other tooling can use the same approach.
Splits training rate into two inputs one for hypernetworks one for embeddings:

Adds option to use an existing caption file, and define how it's input is to be used:

Adds overwtite old option when creating hypernetworks or embeddings.
Fixes a reference to 'last embedding' in hypernetwork training html output.
Adds wiki link to training header text:",2,0
3198,2022-10-20T00:20:05Z,2022-12-10T10:35:52Z,2022-12-10T10:35:52Z,5,71,4,"Addresses  #1692
Currently, my idea is to have the button appear by default if the user is on a device which is not using a traditional mouse. This would be done using css media feature tests  and checking for certain user agents. I also want to make it so that the user has to press the button twice within a certain time frame. This would hopefully prevent too many mishaps caused by mis-clicks.

I do realize that this is less useful on traditional desktop setups but on many atypical setups such as using the web-ui through a VR HMD where you are using a limited pointer or pseudo-mouse, it would make an incredible difference.

Let me know what everyone thinks",5,14
3199,2022-10-20T00:28:51Z,2022-10-21T06:34:45Z,2022-10-21T06:34:45Z,3,23,11,"After #3086 , I and @aria1th realized that hypernetworks inside the webUI have no activation functions, which means they are just linear networks.
So, we suppose it is necessary to add activation functions like ReLU after fully-connected layers. This PR adds a feature to ""create hypernetwork"" UI which allows to select activation functions, and insert it to hypernetworks.
View is like this. At now, ""relu"", ""leakyrelu"" and ""linear"" are supported. If you select ""linear"", no activation functions are applied.",9,10
3218,2022-10-20T06:59:49Z,,2022-10-22T19:07:54Z,1,2,2,"fix img2img color correction only applying to the first image of a batch
when batch size > 1
this should fix issue #1441
I have test with the 3 modes that uses color correction img2img Inpaint Batch img2img
and my fix seems to be working as intended
Win 10
Chrome
Python 3.10.6",2,3
3224,2022-10-20T10:15:03Z,2022-10-21T04:59:00Z,2022-10-21T04:59:00Z,1,13,6,"When user select the option for 'Use original name for output filename during batch process in extras tab' from setting tab, it cause a path issue. This is because, if this option is selected, the original filename including the original path is passed to forced_filename parameter of the save_image function, and the newly designated path is ignored.
So instead of using forced_filename parameter, I modified to pass the filename to basename parameter.
related issues : #3147, #3030",2,3
3241,2022-10-20T14:25:54Z,2022-10-21T06:54:09Z,2022-10-21T06:54:09Z,1,2,2,Had to change the shuffle function as well because somehow indexing numpy arrays with torch tensors of length 1 is a special case.,2,0
3248,2022-10-20T15:35:14Z,2022-10-21T06:50:58Z,2022-10-21T06:50:58Z,1,3,2,"The API flag is currently ignored in launch.py, so users need to run the webui.py command instead. (check out line 201 in launch.py to see why). This fixes that",3,3
3253,2022-10-20T17:26:57Z,2022-10-21T06:52:12Z,2022-10-21T06:52:12Z,1,9,3,"Currently, trying to perform CLIP interrogation on a CPU fails, saying:
RuntimeError: ""slow_conv2d_cpu"" not implemented for 'Half'

This merge request fixes this issue by detecting whether the target device is CPU and, if so, force-enabling --no-half and passing device=""cpu"" to clip.load() (which then does some extra tricks to ensure it works correctly on CPU).",3,2
3263,2022-10-20T20:50:09Z,2022-10-21T06:49:41Z,2022-10-21T06:49:41Z,1,4,0,"Describe what this pull request is trying to achieve.
Whenever Highres. fix is enabled skip/interrupt will not work as intended in txt2img tab. Related issues: #3181 #3087
Additional notes and description of your changes
Two sampler instances are used in the process (sample -> sample_img2img), whenever interruption or skip happened on the first one, it returned last_latent, as intended. Second time it gets through sample_img2img without any action, so default value of last_latent is None, that was the problem. If interruption occurred while the second sampler was running, it was fine. Hence the dependency in the title of #3087
Initially i just returned the first result if the second is None, but then i noticed it is still possible to replicate the problem if interruption happened before sampler makes one iteration even without highres option. So instead i changed the initial value of the last_latent. This should cover both scenarios.
Environment this was tested in

OS: Windows 10
Browser: Chrome
Graphics card: NVIDIA GTX 1060 6GB",2,0
3264,2022-10-20T21:00:27Z,2023-01-13T11:58:03Z,2023-01-13T11:58:03Z,3,48,1,"Describe what this pull request is trying to achieve.
This will allow one to use Tensorboard to log the quality of their training. It's a more upgraded version of the csv writer, so to speak. Tensorboard provides the visualization and tooling needed for machine learning experimentation. For more info see.
Additional notes and description of your changes
Three additional options have been added under Options > Training:

Enable tensorboard logging.
Save generated images within tensorboard.
How often, in seconds, to flush the pending tensorboard events and summaries to disk.


The following scalars are added to tensorboard

The loss and learning rate per epoch
The loss and learning rate over the course of the whole training session
validation Images per epoch


The events will be saved to a tensorboard folder within the training project's folder. No extra packages are required as pytorch supports tensorboard out of the box.  One small change I did that is not directly related to tensorboard was fixing a typo in the code. (ititial_step > intitial_step)
To view a training project with tensorboard, providing it's installed (pip install tensorboard) is to simply go to the project's folder, and run tensorboard --logdir=tensorboard then one can visit http://localhost:8888 to view the board.",5,3
3277,2022-10-21T00:55:35Z,2022-10-22T17:25:33Z,2022-10-22T17:25:33Z,1,44,11,"Loss is highly dependent on image itself.
To track and not mislead users, we need either per-image loss trending information, or mean loss representing dataset itself.
This method allows us to track and understand loss trend correctly, also an effect of learning rate.",4,6
3302,2022-10-21T08:05:03Z,2022-10-21T13:53:06Z,2022-10-21T13:53:06Z,1,1,1,"Background: Some users report when using VAE weights when training a embedding will cause low quality results.
This prevents people from confusing this option with completely unloading VAE weights (by completely I mean having the same result as not having a .vae.pt file in models folder when webui starts).
Preferably I'd like to add another option which does completely unload VAE when doing TI training, but I can't figure out how to do it.",2,0
3309,2022-10-21T09:38:57Z,2022-10-21T13:52:24Z,2022-10-21T13:52:24Z,3,10,2,"Before this, hypernetwork name would not always match the name of the most relevant .pt. Solution bf30673, while an improvement over #3283, still arrives at name through filename, depriving name of its purpose. This proposed change instead makes name match the last-saved .pt in all cases.
A few other benefits:

naming one's Hypernet, say, test:test no longer creates an extensionless file.
the PNG info of a preview image now matches its respective checkpoint exactly.
create_infotext's Hypernet line is more intuitive at a glance.",2,0
3321,2022-10-21T11:48:16Z,2022-10-21T13:51:08Z,2022-10-21T13:51:08Z,1,19,8,"adding recent features to the readme, and linking to the wiki to for a list of online services.",2,0
3329,2022-10-21T12:43:45Z,2022-10-21T13:48:13Z,2022-10-21T13:48:13Z,1,2,0,,2,0
3338,2022-10-21T15:01:38Z,2022-10-25T05:32:34Z,2022-10-25T05:32:34Z,1,2,2,"Fixed bugs #3093 and #3145 so some progress bars are handled correctly.

The total number of steps for img2img is t_enc + 1, not steps.
I checked KDiffusionSampler to check for this bug, but VanillaStableDiffusionSampler may also need to be fixed (haven't check yet).",4,4
3348,2022-10-21T16:31:43Z,2022-10-26T10:17:22Z,2022-10-26T10:17:22Z,1,488,0,"same as pr #3018, I mess-up the git history so I close that one and re-open a pr again.
I've finished all the translation up to f49c08e, welcome for any improvement",13,64
3364,2022-10-21T18:43:36Z,2022-10-22T16:36:58Z,2022-10-22T16:36:58Z,1,7,1,Resolves #3305,3,2
3377,2022-10-21T22:19:51Z,2022-10-22T10:58:01Z,2022-10-22T10:58:01Z,2,19,3,"Hello,
I added the possibility to select which GPU to use with CUDA by a commandline argument ""--device-id"".
Screenshot after starting on the GPU 1:

Screenshot while loading on GPU 0 (then my session crashed because of a lack of RAM) :

On top of allowing to select which GPU to use, it can allow two sessions in parallel if the system has enough RAM (not my case tho 😅)",6,9
3381,2022-10-21T23:38:59Z,2022-10-23T06:26:56Z,2022-10-23T06:26:56Z,3,71,8,"Validated with curl, a postman client (insomnia), and a frontend that I built.

  
    
    

    insomnia.mp4
    
  

  

  



  
    
    

    frontend.mp4",10,22
3414,2022-10-22T13:44:16Z,2022-10-22T16:32:14Z,2022-10-22T16:32:14Z,3,63,40,"1. Add more activation functions like torch.nn.ELU and torch.nn.Hardswish as a ""create hypernetwork"" option.
Activation functions other than ReLU might avoid ""dying ReLU"" problem.
2. Add a checkbox to use dropout. If selected, torch.nn.dropout is inserted before every fully-connected layers.
This prevents hypernetworks from overfitting.
At now, I do not implement enter-dropout-rate textbox and the all dropout rates are 0.3.
3. Refactor some codes on hypernetworks, which is by @aria1th .

If this PR is merged, you will be able to use older hypernetworks without any changes.
View is now like this:",6,8
3421,2022-10-22T15:03:29Z,2022-10-22T16:27:16Z,2022-10-22T16:27:16Z,1,3,2,"After deepdanbooru overhaul you've broke it's linux support by removing ""spawn"" method of multiprocessing, which is explicitely required for torch and tensorflow not to block each other.",2,2
3428,2022-10-22T15:46:23Z,2022-10-24T07:28:43Z,2022-10-24T07:28:43Z,1,92,33,"allows the user to customize the Format and Timezone of [datetime]
Syntex
[datetime]
[datetime<format>]
[datetime<format><time zone>]
[datetime<><time zone>]
e.g.
[datetime]
20221022203816
[datetime<%Y_%m_%d %H-%M-%S><Asia/Tokyo>]
2022_10_22 20-38-16
a demo script can be found hear
https://gist.github.com/w-e-w/ba208f745107316e0c084027e0c40a1f
this is a backwards compatible implementation
the original [datetime] behavior is preserved
test to be working, and did not find issue
using this as the pattern
[datetime<%Y_%m_%d %H-%M-%S.%f><Asia/Tokyo>]_[steps]=[cfg]=[prompt]=[prompt_no_styles]=[prompt_spaces]=[width]=[height]=[styles]=[sampler]=[seed]=[model_hash]=[prompt_words]=[date]=[datetime]=[job_timestamp]
resulting output
01034-2022_10_23 00-22-41.414133_3=7=(1girl),_(sitting)=(1girl), (sitting)=(1girl), (sitting)=64=64=None=Euler a=85172513=925997e9=1girl sitting=2022-10-23=20221023002241=20221023002232.png
01033-2022_10_23 00-22-41.406134_3=7=(1girl),_(sitting)=(1girl), (sitting)=(1girl), (sitting)=64=64=None=Euler a=85172512=925997e9=1girl sitting=2022-10-23=20221023002241=20221023002232.png
01032-2022_10_23 00-22-41.398135_3=7=(1girl),_(sitting)=(1girl), (sitting)=(1girl), (sitting)=64=64=None=Euler a=85172511=925997e9=1girl sitting=2022-10-23=20221023002241=20221023002232.png
01031-2022_10_23 00-22-41.379153_3=7=(1girl),_(sitting)=(1girl), (sitting)=(1girl), (sitting)=64=64=None=Euler a=85172510=925997e9=1girl sitting=2022-10-23=20221023002241=20221023002232.png



I used every pattern in the decorator and the output seems correct
notable changes
due to the formatting of datetime is case sensitive
I have to rework apply_filename_pattern() so that it does not require the input pattern lowercase
originally due to the use of str.replace() and if ""sub str"" in ""str"" being case sensitive
the input pattern converted to do case before being input
due to time formatting, there is the need of preserving the Case of the pattern
so I replaced all str.replace() and `if ""sub str"" to its regular expression case insensitive equivalent
Further for proposal if this code is accepted
this is partially brought about by Feature #3322
I myself and many others might wish to use a time-based file name Prefix system
my reason being it would make sorting of images across different instances of stable-diffusion-webui much easier
I use Google Colab and My own computer to run webui
due to the way that the current naming system works
there's a numbering prefex in the image file name 01234-seed-......png
it is derived from the largest image numbering in the output folder
this means that images created from different instances of webui will not be related
as such it it is very troublesome to store thes images generated from different places together together
if I were to store the image in the same directory, I would get unrelated images interlaced between each other
and due to how currently the webui works the numbering Cannot be disabled by the user
I understand look need of a number system that ensures that file names will not be repeated
this is why I'm wish to propose a time based prefix system
so that image is generated from different instances could be chronologically ordered
note: one could use the file creation dates modification date to order the images, but these dates can be unreliable
if you're willing to accept the proposal
I will be willing to work on it
my intention is that
The user will have to enable such it, the current behavior is not changed
my current idea is to have some value in the config that user can adjust the padding of the numbering
a padding a negative will disable the numbering system
additional file pattern [index<int>] that file decoration pattern
this number serves a similar purpose as the current numbering
the only difference is that the user can decide the location
if this were implemented, I would use a file pattern like
[datetime<%Y_%m_%d %H-%M-%S><Asia/Tokyo>]-[index]=[seed]=[prompt_words]
result will be
01046-2022_10_23 00-32-29-1=2287592556=1girl sitting.png",2,4
3454,2022-10-22T21:02:57Z,2022-10-23T06:32:24Z,2022-10-23T06:32:24Z,1,475,0,Completed and checked JSON validity with https://jsonchecker.com/,3,1
3479,2022-10-23T07:55:32Z,2022-10-23T09:34:16Z,2022-10-23T09:34:16Z,1,2,0,"currently if one is to click the Restart Gradio and Refresh components (Custom Scripts, ui.py, js and css only) under settings the web ui will be restarted

but if the --autolaunch option is used it will open a second web UI page
causing multiple web ui page to be opened in the browser
I believe this is undesirable Behavior
this change fixes this behavior
make it and only apply to the initial launch

--autolaunch
open the webui URL in the system's default browser upon launch",2,0
3486,2022-10-23T11:06:47Z,2022-10-24T06:07:40Z,2022-10-24T06:07:40Z,1,24,16,"Statistics logging is using {filename : list[losses]}, so it has to use loss_info[key].pop()
This KeyError happened when files were over 1024 items.",3,4
3490,2022-10-23T12:10:46Z,2023-01-04T14:40:29Z,2023-01-04T14:40:29Z,1,28,1,Fixes textual inversion training by adding a dummy mask when using an inpainting model,6,11
3493,2022-10-23T12:34:39Z,2022-10-24T06:20:05Z,2022-10-24T06:20:05Z,1,164,95,"I have almost completed the overall translation, made improvements in several areas, and translated a few custom-script-derived words.
added custom script:https://github.com/animerl/novelai-2-local-prompt",2,1
3494,2022-10-23T12:43:37Z,2022-10-24T05:46:32Z,2022-10-24T05:46:32Z,1,1,1,"fix #2404
Ubuntu 20.04.4 LTS, NVIDIA GeForce RTX 3080,   NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7
first pip uninstall deepdanbooru  , and restart with --deepdanbooru
no error:",3,2
3511,2022-10-23T19:33:06Z,2022-10-29T04:24:38Z,2022-10-29T04:24:38Z,2,103,56,"Since each ""extras"" mode did something different I decided to split the endpoint into three, the idea is to keep the client from having to juggle so many empty fields.
Other changes include:

run_extras doesn't have a class to get it's args as far as I know, so I created it's own model schema.
Renamed processing.py to models.py and moved all the endpoints models there.
Updated the API to use models in it's response (It already kinda did but wasn't configured) so its reflected in the docs
Updated the API to use Gradio's own utils functions for file <-> base64 conversions, tried to use decode_base64_to_image when possible because it keeps the associated generation info.

I tested every endpoint so it should work.",3,6
3517,2022-10-23T20:08:11Z,2022-10-24T05:58:57Z,2022-10-24T05:58:57Z,1,17,14,"Describe what this pull request is trying to achieve.
When using inpainting with color correction, the user can save the pre-cc version, but it was saved before the unmodified part of the original image was applied to it.
Additional notes and description of your changes
There's a similar issue with distorted image being shown in previews essentialy for the same reason. Would it be much of an overhead to fix it there too? Should be minor compared to decoding it every n steps.
Environment this was tested in

OS: Windows 10
Browser: Chrome
Graphics card: NVIDIA GTX 1060 6GB

Screenshots or videos of your changes
Color corrected images are on the left.
Before:


After:",2,1
3537,2022-10-24T03:45:53Z,2022-10-24T06:28:38Z,2022-10-24T06:28:38Z,9,18,653,"1.Move out images browser from project
2.add the function of loading javascript from extensions
3.add a cmd_opts ""--ui-debug-mode"" for  developers  to avoid loading model
4.add img2img_paste_fields and txt2img_paste_fields to global variable for “send to” funciotns in extenstions
Images Browser repository：
https://github.com/yfszzx/stable-diffusion-webui-images-browser
Inspiration repository：
https://github.com/yfszzx/stable-diffusion-webui-inspiration",2,2
3538,2022-10-24T03:56:45Z,,2023-03-15T18:27:35Z,2,55,31,"Apply the code from #3277 to textual inversion.py. The original author is @aria1th.
(First-time user of GitHub...hope I didn't miss anything)",4,13
3545,2022-10-24T06:20:26Z,2022-10-24T07:22:49Z,2022-10-24T07:22:49Z,1,1,1,"indentation for config.json
probably someone forgot to add it
note:
indentation is already being used for ui-config.json
before

after",2,0
3548,2022-10-24T06:39:45Z,2022-10-26T10:26:36Z,2022-10-26T10:26:36Z,1,32,22,"This is a more conservative attempt to clean up the Prompts From File UI and adds checkbox to allow the seed to be iterated per-line: functionality that's helpful when you don't want to generate against the same seed(s) for every line.
As it loads...

After loading or typing text:

Changes in more detail:
UI fixes: shrink the textbox to one line until there is a change. At that point it grows to the standard size (or two lines if there aren't at least two lines of text).  I also changed the file control to automatically populate the textbox and then clear itself, so it's clear to the user whether we are pulling from the textbox or the file control.
That's the best cleanup I can do without either reworking the visibility code or adding custom javascript, and I wanted to limit this change to the custom script.
Testing: I've been running with these changes for a while. I also tested all the other scripts, including the gradient-aesthetics extension, just to make sure there aren't weird issues. I tested loading prompts from a file, pasting them in, dragging things to the textbox, etc.  I also tested the iteration changes with all variants of single or multiple batches and batch sizes.
PS: At some point, I think it would be helpful to revisit the way we handle showing/hiding script controls. If we put each script's content into a wrapper column, we could show/hide that column, and we wouldn't run into the current problem:  Gradio controls which aren't IOComponents (aka, layout controls like tabs and such) can't really be used, since returning them from ui causes problems, because they throw an exception when passed as an input, due to the lack of any preprocess implementation. I could code that change up, but only if it's something that you're interested in.",3,4
3549,2022-10-24T06:46:58Z,2022-10-25T05:40:20Z,2022-10-25T05:40:20Z,2,14,3,Add script callback after modules.images.save_image is called. Looking to support doing additional things to the files saved. For example: https://github.com/tsngo/stable-diffusion-webui-aesthetic-image-scorer,6,7
3562,2022-10-24T12:01:18Z,2022-10-26T10:15:40Z,2022-10-26T10:15:40Z,1,16,2,"Since making a pull request for every string updates will be a huge nuisance, I will make a draft PR first and make it live once enough changes are accumulated.
Current changes - Added translation for the updated settings tab",3,4
3571,2022-10-24T14:15:12Z,2022-10-29T20:04:26Z,2022-10-29T20:04:26Z,1,3,4,,3,2
3576,2022-10-24T15:54:52Z,2022-10-25T06:44:54Z,2022-10-25T06:44:54Z,1,3,1,"@AUTOMATIC1111
you've accidentally make a breaking change
the current default (when blank) filename patten is [seed]
before it was [seed]-[prompt_spaces]
current

  
    
      stable-diffusion-webui/modules/images.py
    
    
         Line 481
      in
      df0a1f8
    
  
  
    

        
          
           file_decoration = opts.samples_filename_pattern or ""[seed]"" 
        
    
  


before

  
    
      stable-diffusion-webui/modules/images.py
    
    
         Line 474
      in
      eb007e5
    
  
  
    

        
          
           file_decoration = opts.samples_filename_pattern or ""[seed]-[prompt_spaces]"" 
        
    
  


I'm currently writing the wiki and found this during testing
I think this needs to be fixed quickly so I ping you",2,1
3577,2022-10-24T16:17:12Z,2022-10-26T06:46:17Z,2022-10-26T06:46:17Z,2,14,7,"Describe what this pull request is trying to achieve.
Fixes the api response type for python 3.7 users (people on Colab). Also sends the info field back as actual json (rather than a string), and adds an optional include_init_images to decrease response size
Additional notes and description of your changes
include_init_images is false by default (so the mask and init_image won't be returned). This decreases the response size by ~50%, but might be confusing for users who want to see that. Might be worth flipping if we allow users to send in a URL as @ArcticFaded suggested here
Environment this was tested in
List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.

OS: Linux
Browser: REST client
Graphics card: nvidia a100 (using lambda labs)

Screenshots or videos of your changes",3,0
3580,2022-10-24T16:26:41Z,2022-10-25T05:15:44Z,2022-10-25T05:15:44Z,3,550,690,"Adds necessary styles to view RTL languages in the correct direction. It complements #3560 without changing too much of the base code. Now localization files can accept an optional item named ""rtl"" that can be true or false.
P.S. The Arabic localization file was rearranged to match a newly generated file with the image viewer and aesthetic extensions loaded. The translation needs improvement but this should do for now.",2,2
3595,2022-10-24T21:36:42Z,2022-10-26T06:50:25Z,2022-10-26T06:50:25Z,1,4,7,"The new FilenameGenerator class cannot handle nested brackets correctly.
Fixed it.
It can handle patterns like [[datetime]] to [20221025030405].",3,1
3605,2022-10-25T00:54:51Z,2022-10-25T05:17:42Z,2022-10-25T05:17:42Z,1,418,0,"I manually translated line by line, I kept some words in English for best usage, example: ""prompt"" and ""steps"" should be a standard in every language.",4,4
3621,2022-10-25T06:18:37Z,2022-10-26T06:17:01Z,2022-10-26T06:17:01Z,3,44,11,"As you expected, activation_dict was for variable activation functions, so it should offer any available activation functions.

Yes. (would people be interested in weird activation functions? 🤔 more ML data scientists?)

Also added peeking hypernetwork information for debugging.
Currently supported options are Normal (tweaked bias initialization to use normal_ instead of zeros_), Xavier & He (normal / uniform).

For ReLU / Leaky ReLU, it is known that He (or Kaiming) initialization sometimes offers better result.
For Sigmoid / Tanh, it is known that Xavier initialization sometimes offers better result.
But these changes would not necessarily mean there is something definitely better, rather, it is just offering more chances.
We need proper testing tool like hyperparameter tuning setup to prove something in future.",6,8
3627,2022-10-25T07:36:50Z,,2022-10-26T13:47:09Z,3,6,0,"1.extensions need use init images components
2.add  an administrator option to permit visit all files in PC",3,2
3628,2022-10-25T08:46:01Z,2022-10-26T07:10:57Z,2022-10-26T07:10:57Z,1,8,0,"Fixes #3524 and #292 and #177. Uses PIL's exif_transpose to consider the orientation of photos taken with smartphones.
✅Works with img2img
✅Works with Inpaint, no mask
✅Works with Inpaint, drawn mask
✅Works with Inpaint, uploaded mask
✅Works with Batch img2img",3,3
3653,2022-10-25T15:03:07Z,2022-10-26T06:47:41Z,2022-10-26T06:47:41Z,1,415,0,"I did my best but some terms are better left in english, as are some acronyms.",3,1
3654,2022-10-25T15:14:47Z,2022-10-26T06:51:32Z,2022-10-26T06:51:32Z,1,1,1,"[datetime<Format><Time Zone>]
currently if the Format is blank
a patten like so
[datetime<><Time Zone>]
since """" (blank) is a valid format, it will not trigger the exception and so it will not use the default
adn will result in output """"
also #3595 allow nested bracket
this PR is compatible with the fix PR #3595
I tested with lots combinations of nested brackets and blank parameters that I think it's practical to test
hopefully my test is for enough and this will be the last of bugs concerning this topic",2,0
3669,2022-10-25T18:26:00Z,2022-10-29T04:49:48Z,2022-10-29T04:49:48Z,4,79,52,"So the Runway inpainting model uses this preprocessing when making the conditioning image.
# From https://github.com/runwayml/stable-diffusion/blob/main/scripts/inpaint_st.py
masked_image = image * (mask < 0.5)
Essentially, it masks off the original image wherever its is replacing the content, and we currently implement this logic. This is probably good for outpainting and small area inpainting since it only keeps the relevent parts of the image for the extra info. However, when performing large inpaintining or just full img2img, this makes the image conditioning kind of pointless since it will be mostly (or all) zeros.
I wanted to try and keep the full unmasked conditioning image for img2img and see how the results would change. I'm not sure where to put the flag to turn this on and off, and for now I just put it in the settings tab. Here is an example of the difference in full img2img. The proposed change is Unmasked Latent and the original logic is Masked Latent. Feedback and experiments are appreciated.
A painting of a woman by [ARTIST]
Steps: 100, Sampler: DPM2 Karras, CFG scale: 7, Denoising strength: 0.8",14,23
3682,2022-10-25T20:46:25Z,,2023-01-04T15:21:45Z,3,127,1,"Initial commit, added a button under batch img2img, and a function under interrogate.py to process a directory of images
Personally, I'm not too familiar with Python, PyTorch or Gradio, but I added a new button that loops through a directory and generates CLIP prompts for each image, and saves it all in a CSV in the output (or input) directory.
Just wanted to get this PR out there, but I think this can still be improved by adding a progress bar of some sort (to both the console and Gradio UI hopefully), and maybe reducing the amount of duplicated code as currently it's basically just the interrogate() function contents with some of the loads/unloads rearranged and a loop.
Intention of this is that it's a precursor to another addition I was looking into, a way of reading prompts from a file when doing a batched img2img.
I'm only able to test this on a Linux + AMD setup, but it worked for me on a test batch of 10 PNGs.
Screenshots below:",3,2
3691,2022-10-25T23:20:50Z,2022-10-29T06:03:36Z,2022-10-29T06:03:36Z,3,398,425,"This version abandons literal translation and adds explanations where possible. The translations are ordered by the way they appear in the UI. The final file was trimmed and cleaned of non-translatable parts like the samplers names.
Used the following resources:

https://www.almaany.com/
https://translate.google.com/
https://techtionary.thinktech.sa/
https://sdaia.gov.sa/files/Dictionary.pdf",4,6
3698,2022-10-26T04:14:05Z,2022-11-04T06:02:21Z,2022-11-04T06:02:21Z,2,26,14,"From what I understand, a Hypernetwork learns how to nudge the context in Cross Attention mechanisms.
# modules/hypernetworks/hypernetwork.py#L89
def forward(self, x):
    return x + self.linear(x) * self.multiplier
I believe it'd make more sense if the output layer is not activated.
For example, if ReLU is chosen as the activation function, then the output from self.linear(x) would only contain non-negative values, which is unnecessarily restrictive.
Having said that, I haven't compared the results with or without final activation, so I'd appreciate it if someone could test it out.",6,13
3709,2022-10-26T08:36:35Z,2022-10-26T10:16:07Z,2022-10-26T10:16:07Z,1,1,1,"As the title says, sorry for the title error earlier.
This adds ""Do not write Latest version/repo/commit, as this means nothing and will have changed by the time we read your issue"" to the description of the commit field in the bug report form.",2,0
3711,2022-10-26T11:17:23Z,2022-10-29T05:13:14Z,2022-10-29T05:13:14Z,1,488,0,"Add new localisation JSON for Traditional Chinese (used in Taiwan, Hong Kong, California, etc.), with ISO language code zh_TW.
This localisation uses Taiwanese vocabulary for common technical terms, for example 設定 instead of 設置 (设置) for ""Setting"", 檔案 instead of 文件 for ""File"", 畫素 instead of 像素 for ""Pixel"", 解析度 instead of 分辨率 for ""Resolution"", 伺服器 instead of 服務器 (服务器) for ""Server"", and 網路 instead of 網絡 (网络) for ""Network""; users of Mainland China vocabulary are able to use the zh_CN localisation from PR #3348 instead.",5,5
3715,2022-10-26T12:33:42Z,2022-11-02T10:41:36Z,2022-11-02T10:41:36Z,1,2,1,"Some custom scripts read images directly (e.g. vid2vid, I got an error when using my own script but they are similar) and no need to select images in UI, this will cause an error.",2,0
3717,2022-10-26T13:07:26Z,2022-10-29T04:30:14Z,2022-10-29T04:30:14Z,1,1,0,"In hypernetwork, the linear activation_func that the old implementation used is missing from ui.
Add the linear activation in hypernetwork back to ui.",5,8
3722,2022-10-26T14:39:22Z,2022-10-30T06:02:01Z,2022-10-30T06:02:01Z,3,108,27,,4,30
3723,2022-10-26T15:11:49Z,2022-11-01T11:59:10Z,2022-11-01T11:59:11Z,1,1,1,Resolves issue where the incorrect callback was being called for on_before_image_saved.,3,2
3725,2022-10-26T15:34:47Z,2022-10-29T06:04:08Z,2022-10-29T06:04:08Z,1,492,0,,4,2
3728,2022-10-26T16:04:21Z,,2022-10-27T14:10:02Z,2,13,6,"I found out that the dataset does not shuffle for every epoch. Epoch 2 was different from epoch 1, but after that the epoch was using the same dataset order as epoch 2. So I added a random.shuffle when an epoch ends.
Because we usually use batch size=1 for training, this would be a huge fix.
Not the cleanest solution, and I know there's a shuffle function inside the Dataset class, but couldn't figure out how it's supposed to work.
Edit: While I can't post my works, the training results dramatically improved on my end.",3,6
3755,2022-10-26T23:15:13Z,2022-10-29T04:38:49Z,2022-10-29T04:38:49Z,1,468,0,,4,2
3757,2022-10-27T01:04:26Z,2022-10-29T06:04:23Z,2022-10-29T06:04:23Z,1,419,0,,3,0
3771,2022-10-27T07:10:06Z,2022-10-29T04:29:03Z,2022-10-29T04:29:03Z,1,2,1,"Task list
Pending tasks

 #3698 and fixes Dropout

Done tasks

 Disable unavailable or duplicate options.462e6ba This
MultiHeadAttention is not an standard activation function.
Swish is duplicate key of hardswish. But for supporting generated HNs, dict itself won't be mutated.

Future jobs:


 Fix Hypernetwork multiplier value while training
As far as I read the code, hyperparameter multiplier can be changed while training


 Save and load optimizer state dict
People complained about optimizer not resuming properly, it was because we don't save optimizer state dict.


 Generalized way to save / load optimizers
This is for generalizing optimizer resuming process. It does not necessarily mean it will offer more optimizer options immediately.


 Also offer an option to nuke optimizer state dict
Sometimes you want to nuke optimizer state dict, which will very likely change its training direction.


 Add an option for specify standard deviation + scale multiplier for initialization + nonzero bias initialization
related - #2740
Analyzed data : Colab
Shortly, Xavier and Kaiming have too big standard deviation in weight initialization compared to normal.
But rather than using magic numbers, the std should be parameterized, and we can use xavier normally, if we scale it. (its called gain in pytorch parameter)


 Add an option to fix weight initialization seeds.
This is for reproducing results.


 Add an option to specify dropout structure.
Few  examples have shown that 1, 2, 2[Dropout], 1 structure is promising. This is actually bug-generated networks, which won't be able to struct same structure with fix.
Instead of totally removing the functionality, we need to offer detailed way to specify dropouts.
Example : [0, 0.1, 0.15, 0] -> applies dropout at second, third layer. The sequence should follow the layer structure, First and last value should never use value other than 0.


Optional


 Quick-start in page / Offering references of previously trained HNs


 Emphasize the importance of dataset quality


 Grouping activations by type


 Generalized ways to evaluate HNs properly


 Hyperparameter tuning pipeline


 Add ways to use multiple hypernetworks sequentially or in parallel",4,6
3783,2022-10-27T11:27:31Z,2022-10-29T19:22:00Z,2022-10-29T19:22:00Z,1,2,0,"@AUTOMATIC1111
I believe this should make me have access to the ko_KR.json file alone and nothing else as we disscused in #3562, but I'm not 100% sure
Maybe additional edits in the repository settings should be made? idk
Hope this does give the right permissions that we need",2,0
3791,2022-10-27T13:02:52Z,2023-01-04T15:17:50Z,2023-01-04T15:17:50Z,1,12,4,"This  PR fixes the error for too long prompt. (Fix #705)
Example
1girl, masterpiece, best quality some girls, 美味しいヤミー❗️✨ 🤟 😁 👍 感謝❗️🙌✨感謝❗️🙌✨❗️🍖😋🍴✨🙏✨🙏✨ 🙏✨ 🙏✨🙏✨🙏✨ ハッピー🌟スマイル❗️👉😁👈
OSError: [Errno 36] File name too long: 'outputs/txt2img-images/01066-3029268929-1girl, masterpiece, best quality some girls, 美味しいヤミー❗️✨ 🤟 😁 👍 感謝❗️🙌✨感謝❗️🙌✨❗️🍖😋🍴✨🙏✨🙏✨ 🙏✨ 🙏✨🙏✨🙏✨ ハッピー🌟スマイル❗️👉😁👈.png'",2,1
3798,2022-10-27T14:26:40Z,2022-10-29T06:11:06Z,2022-10-29T06:11:06Z,3,55,4,"Fulfills #3668.
Also added a shortcut, press s to save image if it is open in the modal view.",2,0
3803,2022-10-27T17:22:04Z,2022-10-29T04:52:51Z,2022-10-29T04:52:51Z,1,2,2,"Continuation of #3728

Turns out I wasn't hallucinating and torch.randperm was giving a fixed list as training continued. ([1, 93, ...]) This doesn't happen every time - I am suspecting that this is caused on full VRAM, or conflicting with multiprocess such as running image previews mid-training.
So I swapped the function with a numpy one and it's working smoothly, even working properly when torch.randperm fails to become random.
The commit message should be fixed - this fix also applies to hypernetworks.",3,8
3810,2022-10-27T19:23:20Z,2022-11-06T09:28:00Z,2022-11-06T09:28:00Z,2,22,0,"This change adds a very simple API endpoint for running CLIP/BLIP interrogate calls.
It only accepts an image (base64) and responds with the caption.
Can be further extended to more parameterization.
I also fixed an issue with the --nowebui not being picked up by webui.bat (since it called webui.webui() directly, skipping the cmd line arg check).",2,2
3818,2022-10-27T21:03:21Z,2022-10-29T06:16:01Z,2022-10-29T06:16:01Z,1,7,4,"A few tweaks to reduce peak memory usage, the biggest being that if we aren't using the checkpoint cache, we shouldn't duplicate the model state dict just to immediately throw it away.
On my machine with 16GB of RAM, this change means I can typically change models, whereas before it would typically OOM.",2,0
3826,2022-10-28T03:57:42Z,2022-10-29T05:02:03Z,2022-10-29T05:02:03Z,1,5,2,Example before and after:,2,0
3831,2022-10-28T06:43:03Z,2022-10-30T05:47:18Z,2022-10-30T05:47:18Z,3,7,2,"Addresses #3825. Saves a generation's Hypernetwork strength to infotext (both PNG chunks/EXIF data and .txts). Also loads it with PNG info's ""Send to..."" tabs.
Also made it so infotext explicitly states when Hypernet is None. This unloads a Hypernet when a PNG Info with ""Hypernet: None"" is sent anywhere.",2,8
3842,2022-10-28T10:20:07Z,2023-01-04T16:57:02Z,2023-01-04T16:57:02Z,4,43,14,"Providing gradient clipping to user. It's used to handle exploding gradient. I thought it might help for too-large learning rate in hypernetwork training, but it doesn't seem so. Using 0.0001 lr with 0.00001 norm produces different result from just using 0.00001 lr though, so there might be some way to use this. But for now, this is just exposing one of torch's feature with UI. This feature is for hypernetwork and textual embedding and supports learning rate syntax.

Prior discussion

Here's 1-2-1 swish dropout at lr 0.0001 without gradient clipping (old screenshot):



Here's 1-2-1 swish dropout at lr 0.0001 with norm clipping at 0.1:



Here's 1-2-1 swish dropout at lr 0.0001 with norm clipping at 0.01:



Here's 1-2-1 swish dropout at lr 0.0001 with norm clipping at 0.001:



Here's 1-2-1 swish dropout at lr 0.0001 with norm clipping at 0.0001 (same as lr):



Here's 1-2-1 swish dropout at lr 0.0001 with norm clipping at **0.00001** (lower than lr):



Here's 1-2-1 swish dropout at lr **0.00001** without norm clipping (lower lr than before):",4,8
3858,2022-10-28T13:56:57Z,2022-10-29T04:55:20Z,2022-10-29T04:55:20Z,3,20,18,"Fix #3847
Additionally, csv loss log was inconsistent with console output.

csv: epoch 1-based index, step 1-based index
output: epoch 0-based index, step 1-based index

I made it to be the same as output (epoch 0-based index).
Also, learn scheduler was weird that it used ""<="" sign, so 0.01:10 doesn't mean the first 10 steps, but first 11 steps (applies until step 10 in 0-based steps). I changed this to ""<"" sign.
Screenshots: (debug prints have been cleaned)

Hypernetwork training



Textual embedding training



CSV log",2,0
3874,2022-10-28T21:37:34Z,2022-10-29T04:44:17Z,2022-10-29T04:44:17Z,2,130,55,"This patch makes some updates to the 'extras' page:


It adds an option to run upscaling before running face restoration. This seems to work much better for me, since the face restoration gets a chance to fix the upscaler artifacts.


Reworks the caching upscaler mechanism to allow a bit more freedom in experimenting with the settings without retriggering long upscales. The cache now holds more items (was 2, now 5), uses LRU eviction and is cleared when the source UI image changes.


Also fixes a bug where the Extras page could not process consecutive similar images, since the 'cache key' was only considering a cropped portion of the image, so could ignore new images.",2,0
3877,2022-10-28T23:17:27Z,2022-10-29T04:32:11Z,2022-10-29T04:32:11Z,1,5,4,Filename tags [height] and [width] are wrongly referencing to process size instead of resulting image size. Making all upscale files named wrongly.,2,0
3898,2022-10-29T08:44:09Z,2022-10-30T06:29:29Z,2022-10-30T06:29:29Z,1,21,14,Also provide clearer error message for invalid learning rate,2,0
3915,2022-10-29T13:38:17Z,2022-11-03T21:44:12Z,2022-11-03T21:44:12Z,1,259,135,"如果您有对于翻译的建议，请留言。@dtlnor @bgluminous

需要改进的翻译清单：

部分内容不符合实际功能表现。✅




英文
直译
对应功能
备注




favorites
收藏夹
收藏夹(已保存)
在txt2img页面和设置均为save保存，统一语义


CFG Scale
CFG 规模
提示词相关性(CFG Scale)



Denoising strength
去噪强度
重绘幅度(Denoising strength)



inpaint
去瑕疵
局部重绘







inpaint





去瑕疵
百度翻译


填补，修复，修补
有道翻译


修补
谷歌翻译


绘画，涂料，上漆
Deepl翻译


绘画，去水印，局部重绘
用户翻译




部分文本为自造词，或为专业词汇，用户可能难以理解。




英文
旧
新
备注




Extras
后处理
更多
✅原义 额外


grids
概览图
宫格图
✅原义 网格


forever
不停的
无限
✅




部分内容没有被翻译✅

35c45df

部分内容有更大众的翻译（其他软件普遍采用）✅




英文
旧
新
备注




Just resize
只缩放
拉伸



Crop and resize
缩放并剪裁
裁剪



Resize and fill
缩放并填充
填充",8,49
3917,2022-10-29T14:46:24Z,2022-11-01T10:45:31Z,2022-11-01T10:45:31Z,1,15,13,"When DDIM is set to any of the following step counts 3, 9, 27, 36, 37, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,111 the last value in the ddim_timesteps array is 1000 which is out of bounds for the 1000 entries in the alphacums array.
The formula 999 / (1000 // num_steps) + 1 will give the next valid DDIM step if it's an integer value.
To test txt2img DDIM with 36 steps as the current code with try 36 and then 37 which is also invalid for DDIM.
A repl with the related functions to show how the fix works.
https://replit.com/@MartinCairnsSQL/PoliticalCostlyAdministrators#main.py
Part of the error message
File ""D:\AI\stable-diffusion-webui\repositories\stable-diffusion\ldm\modules\diffusionmodules\util.py"", line 65, in make_ddim_sampling_parameters
alphas = alphacums[ddim_timesteps]
IndexError: index 1000 is out of bounds for dimension 0 with size 1000

Related Bug Reports #3912, #3063,  #2773, #2696, #2126, #1954, #389",2,0
3923,2022-10-29T17:09:53Z,2022-11-04T05:59:12Z,2022-11-04T05:59:13Z,2,14,15,"Resolve #3888 by making weighted masked work with Highres Fix.
Also fixes full resolution in-painting for tall images.",9,13
3928,2022-10-29T18:04:12Z,2022-10-30T06:51:36Z,2022-10-30T06:51:36Z,3,107,51,"Added input validations before loading dataset for training. May save time because dataset loading may take a while
Added missing info on model log, which are the hash and name of sd checkpoint used during training. Ref.
Fix dataset still being loaded when training will be skipped when itital steps == steps",2,0
3941,2022-10-29T19:47:35Z,2022-10-30T04:41:39Z,2022-10-30T04:41:39Z,8,192,0,"Describe what this pull request is trying to achieve.
Semi-automated tests for modules that can be reached via API endpoints. Tests are run with a file run_tests.bat (.sh version should be easily made too). New tests can be easily added.
Those aren't unit tests strictly speaking and i realise that. They're not testing one function, but a lot of them required to produce txt2img or img2img results. Those tests are going in one after the other and state of the app is affected by previous tests. Moreover we assume that API works correctly. To test txt2img and img2img properly whole webui should be launched, with opts, model and gradio stuff. I feel like it's possible to do such a fixture to replicate the state of the app that can be used for testing, but it must be hard enough.
There is no way but the manual testing at the moment, but project now is huge and it takes a lot of effort to verify that some slight changes in one particular part won't affect something elsewhere. Just recently my PR got merged and it broke txt2img for a few minutes, since i did not test it in places that seemed to me unaffected by changes i've made. #3517
I need some opinions on the matter. Is this something that would be useful and should be added, maybe after some changes?
Initially i did it for myself, to prevent from breaking something again. If you're interested, tell me what's wrong and i'll try to fix it.
Additional notes and description of your changes
It works like this:

I used directory structure from gradio repo.
Current issues include but not limited to:

Url is hardcoded to be localhost:7860
Timeout is hardcoded to be 4 minutes
Maybe i missed some important things while setting up enviroment, like when it's custom. I assumed that if someone wants to test the app, environment should be set already.
For now it just tests that process finishes and results are generated without any problems. It IS possible to verify if results are correct, but i can't do that because i'm not sure about what is ""Correct"" result and if i can generate it. There's a lot of issues with a problem of degraded results, results depending on cl args, optimizations, hardware, so i believe that ideal images for testing should be produced collectively.

I know that i could use some commandline argument like --run-tests and do it from the inside, thus no .bat file or polling would be required. The goal was to make tests completely detachable and the idea to change main app to test it seems wrong to me.
And correct me if i'm wrong but is it even possible to make CI/CD with automated testing since we need to download model with authentification every time? Lightweight dummy model could be used to test for crashes though.
Environment this was tested in
List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.

OS: Windows 10
Browser: Chrome
Graphics card: NVIDIA GTX 1060 6GB

Screenshots or videos of your changes",4,4
3952,2022-10-29T22:16:01Z,2022-10-30T00:07:43Z,2022-10-30T00:07:43Z,1,1069,491,"This is my first version of an alternative localization into Italian language which is a follow-up of the current localization file made by @EugenioBuffo (#3725), which I thanks, and of my discussion ""Italian localization (git newbie)"" (#3633) which covers the main user interface, all the current the Extensions and Scripts, with the following exceptions:
txt2img2img (I got errors therefore I removed it from my local installation of SD Web UI) 
Parameter Sequencer (not installed locally)
Booru tag autocompletion (not installed locally)
Saving steps of the sampling process (not installed locally)

I do not forecast to translate the above scripts in the short period, unless I will install them locally on my machine.
I beg your pardon if I am brutally overwriting the originally submitted file but I find quite exhausting to edit and append over a thousand lines of code to the original file. If this is mandatory, then I will delete this commit and start a new one amending the original it_IT.json file.
It is for sure not perfect and there are some translations that can be improved, therefore I wish to invite @EugenioBuffo and any other Italian mother language person willing give advice and to help to review this extensive translation . I look forward read any feedback from the community and developers. Thank you.",3,2
3955,2022-10-29T23:03:57Z,2022-10-30T06:26:25Z,2022-10-30T06:26:25Z,1,98,4,"@blackneoo since I made the PR and you are a code owner, I cannot approve it myself. I added updated translations and forced values for hidden HTML elements. This makes it easier to see what has changed as the repo evolves.",3,2
3975,2022-10-30T11:52:56Z,2022-11-05T13:15:01Z,2022-11-05T13:15:01Z,3,51,6,"Closes #3894
since its log is messed up.
Optimizers, especially  Adam and its variants are recommended to save and load its state.
This patch offers way to save / load optimizer state, also supports for user-selected optimizer types, such as ""SGD"", ""Adam"", etc.
If Selecting optimizer type is enabled, this line has to be changed for safety:
    if hypernetwork.optimizer_state_dict:
to, whatever like
    if hypernetwork.optimizer_name == hypernetwork_optimizer_type and hypernetwork.optimizer_state_dict
to prevent loading wrong state dict for mismatching optimizer types.
Users will see new option in Training section:

This option should be only enabled when they plan to continue training in future.
Training can continue without saving optimizer state, but some user reported that it was blowing up sometimes when its continued from checkpoint... must by bad luck of optimizer...
For releasing HN, it is recommended to turn off the option (with Apply button) before saving / interrupting training.
Standard (1, 2, 1) network file size comparision is here, it is roughly 3x size difference.

Current Task


 Save and load optimizer state dict
People complained about optimizer not resuming properly, it was because we don't save optimizer state dict.


 Generalized way to save / load optimizers
This is for generalizing optimizer resuming process. It does not necessarily mean it will offer more optimizer options immediately.",4,10
3976,2022-10-30T12:08:06Z,2022-11-02T11:09:38Z,2022-11-02T11:09:38Z,4,33,6,"This is a small PR is to add a few requests regarding the ""extra"" models available. Most of the changes are relatively trivial and don't impact the application logic. These are the changes:

added support for realESRGAN architecture with additional up_conv block (""8x"") (#3536).
automatically move BSRGAN models from the previous BSRGAN directory to the ESRGAN directory, to avoid BSRGAN model from ""disappearing"" when updating the code. (#3551, #3596).
added basic Nearest Neighbor upscaling option when working with pixel-art and avoid blurry scaling (Discord request).
change maximum upscaling size to 8, to accomodate for larger upscale factor models.
set the default upscaling value to 4, the ""native"" upscaling factor of most models.
allow use of 1x scale models like ESRGAN, ScuNET, and others that are mainly used for denoising and deblurring without changing images scales. (#3526).",3,5
3982,2022-10-30T14:47:34Z,2022-11-01T10:47:48Z,2022-11-01T10:47:48Z,2,20,0,This is useful like when you want to add a custom API route from extensions or do some post-processing for FastAPI class.,3,2
3986,2022-10-30T15:18:26Z,2022-11-02T11:12:27Z,2022-11-02T11:12:28Z,6,233,28,"Related feature requests:

#3655
#3415
#3866

Dropdown to select VAE in Settings tab. You can also use ""sd_vae"" quicksettings to have it on top like the checkpoint dropdown.

It has these options:

auto (default)
None
VAEs found

It will search VAEs in this order, and priority if same name exists will be in reverse order, supports subfolder:

/models/Stable-diffusion/*.vae.ckpt
/models/Stable-diffusion/*.vae.pt
/models/VAE/*.ckpt
/models/VAE/*.pt
the vae-path cmd line arg

It will try to load VAE in this order:

vae-path cmd line arg, only for the first load
selected VAE in the settings, fallbacks to auto if not found

If auto:

vae-path cmd line arg again, but also applies for non-first loads
.vae.pt beside the selected sd checkpoint
.vae.ckpt beside the selected sd checkpoint

Note: ""beside"" means in the same directory and with the same file name.
Issues and future improvements:
1. Currently, changing VAE is done by reloading the whole sd checkpoint. The reload checkpoint function has many things I don't understand whether they're also necessary for reloading VAE and how to adjust them for reloading VAE. I'll try to improve this later when I have the time.
2. Due to issue 1, reloading VAE bypasses caching.
2. Due to issue 1, caching now works by checkpoint-vae pair as key so it's inefficient. Though if you're not changing VAE, it's pretty much the same as before (just a bit larger key).
3. VAE loaded through cmd line arg isn't saved nor reflected in the settings dropdown. This means the dropdown can show totally different VAE from the actually loaded one. I don't know how to set it through code. Simply setting it will break it. I'm not sure if saving it is a good idea either.
4. The currently loaded VAE from auto isn't reflected in the settings dropdown nor displayed anywhere (other than the output log). The dropdown stays ""auto"". I'm not sure what to do about this.
5. The base VAE is stored in RAM when you load a separate VAE (for preparation when you select None). I guess this should be a setting (whether you want to store it in RAM, in file, or not at all so selecting None will require reloading checkpoint).",10,27
3993,2022-10-30T17:11:18Z,2022-11-01T07:52:52Z,2022-11-01T07:52:52Z,1,1173,1070,"Updated localization with the latest version of these Scripts/Extensions:
animator v6
StylePile
Alpha Canvas
Changed the description for the script ""To Infinity and Beyond"" (Verso l'infinito e oltre) from 'n' to 'Esegui n volte' (Run n times) Added localization for the 'Random' and 'Random Grid' scripts I changed both the title and the parameters Steps and CFG labels to make it clearer that the min/max order can be inverted. Added a few missing translations and character's corrections here and there. Still a work in progress but I will slowly fix wrong thing when I find them.
I have to get the habit to edit the file in the forked repo instead of the one inside my local installation, I know, forgive me.",3,2
4004,2022-10-30T21:17:30Z,2022-11-01T15:19:12Z,2022-11-01T15:19:12Z,2,3,2,"Added ""--clip-models-path"" switch to avoid using default ""~/.cache/clip"" and enable to run under unprivileged user without homedir",2,0
4005,2022-10-30T21:24:28Z,,2022-11-01T15:15:32Z,1,14,2,"Simple change that allows extensions to append their own APIs to the main one. Each extension will have their own mounting sub url so no conflicts there. The original API is obviously maintained.
This allows:

Extensions to enrich the API with their functionalities.
Non-gradio UI/components to access their script features without needing to resort to hacks or hidden gradio buttons.",3,2
4021,2022-10-31T00:23:42Z,2022-11-02T04:29:16Z,2022-11-02T04:29:16Z,2,44,1,"Briefly discussed in the context allowing scripts to force various symmetries onto the image latents here: #2441
Script with basic mirroring and rotation operation here:
https://gist.github.com/dfaker/ac031e87174a94d8a170d897caac9ff6",2,2
4025,2022-10-31T02:32:51Z,2022-11-01T15:22:16Z,2022-11-01T15:22:16Z,1,6,0,,2,0
4026,2022-10-31T02:40:36Z,2022-11-01T15:21:54Z,2022-11-01T15:21:54Z,1,1,1,Add upscale_first to LRU key to stop invalid cache hits.,3,3
4036,2022-10-31T08:43:11Z,2022-11-04T07:54:24Z,2022-11-04T07:54:24Z,1,9,12,"Simple Fix for #4035. If you'd rather have it to really only keep previous models in the cache dict let me know.
Model is now cached when it's about to be switched, so the cache consists of n previous models only.
Note: Model here means sd checkpoint.

Log of me switching between two models with 1 caching
Loading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt
Loading VAE weights from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt
Applying xformers cross attention optimization.
Model loaded.
Loaded a total of 33 textual inversion embeddings.
Running on local URL:  http://127.0.0.1:7860/

To create a public link, set `share=True` in `launch()`.
Caching currently loaded checkpoint
Loading weights [9bd9137b] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/merges/NAI_SD_0.45ws.ckpt
Loading VAE weights from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/animevae.pt
Applying xformers cross attention optimization.
Weights loaded.
Caching currently loaded checkpoint
Loading weights [925997e9] from cache
Applying xformers cross attention optimization.
Weights loaded.
Caching currently loaded checkpoint
Loading weights [9bd9137b] from cache
Applying xformers cross attention optimization.
Weights loaded.
Caching currently loaded checkpoint
Loading weights [925997e9] from cache
Applying xformers cross attention optimization.
Weights loaded.
Caching currently loaded checkpoint
Loading weights [9bd9137b] from cache
Applying xformers cross attention optimization.
Weights loaded.",2,1
4086,2022-11-01T00:31:43Z,2022-11-01T15:06:14Z,2022-11-01T15:06:14Z,1,3,2,Fixes #3875. exif_bytes now makes sure enable_pnginfo is True before adding pnginfo to pngs.,3,1
4087,2022-11-01T00:50:14Z,2022-11-01T12:08:27Z,2022-11-01T12:08:27Z,1,1,0,"Batch image2image has an error when saving if the directory specified in Output directory does not exist.
If the directory does not exist before saving, it will be created before saving.",2,0
4098,2022-11-01T07:28:36Z,2022-11-01T10:54:00Z,2022-11-01T10:54:00Z,5,34,10,"Solves the issue #3449
This PR unload the sd_model before loading the other.
Some code was keeping a reference and preventing the garbage collector from freeing the memory, to fix it some parts of the code had to be modified to prevent loose references.
The lowvram/medvram was the main problem, some other parts had to be modified to enable to free the memory before loading the other model.
PS: I confirmed that the x/y plot still adds and keeps references, but I couldn't find what part of the code was keeping the references. I cannot run all features, but the basic workflow of txt2img/img2img > change model to inpaint > inpaint > change model back is working without leak.",6,3
4110,2022-11-01T13:57:55Z,2022-11-01T15:02:52Z,2022-11-01T15:02:52Z,1,1,1,"caused error below in pytorch-lightning==1.8.0 and it worked in 1.7.7
Already up to date.
Warning: k_diffusion not found at path /notebooks/stable-diffusion-webui/repositories/k-diffusion/k_diffusion/sampling.py
Traceback (most recent call last):
File ""/notebooks/stable-diffusion-webui/webui.py"", line 12, in 
from modules import devices, sd_samplers, upscaler, extensions
File ""/notebooks/stable-diffusion-webui/modules/sd_samplers.py"", line 11, in 
from modules import prompt_parser, devices, processing, images
File ""/notebooks/stable-diffusion-webui/modules/processing.py"", line 14, in 
import modules.sd_hijack
File ""/notebooks/stable-diffusion-webui/modules/sd_hijack.py"", line 10, in 
import modules.textual_inversion.textual_inversion
File ""/notebooks/stable-diffusion-webui/modules/textual_inversion/textual_inversion.py"", line 13, in 
from modules import shared, devices, sd_hijack, processing, sd_models, images
File ""/notebooks/stable-diffusion-webui/modules/shared.py"", line 14, in 
import modules.sd_models
File ""/notebooks/stable-diffusion-webui/modules/sd_models.py"", line 14, in 
from modules.sd_hijack_inpainting import do_inpainting_hijack, should_hijack_inpainting
File ""/notebooks/stable-diffusion-webui/modules/sd_hijack_inpainting.py"", line 6, in 
import ldm.models.diffusion.ddpm
File ""/notebooks/stable-diffusion-webui/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py"", line 19, in 
from pytorch_lightning.utilities.distributed import rank_zero_only
ImportError: cannot import name 'rank_zero_only' from 'pytorch_lightning.utilities.distributed' (/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/distributed.py)",3,1
4112,2022-11-01T14:19:33Z,2022-11-01T23:14:24Z,2022-11-01T23:14:24Z,1,85,41,"Added translations for these Extensions/Scripts:
Dynamic Prompts
Alpha Canvas
Artists to study
Aesthetic Score
Added a few missing translations and corrected some others. Updated to the latest Extension management tool version.
While I was able to translate in the statistics the text ""Time taken:"" because the timing itself is a separate label, I couldn't do the same with the labels ""Torch active/reserved: 3881/3892 MiB"" and ""Sys VRAM: 4859/8192 MiB (59.31%)"" because the values (memory, percentage) are embedded in the labels (or perhaps I am not enough acknowledged to be sure I am doing it right simply translating the text).
P.S.: I've just realized a typo in row 198: the word ""Esecusione"" should be instead ""Esecuzione"".
Happy Halloween!",3,4
4117,2022-11-01T14:55:26Z,2022-11-11T12:46:21Z,2022-11-11T12:46:21Z,2,8,1,"When conducting training, there is a chance that a fixed tag sequence can effect training while working with deepdanbooru, due to the model utilizing attention.
As far as i know, in the training case of Waifu Diffusion, a tag has been shuffled to reduce the effect of the tag sequence.
Thus I added the following function considering that it would be helpful when training if there is an option that shuffles tag sequence randomly in every text generation.
This worked well when I applied this function on my own embedding training",2,5
4120,2022-11-01T15:32:01Z,2022-11-20T13:49:06Z,2022-11-20T13:49:06Z,1,4,2,"This PR allows for the override_settings field in the generation APIs to take effect for hypernetwork overrides, by loading them on change.
Inspired by #3629 and @evshiron , thanks!",3,3
4127,2022-11-01T16:39:19Z,2022-11-01T22:32:14Z,2022-11-01T22:32:14Z,1,2,2,just fixing a typo,3,1
4142,2022-11-02T01:06:35Z,2022-11-02T04:30:04Z,2022-11-02T04:30:04Z,3,8,3,"This PR fix a bug #4104 introduced in my previous PR #4098
The processing resources was being released before it was fully finished, this PR moves that to a method in StableDiffusionProcessing and call it after all the processing is done.",2,0
4150,2022-11-02T05:27:31Z,2022-11-02T09:18:22Z,2022-11-02T09:18:22Z,3,18,5,"Continuation from #4037, I just renamed it.

Addresses #4028, #3940, #1249, #942, and #760. Allows user to save a copy of image/images before highres fix is applied.
Saving ""-before-highres-fix"" is tested and works with...
✅Batch count 1, Batch size 1
✅Batch count 1, Batch size 2
✅Batch count 2, Batch size 1
✅Batch count 2, Batch size 2
⚠️New setting still needs translations.

Also no longer breaks img2img (checks type of p before passing extra argument n).",2,1
4155,2022-11-02T07:05:52Z,2022-11-02T10:04:56Z,2022-11-02T10:04:56Z,2,4,1,"This is the following of #3982 that I forgot to invoke the callback in API only mode.
Sorry about that!",2,0
4173,2022-11-02T14:46:29Z,2022-11-05T14:20:55Z,2022-11-05T14:20:55Z,1,24,11,"I need the metadata of generated images for future analysis in my project.
This feature was added in e0ca4df, and was (accidentally) removed due to #3972 (to be specified, 198a1ff).",2,6
4177,2022-11-02T16:05:10Z,2023-01-04T14:59:31Z,2023-01-04T14:59:31Z,1,1,1,interation -> interaction,2,0
4178,2022-11-02T16:40:25Z,2022-11-04T07:59:27Z,2022-11-04T07:59:27Z,2,17,10,"Added an option to preview created images on batch completion.
To enable, set ""Show image creation progress every N sampling steps."" in settings to -1.
Tested with arguments --skip-torch-cuda-test --deepdanbooru --no-half-vae --api. Everything's ok.",4,3
4179,2022-11-02T17:00:33Z,2022-11-04T16:27:54Z,2022-11-04T16:27:54Z,1,41,28,"Convert callback lists into a private dict*.
Update functions to use that map.
Update and add utility functions for clearing and removing on a function and script basis.
Converting the current def on_x(callback) methods to callable objects with .remove(func) and .add(func) methods might me a good call too, to maintain backwards compatibility and allow individual removal without leaking the private callback list names.
*RIP Isaac Hayes",2,5
4182,2022-11-02T18:39:16Z,2022-11-04T08:07:05Z,2022-11-04T08:07:05Z,2,19,0,"Added process_one() callback to handle StableDiffusionProcessing on every iteration.
For example, to change the number of steps or cfg_scale:
def process_one(self, p, n):
    gen = random.Random()
    gen.seed(p.all_seeds[n])
    p.cfg_scale = gen.uniform(5.0, 15.0)
    p.steps = gen.randrange(20, 29)",2,0
4191,2022-11-02T21:53:10Z,2022-11-04T05:56:35Z,2022-11-04T05:56:35Z,1,1,0,Missing parameter is upscale_first: bool.,2,0
4193,2022-11-02T22:27:17Z,2022-11-03T12:34:27Z,2022-11-03T12:34:27Z,1,22,6,"Updated localization with the latest version of these Scripts/Extensions:
unprompted (new)
img2tiles
random
random grid
Some new options in the Extras and Settings have been translated too.
I found a couple of mistakes after committing the first file and then I uploaded a second version, but I did it also on the local fork's main branch, so I do hope you can clean up my mess. Please, keep only the latest uploaded file the one with (fix typos) in the subject. Sorry for the mess. Thank you.",3,0
4196,2022-11-03T02:41:36Z,2022-11-04T08:04:02Z,2022-11-04T08:04:02Z,1,3,2,"There is no need to re-hash the input image each iteration of the loop. Save a little bit of time by moving the hash outside of the loop.
This also reverts PR #4026 as it was determined the cache hits it avoids were actually valid.",2,0
4199,2022-11-03T04:05:06Z,2022-11-04T08:03:19Z,2022-11-04T08:03:19Z,1,1,1,Really just a QOL feature.,2,0
4201,2022-11-03T05:10:04Z,2022-11-04T08:03:03Z,2022-11-04T08:03:03Z,1,7,4,"There exists a cmd argument --nowebui that is supposed to only launch the API server but not the webui app, but it is currently ignored. This PR fixes that.",2,0
4210,2022-11-03T10:56:47Z,2022-11-04T07:47:50Z,2022-11-04T07:47:50Z,1,1,2,"Fix #3904
(Still some sort of a workaround, the best way is to add unique id or class name to those prompt boxes)",2,0
4218,2022-11-03T13:24:12Z,2022-11-04T07:46:52Z,2022-11-04T07:46:52Z,3,210,8,"This PR adds a bunch of endpoints that clients can find useful:

Get config

Uses the current config to create a model automatically


Update config

Can't add new keys, they need to be created using shared.opts.add_option(shared.OptionInfo(...))


Get cmd flags
Get samplers list

Gives names, aliases and options


Get upscalers list

Gives name, model name, model path and model URL


Get SD models list

Gives title, model name, hash, file path and config file path


Get hypernetworks list

Gives name and file path


Get face restorers list

Gives name and file path


Get realesrgan models list

Gives name, file path and scale


Get promp styles list

GIves name, prompt and negative prompt


Get artist's categories list

Just a list of names


Get artists list

Gives name, score and category",2,0
4222,2022-11-03T16:27:02Z,,2023-01-04T11:03:29Z,4,29,0,"Whether or not this repo has a license for the code written by @AUTOMATIC1111 and other contributors is up to the authors (re issue #2059)--however, at the very least, this repo is out of compliance with the license requirements of all the places where it copied code from projects that contain licenses requiring inclusion of their licenses.
Example (just picking one place at random):
The code here: https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/codeformer/codeformer_arch.py#L1 -- which notes that it was copied from sczhou/CodeFormer does not include a copy of the license from that codebase as noted in the repo for that source:
https://github.com/sczhou/CodeFormer/blob/master/LICENSE#L1-L10
Any place that has copied code from other projects will need to be addressed before this software is legally usable.
I have started the process of proper licensing with this pull-request to add a license for a simpler case in code copied and derived from https://github.com/victorca25/iNNfer",10,10
4233,2022-11-03T18:50:35Z,2022-11-11T15:01:59Z,2022-11-11T15:01:59Z,1,18,1,Fix cuda error in register_buffer on Mac OS.,4,8
4245,2022-11-03T22:57:19Z,2022-11-04T08:00:38Z,2022-11-04T08:00:38Z,1,5,4,,3,3
4249,2022-11-04T00:44:23Z,2022-11-04T05:57:18Z,2022-11-04T05:57:18Z,1,3,2,,2,0
4270,2022-11-04T09:52:59Z,2022-11-04T13:07:20Z,2022-11-04T13:07:20Z,1,1,1,"""Denoising strength"" in UI was translated as ""重绘幅度"" while ""denoising"" in the X/Y plot is translated as ""去噪"", totally confusing.",3,0
4271,2022-11-04T09:54:33Z,2022-12-03T07:20:17Z,2022-12-03T07:20:17Z,2,8,0,"Fix #4137
When training both TI and HN, the progress bar callback can call set_current_image in a separate thread.
This poses a problem when ""Move VAE and CLIP to RAM when training if possible"" is checked and ""Show image creation progress every N sampling steps"" is not 0, as set_current_image attempts to call sd_samplers.sample_to_image which expects the VAE to be in GPU memory.
However, the VAE can either be on the CPU or being copied from one device to another. This leads to a race condition where errors can occur during training.
This fixes the race condition by setting parallel_processing_allowed to false during the duration of training, so set_current_image does not call sd_samplers.sample_to_image",4,3
4273,2022-11-04T11:36:11Z,2022-11-05T13:16:18Z,2022-11-05T13:16:18Z,1,1,1,"This is a small change to order the list of hypernetworks alphabetically. I have a lot of hypernetwork checkpoints in my list, so having them ordered by number of steps is very useful",3,3
4293,2022-11-04T18:14:51Z,2022-11-05T13:18:29Z,2022-11-05T13:18:29Z,1,1,1,"Fixed for ""Available"" tab",3,3
4294,2022-11-04T18:41:01Z,2022-11-05T13:20:45Z,2022-11-05T13:20:45Z,2,13,3,"Rescue current API usage after 5f01171.
This command argument works in both --api and --nowebui mode.

Use --cors-allow-origins=* to allow all origins
Use --cors-allow-origins=http://127.0.0.1:5173,https://codesandbox.io to allow CORS requests from http://127.0.0.1:5173 and https://codesandbox.io",6,6
4297,2022-11-04T19:25:56Z,2022-11-05T13:22:50Z,2022-11-05T13:22:50Z,1,8,6,"Previously it was continue block. 
Fixes #4287",2,0
4301,2022-11-04T20:22:37Z,,2022-11-19T12:17:07Z,5,24,5,This PR allows for the API to load models from checkpoints - it also fixes a typo,7,10
4304,2022-11-04T23:04:16Z,2022-11-05T15:28:25Z,2022-11-05T15:28:25Z,2,5,1,"Updates to k-diffusion 0.0.1.0 and adds the new DPM-Solver++ samplers.
Closes #4280",3,4
4311,2022-11-05T03:40:54Z,2022-11-05T13:06:22Z,2022-11-05T13:06:22Z,1,13,13,Fixes #4289 #4272,3,0
4320,2022-11-05T09:17:30Z,2022-11-05T13:05:51Z,2022-11-05T13:05:51Z,2,21,2,Closes #948,2,0
4342,2022-11-05T17:12:45Z,2022-11-05T23:54:10Z,2022-11-05T23:54:10Z,1,423,91,"Updated localization with the latest version of these Scripts/Extensions:
SD-Chad - Stable Diffusion Aesthetic Scorer (added) AlphaCanvas
StylePile
Alternate Sampler Noise Schedules
SD Latent Mirroring (new)
SD Dynamic Prompts
Dataset Tag Editor
Depth Maps (new)
Improved prompt matrix (new)
New Extensions:
Auto-sd-paint-ext (new)
training-picker (new)
Unprompted
NovelAI-2-local-prompt (new)
tokenizer (new)
Hope there aren't too many mistakes or wrong translations, in case let me know.",3,0
4343,2022-11-05T17:40:18Z,,2023-01-04T13:21:17Z,4,166,50,"This patch makes hypernetwork training respect the batch count and batch size settings in the txt2img tab when generating previews, so one can get image grids.  This allows one to see how the training is affecting more than one seed at once, and thus get a much better sense of how the training is going.
Some typos were also fixed, as well as clarifying a UI warning. Lastly, as someone who spent two weeks banging my head against the wall at the terrible hypernetwork results I was getting from anything except for Linear, only to discover that LMS plays poorly with hypernetworks, I added the recommendation to use Euler as a sampler in the training tab.
This is my first patch for this project, and indeed any project on Github, so my apologies if I did something out of order  :)",4,2
4358,2022-11-05T22:12:21Z,2022-11-19T11:50:02Z,2022-11-19T11:50:02Z,2,11,11,"This PR does the following;

Adds Skip endpoint
Fix options GET endpoint by taking its model from the options metadata
Fix options POST endpoints, no longer applies defaults to fields outside the list",2,0
4368,2022-11-06T02:17:38Z,2022-12-03T07:31:09Z,2022-12-03T07:31:09Z,2,2,2,Fix #3451,2,1
4371,2022-11-06T03:36:56Z,2022-11-06T05:18:16Z,2022-11-06T05:18:16Z,1,11,3,"Describe what this pull request is trying to achieve.
Fixes issues: #800 #1562 #2075 #2304 #2931
When using the LDSR upscaler, the resulting image cuts off part of the image, towards the bottom and right, filling it with black bars instead.
The black bars are also present in Hafiidz/latent-diffusion and Sygil-Dev/sygil-webui, in fact, I traced this bug all the way to the source CompVis/stable-diffusion, even the official code has black bars.
This fix works for all resolutions and aspect ratios, and has been tested on images from the tiny to the large.
Additional notes and description of your changes

Pad width and height to multiples of 64 with the edge values of image to avoid artifacts.
do_upscale.
Crop out padding.

Also fixes a double upscaling bug, if downsampling is required.
Environment this was tested in
List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.

OS: Windows 10
Browser: Chrome
Graphics card: NVIDIA GTX 1080Ti 11GB",2,0
4373,2022-11-06T04:37:23Z,,2023-01-04T13:16:23Z,1,2,2,"I am looking into various of the new samplers and try it in my https://github.com/liuliu/swift-diffusion
There is this magical sigma_min / sigma_max for Karras scheduler seems a bit abrupt. Looking deeper into it, I believe that it is best to match the actual sigma max / sigma min used when training the model. That means for Stable Diffusion, it is the sigma at step 999 and at step 1.
I switched to that value, and both DPM++ 2S Karras and DPM++ 2M Karras looks much more consistent between step 10, step 20, step 30 and step 40. I believe this is a good improvement and don't want to keep it secret only in my repo.
I renounce all the copyright of this PR. Just merge it and fix it for everyone if people verified this is indeed a good fix.",10,11
4395,2022-11-06T18:56:01Z,2022-11-11T12:41:30Z,2022-11-11T12:41:30Z,2,15,2,"A parameter model is added to the interrogate API.
This enables users to select the interrogate model by specifying its name, clip (default) or deepdanbooru (if launches with --deepdanbooru).
The input image is converted to RGB because images with transparency will crash interrogate models.",2,0
4407,2022-11-07T01:07:48Z,2022-12-03T07:30:34Z,2022-12-03T07:30:34Z,1,7,0,"16XX cards dont natively support FP32; but with this simple workaround they do work, without --precision full and --no-half",9,15
4416,2022-11-07T08:19:36Z,2022-11-27T15:50:12Z,2022-11-27T15:50:12Z,2,10,5,"After #4294 added a launch argument to allow regex, I found that it was too inflexible to list the explicit protocol:hostname:port since I wanted to allow all ports on localhost, either http or https, and subdomains at my project's domain.
This keeps that comma-separated argument but adds another for specifying regex. The two flags are able to work together.
This example allows local IPs (useful during development) plus http://example.com:80 and https://example.com:443:
--cors-allow-origins-regex=""https?://((localhost|127\.0\.0\.1|192\.168\.\d+\.\d+):\d+|example\.com)""",2,2
4459,2022-11-08T01:07:48Z,2022-12-03T07:06:27Z,2022-12-03T07:06:27Z,3,36,10,"Following a comment by @matrix4767 on the discussion post for img2img-color-sketch, I implemented rudimentary functionality to let end users color on top of their uploaded images in the browser to better guide the inpainting process. The new feature infers a mask for diffusion sampling by caching the image upon its first upload. It then cross-references the cache with the subsequent edits of the image to determine altered regions after the end user is allowed to paint on top.
The state machine that implements this functionality is a little wonky, and I didn't want to surprise consumers of this repository who are already used to the other way of doing things. So, following in line with img2img-color-sketch, new behavior can be enabled by appending --gradio-inpaint-tool color-sketch to the command line arguments, with the prior behavior, sketch, set as the default.
Caveat Emptor
I couldn't figure out how to implement the state machine such that it ignores when an image is merely edited, as opposed to cleared and reuploaded. GradIO's docs indicate that the Image component has upload and change events, which might lead one to believe these hooks have distinct triggers, but they seem to fire when images are edited, as well. I could perhaps have wired up the edit event to cancel the upload event using coroutines if it turned out to be more exclusive, but that would have been more complicated, might have required enabling queue functionality, and could have introduced a race condition in cases where the events did not fire in the expected order.
Instead of heading further into the weeds, I opted to check whether channel vectors at corresponding locations between the two images are exactly equal, given that the two images are the same size. So, one possible issue is that a fresh upload might contain a pixel-perfect color match on top of the same spatial coordinate, and so falsely be identified as an edit. However, there's a low probability of this happening in different images given uniformly distributed values for each pixel's channels-- of which there are four, in this case $(R, G, B, A)$. Supposing we drop Alpha (maybe the end user uploaded a JPEG), we still only have
$$
P(\mathrm{collision}) = \frac{1}{256\mathrm{\ possible\ values\ }^{3\ \mathrm{channels}}} \approx 5 \times 10^{-8}
$$
or, a false positive on the order of once every hundred million fresh uploads.
I recognize there are flaws in assuming conditional independence between regions of spatially correlated pixels or between the color channels, but pragmatically, in order for this heuristic to fail, the end user would have to paint over the image completely or upload a new image of an identical size, with an exactly identical pixel located at an exactly identical spatial coordinate. I found a helpful blog with some alternative computer vision algorithms designed for detecting duplicates that I'll consider implementing if branching on the chosen condition is found to be insufficient.",10,19
4461,2022-11-08T02:31:41Z,2022-11-27T10:48:25Z,2022-11-27T10:48:25Z,2,6,1,"This fixes GFPGAN and CodeFormer not working with MPS (and although untested it likely fixes CUDA device IDs other than the default not working; see #3713). Directly assigning the device in FaceXLib/FaceLib is necessary, and the device argument is needed by the GFPGAN constructor. The hasattr() check exists just in case the device not being fully set in FaceXLib and/or FaceLib eventually gets fixed and the global removed (e.g. by xinntao/facexlib#19 - although it is worth noting that that PR hasn't been touched in months).",2,0
4488,2022-11-08T16:11:18Z,2022-11-12T07:29:15Z,2022-11-12T07:29:15Z,2,26,2,"By creating a file called ""preload.py"" in an extension folder and declaring a preload(parser) method, we can add extra command-line args for an extension.",3,1
4509,2022-11-09T00:31:25Z,,2023-01-15T20:07:06Z,4,282,46,"Problem: managing training rates with hypernetworks is a pain.
What humans do: look at the preview image(s) and if they seem to be changing too quickly, lower the learning rate (or vice versa)
What this patch does: automate what humans do.
Usage: Instead of specifying a single number as the learning rate, or a comma-separated list of learning rates and cycle numbers, the user can optionally instead specify the training rate as:
=Step0LearningRate/DesiredImageChangeRate/HalfLife
...where
Step0LearningRate is what it says on the tin - the learning rate it starts out with on step 0.
DesiredImageChangeRate is how much the user would like to see the preview images change with each generation, as a decimal percentage (for example, 0.08 = 8% image difference).
HalfLife is the number of cycles over which DesiredImageChangeRate halves.  So for example for =1e-6/0.08/30000,  at step 0 the desired change rate would be 8%, at step 30k it would be 4%, at step 60k it'd be 2%, and so forth.
The latter two parameters are optional; defaults are 8% and 30000 cycles, respectively.
Features
Stability: While it does not guarantee no blow-ups, it seems to be more stable and less of a PITA than manual rate specification.
Caution: It is capable of ramping learning rates down quickly, as fast as a literal order of magnitude, in response to rapid image changes. By contrast, ramping up cannot exceed 30% per preview image cycle, and 75% of the value of the new learning rate is based on the old learning rate.  Aka, the NN transitioning from one plateau to a next isn't a problem.
Resumption: The user can resume at any point without changing the rate, and it will pick up where it left off. If there is an .optim file, it uses the last rate in the .optim file. If there is none, it makes a pessimistic guess at the rate; it then readjusts up to the desired image change rate over the coming preview cycles.
Annealing: Learning rates fluctuate up and down, usually twofold or so. This adds a small annealing impact to the learning process, which is generally seen as beneficial.
Limitations
Not magic: While it helps resist blowups, it does not prevent them.


If you specify too high of a step 0 learning rate, it can blow up before it even really gets going.


If you generate previews too infrequently, you might go from ""everything's running just fine"" to ""blown up"" with no previews in-between. This isn't common in my experience, but if you try to push it too hard it might happen.


If you only generate preview images for one seed, you might not get a good idea of how the model as a whole is changing. Pull request #4343  for allowing one to generate multiple preview images as a grid is useful.


Of course, if you generate images too frequently and too many seeds at once, you'll slow down your generation, so there's a balance to be struck.


It's possible to get a ""slow blowup"", without any radical movements.  This generally happens if you push your luck too far, like going with a half-life of say 80k cycles or whatnot, aka trying to keep the model making large changes for very long periods of time.  Basically, the autolearning system will prevent its attempts at quick blowups until the model finally finds a way to pull off a slow blowup that sneaks through.


So to repeat: it helps, but it's not magic.  Stick within reasonable bounds and it makes training a more pleasant experience.  :)
Future possibilities
I wanted to also implement two auto-rollback systems:
Rollback to the last checkpoint and slow down if there's sudden radical changes in the image.  Basically, step off plateaus more gently.
Rollback >= 10k steps and slow down if the loss rate gets too high.  Basically, if it's clearly blown up and you're getting loss rates like 0,3 or whatnot, jump way back.
Unfortunately, I can't do this because of the memory leak; you can't restart training without using up VRAM and eventually crashing.  That said: if someone finally fixes the memory leak, I'll implement this auto-rollback functionality.",8,49
4514,2022-11-09T05:04:20Z,2022-11-11T13:04:17Z,2022-11-11T13:04:17Z,1,18,11,"This fixes #4448
previously the wrong model has been added to the ckpt cache (it was added to the cache, before it was loaded from file).
Additionally, if the ckpt cache is disabled in the settings, the cache won't be used.
i would like someone to additionally test / verify it.",7,20
4527,2022-11-09T17:16:11Z,2022-11-19T07:49:44Z,2022-11-19T07:49:44Z,6,30,4,"Add option to export ACCELERATE=""True"", which will in turn cause the application to be launched via accelerate launch, versus python.",9,9
4543,2022-11-10T02:46:12Z,2022-11-11T15:00:14Z,2022-11-11T15:00:14Z,1,1,1,"The function list_files_with_name in modules.scripts.py has a bug, here is the quick fix.",2,0
4546,2022-11-10T02:55:32Z,,2023-01-14T13:30:51Z,2,54,9,"Alternative solution based on the PR #2459 and calculating the hash using the CRC as suggested by @RupertAvery in #2459 (comment) so people can try this suggestion.
Please see the original PR #2459 for more informations.

added option to select the hash version (default: 1)
added option to show the old hash along with the new one (default: False)
added backward compatibility with v1, so uploading a info or x/y plot with a v1 hash will select the correct model, no matter the hash version configured",2,1
4573,2022-11-10T18:47:20Z,2022-11-11T12:51:14Z,2022-11-11T12:51:14Z,4,62,1,"This PR implements feature request 4415:
#4415
When a user clicks on any of the images in the txt2img or img2img galleries, the generation text beneath the image will be updated to show the correct information for that image.
The approach I took relies on using the ""infotexts"" field of the Processed images object. So that means the generation text / infotext is generated once on image generation, and never recalculated.
This works perfectly in most cases (including normal image generation and using the Wildcards extension) but some of the extra image generation Scripts have some issues:

I had to update the prompt_matrix and prompts_from_file scripts to store infotexts correctly/consistently. They work fine now
The X/Y plot should work when batch size = 1. But when batch size is greater than 1, all the images displayed in the gallery are grids (rather than single images) so it's unclear what generation text should be displayed beneath them. Currently, the infotext updates when you click an image, but it probably doesn't show the correct info for that image.
And I can't make any guarantees for how this will work for other custom scripts/extensions. This approach relies on the ""infotext"" field so it's up to the developers of those scripts to set that field properly",4,6
4583,2022-11-11T01:55:19Z,2022-11-27T13:30:24Z,2022-11-27T13:30:24Z,1,2,2,For security reason.,3,4
4623,2022-11-12T03:08:22Z,2022-11-12T06:51:33Z,2022-11-12T06:51:33Z,1,10,3,Now program will fallback to mps unconditionally without checking whether mps is really usable.,2,0
4635,2022-11-12T12:39:00Z,2022-11-27T13:40:26Z,2022-11-27T13:40:26Z,11,125,44,"Describe what this pull request is trying to achieve.
Use github-actions workflow to test every pull request automatically. Fix and improve tests overall.
Additional notes and description of your changes
Changes:

Added empty checkpoint in test_files dir to be used in tests by default if no other checkpoint is specified
Server polling now checks whether the sdwebui process is still on and will cancel polling immediately on crash instead of on timeout
DDIM added to vanilla sampler txt2img test
options_write utility test temporarily disabled removed because the functionality is broken at the moment.
Typo fixed in txt2img with tiling test name
Tests separated in two folders. basic_features tests require no additional downloads, they could be run on cpu quite quickly. advanced_features tests should check correctness of results, so they require clip model, sd model, interrogate, upscale, face restore models etc. Some of them will require cuda device too (probably).
Tests are run with --tests argument as before, but now it is possible to specify the directory: [--tests advanced_features]. If no options were provided then all tests will be ran.
Autotesting with github-actions will use basic_features tests for now, output of the main program is available as an uploaded artifact under job summary:


As i stated, the goal is to run autotesting on every PR/commit, so it will be on CPU.
Currently it is possible to run tests on cpu like that and it's quite fast:
python launch.py --tests basic_features --no-half --disable-opt-split-attention --use-cpu all --skip-torch-cuda-test
Face restoration cant be done on cpu for some reason, did not investigate it further. Probably FR model is loaded on cuda device no matter the --use-cpu option.
Environment this was tested in

OS: Windows 10
Browser: Chrome
Graphics card: NVIDIA GTX 1060 6GB",4,4
4646,2022-11-12T19:03:56Z,2022-11-19T09:22:51Z,2022-11-19T09:22:51Z,2,7,4,"Hello, @AUTOMATIC1111.
This pull request fixes error with updating extensions WSL2 Docker that set 755 file permissions instead of 644, this results to the error.
Instead of using the git pull origin command, I use git fetch --all and git reset --hard origin to force update directory.
Error
Example
Error pulling updates for ddetailer:
Traceback (most recent call last):
  File ""/stable-diffusion-webui/modules/ui_extensions.py"", line 39, in apply_and_restart
    ext.pull()
  File ""/stable-diffusion-webui/modules/extensions.py"", line 70, in pull
    repo.remotes.origin.pull()
  File ""/usr/local/lib/python3.10/site-packages/git/remote.py"", line 910, in pull
    res = self._get_fetch_info_from_stderr(proc, progress,
  File ""/usr/local/lib/python3.10/site-packages/git/remote.py"", line 750, in _get_fetch_info_from_stderr
    proc.wait(stderr=stderr_text)
  File ""/usr/local/lib/python3.10/site-packages/git/cmd.py"", line 502, in wait
    raise GitCommandError(remove_password_if_present(self.args), status, errstr)
git.exc.GitCommandError: Cmd('git') failed due to: exit code(1)
  cmdline: git pull -v origin
  stderr: 'error: Your local changes to the following files would be overwritten by merge:'

Details
cd /stable-diffusion-webui/extensions/ddetailer


  ls -la
total 4
drwxrwxrwx 1 1000 1000 4096 Nov  9 13:25 .
drwxrwxrwx 1 1000 1000 4096 Nov  9 13:32 ..
drwxrwxrwx 1 1000 1000 4096 Nov 12 18:52 .git
-rwxrwxrwx 1 1000 1000   63 Nov  9 13:25 .gitignore
-rwxrwxrwx 1 1000 1000 2888 Nov  9 13:25 README.md
drwxrwxrwx 1 1000 1000 4096 Nov  9 13:25 misc
drwxrwxrwx 1 1000 1000 4096 Nov  9 13:25 scripts



  git diff
diff --git a/.gitignore b/.gitignore
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/misc/ddetailer_example_1.png b/misc/ddetailer_example_1.png
old mode 100644
new mode 100755
diff --git a/misc/ddetailer_example_2.png b/misc/ddetailer_example_2.png
old mode 100644
new mode 100755
diff --git a/misc/ddetailer_example_3.gif b/misc/ddetailer_example_3.gif
old mode 100644
new mode 100755
diff --git a/scripts/ddetailer.py b/scripts/ddetailer.py
old mode 100644
new mode 100755



  git pull origin
hint: Pulling without specifying how to reconcile divergent branches is
hint: discouraged. You can squelch this message by running one of the following
hint: commands sometime before your next pull:
hint: 
hint:   git config pull.rebase false  # merge (the default strategy)
hint:   git config pull.rebase true   # rebase
hint:   git config pull.ff only       # fast-forward only
hint: 
hint: You can replace ""git config"" with ""git config --global"" to set a default
hint: preference for all repositories. You can also pass --rebase, --no-rebase,
hint: or --ff-only on the command line to override the configured default per
hint: invocation.
Updating 52d2e57..8e58e7f
error: Your local changes to the following files would be overwritten by merge:
        README.md
Please commit your changes or stash them before you merge.
Aborting",5,5
4663,2022-11-13T02:38:24Z,2022-11-27T11:16:46Z,2022-11-27T11:16:46Z,1,1,1,,2,0
4664,2022-11-13T03:43:14Z,2022-11-19T12:30:32Z,2022-11-19T12:30:32Z,1,2,2,"Describe what this pull request is trying to achieve.
Fix typos in the env vars in launch.py
Environment this was tested in
List the environment you have developed / tested this on. As per the contributing page, changes should be able to work on Windows out of the box.

OS: [e.g. Windows, Linux] Ubuntu 20.04
Browser [e.g. chrome, safari] Brave
Graphics card [e.g. NVIDIA RTX 2080 8GB, AMD RX 6600 8GB] RTX3090

Screenshots or videos of your changes
N/A",2,0
4679,2022-11-13T14:34:08Z,2022-11-19T09:24:44Z,2022-11-19T09:24:44Z,1,1,0,"Add 'Inpainting strength' to the generation_params dictionary of infotext which is saved into the 'params.txt' or png chunks.
Value appears only if 'Denoising strength' appears too.
New 'Inpainting strength' option allows to apply/transfer style to an image while preserving its composition and details. To achieve best results you should match 'Denoising & Inpainting strength' parameters to each other.
So you often need to remember which value you used to achieve that result.",3,1
4680,2022-11-13T15:36:44Z,,2022-11-19T12:02:59Z,0,0,0,"Some new features, optimizations, and fixes for TI and hypernetwork training!

Correctly sample from the VAE encoder while training Add latent space sampling methods
From the original repository code of ldm and textual inversion, when encoding the training data image, a random sample is created for every loop of training. For all this time however, we have been using just one sample from the VAE. This is not how VAE are supposed to work, because its a variational autoencoder, so it encodes data to a normal-distributed latent space, not just one fixed result.
But while tinkering with the code, I discovered that sampling from the mean of latent space can bring better results than one random sample or multiple random samples. So I would like to add options to try out different latent space sampling methods.


'once': The method we have been using for all this time.
'deterministic': My method.
'random': The original method implemented by the original authors. Warning: may increase VRAM usage.



Gradient Accumulation added
Now we can train with larger batch sizes without being limited by VRAM.


Use DataLoader
Code is much more cleaner now. Also added a shared option to turn on/off pin_memory.


Changed the loss display to show its true value, instead of showing the mean of last 32 steps.
Also disabled report_statistics for hypernetworks, with gradient accumulation applied, viewing loss for individual images are not needed.


torch.cuda.amp.scaler is applied for TI.


Added some extra optimizations for training.




This pr partially fixes training when other SD VAE are selected. The error is directly related to scale_factor that is multiplied to the random sample from the encoder. When no VAE is selected, it correctly applies scale_factor=0.18215, but when other VAE is selected, it applies scale_factor=1, botching the training process. This pr universally applies scale_factor=0.18215, but it can still fail with other VAE/model because their optimal scale_factor can be different from 0.18215.


 Add latent space sampling methods


 Gradient Accumulation


 DataLoader with pin_memory


 Print true loss instead of rolling loss


 Fix shuffle_tags and tag_drop_out to properly work with hypernetwork training


 Fix scaling with autocast (No more gradient turning to zero)


 Fix VAE noise-to-signal ratio mismatch",10,20
4684,2022-11-13T18:42:14Z,2022-12-24T06:06:10Z,2022-12-24T06:06:10Z,1,14,1,"This should address #4485 and #4474
We try to do the rename first since it'll be faster and more reliable for setups where it works, but when it doesn't we fall back to using shutil.move() so that it can handle when there's multiple filesystems involved (usually docker, or symlinks involved).",3,3
4688,2022-11-13T20:21:26Z,2022-11-27T19:46:49Z,2022-11-27T19:46:49Z,1,1,1,"I'm fitting embeddings with textual inversion, using a prompt template file that contains just [filewords] and a text file with the prompt per image in the training data. I tried using [name] in the prompts in those text files, but it doesn't get translated to the token for the embedding, the Last prompt shown in the training progress part of the UI always literally has [name] in it instead of the token. This tiny change solves that issue.
I tested to confirm this works as intended and embedding training still happens. When I try to run the repo tests, I get a generic error message (see below) and I don't think any of the tests actually run... I personally think this change is simple enough that it could just be merged given it ""works for me"" and there don't even seem to be any meaningful tests of embedding or hypernetwork training yet. Up to you.
Launching Web UI in another process for testing with arguments: --skip-torch-cuda-test --deepdanbooru --no-half-vae --api
Launch unsuccessful
Stopping Web UI process with id 30880
Press any key to continue . . .",3,2
4711,2022-11-14T14:44:41Z,,2022-12-15T20:19:36Z,2,5,5,"As discussed d8ahazard/sd_dreambooth_extension#149
and
https://stackoverflow.com/a/8582277
This resolves potential issues with spaces in bat files.",3,7
4717,2022-11-14T20:14:38Z,2022-11-19T12:31:10Z,2022-11-19T12:31:10Z,2,5,2,"I was able to connect to SDWUI while running my new extension by using --server-name 0.0.0.0 instead of --listen without having to pass --enable-insecure-extension-access. lf --listen is considered insecure without user confirmation then --server-name should be as well. Additionally I have given --server-name priority over --listen to prevent confusion. This is because if a user wants the server name to be bear.com by passing --server-name bear.com but also passes --listen, currently, it would not work as the user expects as the server name would still be set to 0.0.0.0.",2,0
4733,2022-11-15T08:21:29Z,2022-11-19T12:20:04Z,2022-11-19T12:20:04Z,2,43,21,"If the WebUI was exposed to the Internet, the API might be harmed, so add the HTTP Basic Authentication.
Note: I am not a whatever security expert or even networking engineer or something, but as far as I know, the HTTP basic authentication was still not safe. For example, the timing attack or MITM (man in the middle) attack could get the credenticals. But it is still a good enough approach to protecting your service.
Usage:

Add --api-auth user:pass to command arguments
Encode user:pass into base64 encoding.
Add Authorization: Basic <base64> to every request headers while <base64> is the base64-encoded string from 2).
If the Authentication is not provided, API would return a 401 error.",2,1
4754,2022-11-16T00:38:49Z,,2023-01-04T13:24:32Z,1,28,0,!draw computer with birthday hat,5,2
4759,2022-11-16T04:13:33Z,2022-11-19T10:49:21Z,2022-11-19T10:49:21Z,1,3,0,"Why?
as state in cmd help, we hide it because it slows down ML if you have hardware acceleration in browser
As Gradio version bump to 3.9 at 804d9fb
it changes some property thus the old code not able to hide it
changes:
gradio-app/gradio@c08b12a#diff-d2cd6ef7d789a4bb148dd3c174e8a4ebf5fd8f9c930261ff0d35f25840c26881
gradio-app/gradio@f79a76c#diff-627ef0c33bdebb0f38edab24ee4be0da0687bcd1080b1259ccd17b943ded608d
I am new to css, I saw class z-20 has been referenced at other places, idk if it affecting other components. If it does, or if there are any other better method, plz tell me or suggest change.",2,0
4778,2022-11-16T18:16:52Z,2022-11-19T09:52:55Z,2022-11-19T09:52:55Z,1,5,0,"Problem
When running scripts that loop and you have a style (large prompt), subsequent iterations slow down, eventually to a crawl.
Affects

Loopback
SD upscale
batch size (not quite as extreme)

Fix
Store incoming prompts and restore them at the end to prevent unbounded prompt growth causing slowdowns.
This also fixes determinism regardless if you use a 'text' prompt or a style (with same content).
(Disclaimer: not a Python programmer, just did enough to verify the issue and fix)
Test
Below is a test for loopback script on 100+ prompt for 6 iterations. Result 2x speed up.
Before
Running DDIM Sampling with 56 timesteps
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.32it/s]
Running DDIM Sampling with 56 timesteps██████████████████▊                                                                                                                                                                                            | 56/342 [00:08<00:45,  6.32it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  6.14it/s]
Running DDIM Sampling with 56 timesteps███████████████████████████████████████████████████████▎                                                                                                                                                      | 112/342 [00:20<00:36,  6.28it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  6.13it/s]
Running DDIM Sampling with 56 timesteps████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                  | 168/342 [00:34<00:28,  6.15it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  6.04it/s]
Running DDIM Sampling with 56 timesteps████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                             | 224/342 [00:55<00:19,  6.09it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  6.00it/s]
Running DDIM Sampling with 56 timesteps█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 280/342 [01:22<00:10,  6.07it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:09<00:00,  5.95it/s]
Total progress:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 336/342 [02:01<00:02,  2.76it/s]

After
Running DDIM Sampling with 56 timesteps
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.28it/s]
Running DDIM Sampling with 56 timesteps██████████████████▊                                                                                                                                                                                            | 56/342 [00:08<00:44,  6.49it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.25it/s]
Running DDIM Sampling with 56 timesteps███████████████████████████████████████████████████████▎                                                                                                                                                      | 112/342 [00:20<00:36,  6.33it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.27it/s]
Running DDIM Sampling with 56 timesteps████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                  | 168/342 [00:30<00:28,  6.20it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.22it/s]
Running DDIM Sampling with 56 timesteps████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                             | 224/342 [00:40<00:18,  6.30it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.27it/s]
Running DDIM Sampling with 56 timesteps█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 280/342 [00:50<00:09,  6.22it/s]
Decoding image: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.25it/s]
Total progress:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 336/342 [01:01<00:01,  5.43it/s]",3,6
4812,2022-11-18T02:06:03Z,2022-11-19T10:26:33Z,2022-11-19T10:26:33Z,2,10,2,Wasn't sure why it was missing from just preprocessing so I added it,3,0
4819,2022-11-18T10:39:16Z,2022-11-19T10:26:03Z,2022-11-19T10:26:03Z,1,1,1,"Fix for issue #4818 where tiling can get stuck enabled after switching checkpoints.
I'm not sure if this is the correct fix, but it looked like undo_hijack was not cleaning up the apply_circular correctly. This should be the appropriate fix if this is the case.",3,0
4841,2022-11-19T05:11:50Z,2022-12-10T06:58:21Z,2022-12-10T06:58:21Z,2,19,20,"This is a split PR of #4666
There was still some issue for the VAE selector:

restore_base_vae was actually never called (you can see I actually called it wrong but error was never thrown), so ""None"" VAE option was bugged.
Base VAE caching was done after loading selected VAE. Should be before
Base VAE caching didn't work because it now requires deepcopy

So here's fixes for them. The None option is used for #3866
2 rounds of test just like before:
Prompt:
ganyu \(genshin impact\),
Steps: 20, Sampler: Euler a, CFG scale: 7, Seed: 1, Size: 512x512, Model hash: 925997e9, Eta: 0.0022321428125, Clip skip: 2, ENSD: 31337

Order (2 rounds):

auto (final_pruned.vae.pt) (animevae)
wd_kl-f8-anime2
sd_1.5_mse
None

First round (for comparison between VAEs used, they should be different even just a little)
animevae:

wd_kl-f8-anime2:

sd_1.5_mse:

None:


For comparison between rounds (grouped by VAEs, each having two images which should be the same):
animevae



wd_kl-f8-anime2



sd_1.5_mse



None



None Option test
Test alternating between animevae and none. Same prompts.
Order:

auto (animevae)
None

First round (for comparison between VAEs used, they should be different even just a little)
animevae:

None:


animevae



None



Comparison between this one's None and the previous one (from the test with 4 options tried)
Current:

Previous:


Embedding training test
Embedding training after switching to None.
To ensure ""None"" option works for embedding training (it goes bad if VAE is loaded).
NAI model. ""auto"" resolves to animevae
Parameters




VAE ""auto"", select ""None"", then train",4,2
4842,2022-11-19T05:16:40Z,2022-11-19T07:59:43Z,2022-11-19T07:59:43Z,3,18,9,"This is a split PR of #4666
Added option to treat the VAE selector as default fallback. Priority will then become: (1) vae-path arg, (2) similar VAE as checkpoint (e.g. ""beside"" checkpoint, same path but .vae.pt ext), (3) selected VAE. Ref: #3655 and a comment

Test 1: Toggling the option on and off
Model: final_pruned.ckpt
VAE exists beside it: final_pruned.vae.pt
Selected VAE: Anything-V3.0.vae.pt
With the option unchecked:


With the option checked:


Test 2: Switching to another checkpoint
Model: sd_1.4.ckpt
VAE exists beside it: None
Selected VAE: Anything-V3.0.vae.pt
With the option unchecked:


With the option checked:


Test 3: Switching to other VAE
Model: sd_1.4.ckpt
VAE exists beside it: None
Selected VAE: Anything-V3.0.vae.pt
Switch to VAE: final_pruned.vae.pt
With the option unchecked:


With the option checked:",3,2
4844,2022-11-19T05:30:23Z,2022-11-19T09:21:22Z,2022-11-19T09:21:23Z,3,10,15,"This is a split PR of #4666
There's some code from #3986 that's no longer necessary. Some even caused issue #4651 . I can't reproduce the issue though, even with a config file. I hope people having trouble with #4651 can try it out.
Also add more detail to the printed messages.
Test: Switching between models with config file (VAE auto)
1:
Checkpoint: final_pruned.ckpt
VAE beside it: final_pruned.vae.pt
Config: final_pruned.yaml
2:
Checkpoint: sd_1.4.ckpt
VAE beside it: None
Config: None
3:
Checkpoint: final_pruned.ckpt
VAE beside it: final_pruned.vae.pt
Config: final_pruned.yaml
Output log with this patch (embeddings omitted):
Loading config from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.yaml
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 64, 64) = 16384 dimensions.
making attention of type 'vanilla' with 512 in_channels
Loading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt
Using VAE found similar to selected model: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.vae.pt
Loading VAE weights from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.vae.pt
Applying cross attention optimization (Doggettx).
Model loaded.
Loaded a total of 50 textual inversion embeddings.
Running on local URL:  http://127.0.0.1:7860/

To create a public link, set `share=True` in `launch()`.
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
Loading weights [7460a6fa] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/stable_diffusion/sd_1.4.ckpt
Global Step: 470000
Applying cross attention optimization (Doggettx).
Model loaded.
Loading config from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.yaml
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 64, 64) = 16384 dimensions.
making attention of type 'vanilla' with 512 in_channels
Loading weights [925997e9] from /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.ckpt
Using VAE found similar to selected model: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.vae.pt
Loading VAE weights from: /content/nai/stable-diffusion-webui/models/Stable-diffusion/novelai/final_pruned.vae.pt
Applying cross attention optimization (Doggettx).
Model loaded.",2,0
